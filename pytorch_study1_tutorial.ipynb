{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_study1_tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/alohays/Neural-Nets-Study/blob/master/pytorch_study1_tutorial.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "MImc_Bblezm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "91dce074-051f-4ad0-e2d4-355d5bcd9738"
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x5bcc4000 @  0x7f9ee5a6d1c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\r\n",
            "0.4.0\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xxMC6YMXfWBJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "d31ec47a-74a8-4a45-e816-451112223d97"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OfVj0nNQIbHq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gy23EsNYleoy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "247f004e-51a9-43ba-dd1a-51e5651653e6"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdatalab\u001b[0m/  \u001b[01;34mdrive\u001b[0m/\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ARofJoUUpLC7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41bef93d-d4db-4701-fdb4-579330e1c29d"
      },
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper parameters\n",
        "num_epochs = 5\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HItRVUx0Z-3m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear Model with numpy"
      ]
    },
    {
      "metadata": {
        "id": "1bZDU3OLPYvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4051
        },
        "outputId": "da2bff72-c6b5-4ac1-8d53-0f581cef2fc2"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_data = [1.0,2.0,3.0]\n",
        "y_data = [2.0,4.0,6.0]\n",
        "\n",
        "# w=1.0\n",
        "\n",
        "def forward(x):\n",
        "  return x*w\n",
        "\n",
        "def loss(x,y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "w_list=[]\n",
        "mse_list=[]\n",
        "\n",
        "for w in np.arange(0.0, 4.1, 0.1):\n",
        "  print(\"w=\",w)\n",
        "  l_sum=0\n",
        "  for x_val, y_val in zip(x_data,y_data):\n",
        "    y_pred_val = forward(x_val)\n",
        "    l = loss(x_val,y_val)\n",
        "    l_sum+=l\n",
        "    print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
        "  print(\"MSE=\",l_sum / 3)\n",
        "  w_list.append(w)\n",
        "  mse_list.append(l_sum / 3)\n",
        "  \n",
        "plt.plot(w_list, mse_list)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('w')\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w= 0.0\n",
            "\t 1.0 2.0 0.0 4.0\n",
            "\t 2.0 4.0 0.0 16.0\n",
            "\t 3.0 6.0 0.0 36.0\n",
            "MSE= 18.666666666666668\n",
            "w= 0.1\n",
            "\t 1.0 2.0 0.1 3.61\n",
            "\t 2.0 4.0 0.2 14.44\n",
            "\t 3.0 6.0 0.30000000000000004 32.49\n",
            "MSE= 16.846666666666668\n",
            "w= 0.2\n",
            "\t 1.0 2.0 0.2 3.24\n",
            "\t 2.0 4.0 0.4 12.96\n",
            "\t 3.0 6.0 0.6000000000000001 29.160000000000004\n",
            "MSE= 15.120000000000003\n",
            "w= 0.30000000000000004\n",
            "\t 1.0 2.0 0.30000000000000004 2.8899999999999997\n",
            "\t 2.0 4.0 0.6000000000000001 11.559999999999999\n",
            "\t 3.0 6.0 0.9000000000000001 26.009999999999998\n",
            "MSE= 13.486666666666665\n",
            "w= 0.4\n",
            "\t 1.0 2.0 0.4 2.5600000000000005\n",
            "\t 2.0 4.0 0.8 10.240000000000002\n",
            "\t 3.0 6.0 1.2000000000000002 23.04\n",
            "MSE= 11.946666666666667\n",
            "w= 0.5\n",
            "\t 1.0 2.0 0.5 2.25\n",
            "\t 2.0 4.0 1.0 9.0\n",
            "\t 3.0 6.0 1.5 20.25\n",
            "MSE= 10.5\n",
            "w= 0.6000000000000001\n",
            "\t 1.0 2.0 0.6000000000000001 1.9599999999999997\n",
            "\t 2.0 4.0 1.2000000000000002 7.839999999999999\n",
            "\t 3.0 6.0 1.8000000000000003 17.639999999999993\n",
            "MSE= 9.146666666666663\n",
            "w= 0.7000000000000001\n",
            "\t 1.0 2.0 0.7000000000000001 1.6899999999999995\n",
            "\t 2.0 4.0 1.4000000000000001 6.759999999999998\n",
            "\t 3.0 6.0 2.1 15.209999999999999\n",
            "MSE= 7.886666666666666\n",
            "w= 0.8\n",
            "\t 1.0 2.0 0.8 1.44\n",
            "\t 2.0 4.0 1.6 5.76\n",
            "\t 3.0 6.0 2.4000000000000004 12.959999999999997\n",
            "MSE= 6.719999999999999\n",
            "w= 0.9\n",
            "\t 1.0 2.0 0.9 1.2100000000000002\n",
            "\t 2.0 4.0 1.8 4.840000000000001\n",
            "\t 3.0 6.0 2.7 10.889999999999999\n",
            "MSE= 5.646666666666666\n",
            "w= 1.0\n",
            "\t 1.0 2.0 1.0 1.0\n",
            "\t 2.0 4.0 2.0 4.0\n",
            "\t 3.0 6.0 3.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 1.1\n",
            "\t 1.0 2.0 1.1 0.8099999999999998\n",
            "\t 2.0 4.0 2.2 3.2399999999999993\n",
            "\t 3.0 6.0 3.3000000000000003 7.289999999999998\n",
            "MSE= 3.779999999999999\n",
            "w= 1.2000000000000002\n",
            "\t 1.0 2.0 1.2000000000000002 0.6399999999999997\n",
            "\t 2.0 4.0 2.4000000000000004 2.5599999999999987\n",
            "\t 3.0 6.0 3.6000000000000005 5.759999999999997\n",
            "MSE= 2.986666666666665\n",
            "w= 1.3\n",
            "\t 1.0 2.0 1.3 0.48999999999999994\n",
            "\t 2.0 4.0 2.6 1.9599999999999997\n",
            "\t 3.0 6.0 3.9000000000000004 4.409999999999998\n",
            "MSE= 2.2866666666666657\n",
            "w= 1.4000000000000001\n",
            "\t 1.0 2.0 1.4000000000000001 0.3599999999999998\n",
            "\t 2.0 4.0 2.8000000000000003 1.4399999999999993\n",
            "\t 3.0 6.0 4.2 3.2399999999999993\n",
            "MSE= 1.6799999999999995\n",
            "w= 1.5\n",
            "\t 1.0 2.0 1.5 0.25\n",
            "\t 2.0 4.0 3.0 1.0\n",
            "\t 3.0 6.0 4.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 1.6\n",
            "\t 1.0 2.0 1.6 0.15999999999999992\n",
            "\t 2.0 4.0 3.2 0.6399999999999997\n",
            "\t 3.0 6.0 4.800000000000001 1.4399999999999984\n",
            "MSE= 0.746666666666666\n",
            "w= 1.7000000000000002\n",
            "\t 1.0 2.0 1.7000000000000002 0.0899999999999999\n",
            "\t 2.0 4.0 3.4000000000000004 0.3599999999999996\n",
            "\t 3.0 6.0 5.1000000000000005 0.809999999999999\n",
            "MSE= 0.4199999999999995\n",
            "w= 1.8\n",
            "\t 1.0 2.0 1.8 0.03999999999999998\n",
            "\t 2.0 4.0 3.6 0.15999999999999992\n",
            "\t 3.0 6.0 5.4 0.3599999999999996\n",
            "MSE= 0.1866666666666665\n",
            "w= 1.9000000000000001\n",
            "\t 1.0 2.0 1.9000000000000001 0.009999999999999974\n",
            "\t 2.0 4.0 3.8000000000000003 0.0399999999999999\n",
            "\t 3.0 6.0 5.7 0.0899999999999999\n",
            "MSE= 0.046666666666666586\n",
            "w= 2.0\n",
            "\t 1.0 2.0 2.0 0.0\n",
            "\t 2.0 4.0 4.0 0.0\n",
            "\t 3.0 6.0 6.0 0.0\n",
            "MSE= 0.0\n",
            "w= 2.1\n",
            "\t 1.0 2.0 2.1 0.010000000000000018\n",
            "\t 2.0 4.0 4.2 0.04000000000000007\n",
            "\t 3.0 6.0 6.300000000000001 0.09000000000000043\n",
            "MSE= 0.046666666666666835\n",
            "w= 2.2\n",
            "\t 1.0 2.0 2.2 0.04000000000000007\n",
            "\t 2.0 4.0 4.4 0.16000000000000028\n",
            "\t 3.0 6.0 6.6000000000000005 0.36000000000000065\n",
            "MSE= 0.18666666666666698\n",
            "w= 2.3000000000000003\n",
            "\t 1.0 2.0 2.3000000000000003 0.09000000000000016\n",
            "\t 2.0 4.0 4.6000000000000005 0.36000000000000065\n",
            "\t 3.0 6.0 6.9 0.8100000000000006\n",
            "MSE= 0.42000000000000054\n",
            "w= 2.4000000000000004\n",
            "\t 1.0 2.0 2.4000000000000004 0.16000000000000028\n",
            "\t 2.0 4.0 4.800000000000001 0.6400000000000011\n",
            "\t 3.0 6.0 7.200000000000001 1.4400000000000026\n",
            "MSE= 0.7466666666666679\n",
            "w= 2.5\n",
            "\t 1.0 2.0 2.5 0.25\n",
            "\t 2.0 4.0 5.0 1.0\n",
            "\t 3.0 6.0 7.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 2.6\n",
            "\t 1.0 2.0 2.6 0.3600000000000001\n",
            "\t 2.0 4.0 5.2 1.4400000000000004\n",
            "\t 3.0 6.0 7.800000000000001 3.2400000000000024\n",
            "MSE= 1.6800000000000008\n",
            "w= 2.7\n",
            "\t 1.0 2.0 2.7 0.49000000000000027\n",
            "\t 2.0 4.0 5.4 1.960000000000001\n",
            "\t 3.0 6.0 8.100000000000001 4.410000000000006\n",
            "MSE= 2.2866666666666693\n",
            "w= 2.8000000000000003\n",
            "\t 1.0 2.0 2.8000000000000003 0.6400000000000005\n",
            "\t 2.0 4.0 5.6000000000000005 2.560000000000002\n",
            "\t 3.0 6.0 8.4 5.760000000000002\n",
            "MSE= 2.986666666666668\n",
            "w= 2.9000000000000004\n",
            "\t 1.0 2.0 2.9000000000000004 0.8100000000000006\n",
            "\t 2.0 4.0 5.800000000000001 3.2400000000000024\n",
            "\t 3.0 6.0 8.700000000000001 7.290000000000005\n",
            "MSE= 3.780000000000003\n",
            "w= 3.0\n",
            "\t 1.0 2.0 3.0 1.0\n",
            "\t 2.0 4.0 6.0 4.0\n",
            "\t 3.0 6.0 9.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 3.1\n",
            "\t 1.0 2.0 3.1 1.2100000000000002\n",
            "\t 2.0 4.0 6.2 4.840000000000001\n",
            "\t 3.0 6.0 9.3 10.890000000000004\n",
            "MSE= 5.646666666666668\n",
            "w= 3.2\n",
            "\t 1.0 2.0 3.2 1.4400000000000004\n",
            "\t 2.0 4.0 6.4 5.760000000000002\n",
            "\t 3.0 6.0 9.600000000000001 12.96000000000001\n",
            "MSE= 6.720000000000003\n",
            "w= 3.3000000000000003\n",
            "\t 1.0 2.0 3.3000000000000003 1.6900000000000006\n",
            "\t 2.0 4.0 6.6000000000000005 6.7600000000000025\n",
            "\t 3.0 6.0 9.9 15.210000000000003\n",
            "MSE= 7.886666666666668\n",
            "w= 3.4000000000000004\n",
            "\t 1.0 2.0 3.4000000000000004 1.960000000000001\n",
            "\t 2.0 4.0 6.800000000000001 7.840000000000004\n",
            "\t 3.0 6.0 10.200000000000001 17.640000000000008\n",
            "MSE= 9.14666666666667\n",
            "w= 3.5\n",
            "\t 1.0 2.0 3.5 2.25\n",
            "\t 2.0 4.0 7.0 9.0\n",
            "\t 3.0 6.0 10.5 20.25\n",
            "MSE= 10.5\n",
            "w= 3.6\n",
            "\t 1.0 2.0 3.6 2.5600000000000005\n",
            "\t 2.0 4.0 7.2 10.240000000000002\n",
            "\t 3.0 6.0 10.8 23.040000000000006\n",
            "MSE= 11.94666666666667\n",
            "w= 3.7\n",
            "\t 1.0 2.0 3.7 2.8900000000000006\n",
            "\t 2.0 4.0 7.4 11.560000000000002\n",
            "\t 3.0 6.0 11.100000000000001 26.010000000000016\n",
            "MSE= 13.486666666666673\n",
            "w= 3.8000000000000003\n",
            "\t 1.0 2.0 3.8000000000000003 3.240000000000001\n",
            "\t 2.0 4.0 7.6000000000000005 12.960000000000004\n",
            "\t 3.0 6.0 11.4 29.160000000000004\n",
            "MSE= 15.120000000000005\n",
            "w= 3.9000000000000004\n",
            "\t 1.0 2.0 3.9000000000000004 3.610000000000001\n",
            "\t 2.0 4.0 7.800000000000001 14.440000000000005\n",
            "\t 3.0 6.0 11.700000000000001 32.49000000000001\n",
            "MSE= 16.84666666666667\n",
            "w= 4.0\n",
            "\t 1.0 2.0 4.0 4.0\n",
            "\t 2.0 4.0 8.0 16.0\n",
            "\t 3.0 6.0 12.0 36.0\n",
            "MSE= 18.666666666666668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFYCAYAAABKymUhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3Xl8VOWhPvDnzEwm+56ZrGQhgSQk\nhCUJEMIawxYRwcqiF6y9tmqVar3aWq1WWtRWf7ZavdaFWnsFrYgLKoogm2whhBBIwhYSIGTPZF8n\nk5k5vz9ioiiEQGbmzJk838/Hj2ROMvO8Hskz58w57yuIoiiCiIiIZE8hdQAiIiKyDJY6ERGRg2Cp\nExEROQiWOhERkYNgqRMRETkIljoREZGDUEkdYKh0ujaLPp+vrxuamjot+pxS4Vjsj6OMA+BY7JGj\njAPgWAai0XhecRuP1H9ApVJKHcFiOBb74yjjADgWe+Qo4wA4luvFUiciInIQLHUiIiIHwVInIiJy\nECx1IiIiB8FSJyIichAsdSIiIgfBUiciInIQLHUiIiIHwVInIiJyECx1IiIiByH7ud8tqVLXjqom\nPUJ8XaSOQkREDuDMxSa0GczwVNvmGJpH6t/zyb7z+MO6bDS1dUsdhYiIZK69qwd/3Xgcb39+wmav\nyVL/noQoP5jNIg4WVUsdhYiIZC7nZC2MJjMmxmlt9pos9e+ZHK+FWqXA/oJqiKIodRwiIpKxfQVV\nUCoEzEoOs9lrstS/x83FCVOTQlDb1IWzFS1SxyEiIpkqq2nDxdp2JEX7w9fTdtdpsdR/IHNSOABg\nfwFPwRMR0fXZX9jbIdOTQmz6uiz1HxgbHYAAbxfknq5DV7dR6jhERCQzPUYTDp2ogbe7GmOj/Wz6\n2iz1H1AoBEwbG4zuHhOOnK6TOg4REclM/tl6dOiNmJoYBKXCtjXLUr+MqWODIADYV8hT8EREdG36\nPr6dlhRs89dmqV9GgLcrxkT6oqSiBdUNHVLHISIimWhs1ePE+UbEhHoj2N/d5q/PUr+Cad9e3LCf\nR+tERDRIBwqrIUKao3SApX5FE0cHwM1ZhYOFNTCZzVLHISIiO2cWRewvrIbaSYFUG044830s9Stw\nUikxJSEQLR0GFJ5rlDoOERHZueKLzdA165Eap4WrszRLq7DUB9B3fyHvWScioqvZVyDNvenfx1If\nQHigB0ZoPXC8pB6tHQap4xARkZ3q1BuRd6YOWl9XjArzliyHVUu9uLgYmZmZ2LBhAwDggQcewKpV\nq7Bq1SrcdNNNePLJJy/5/o8//hgzZ87s/57XXnvNmvGuShAETEsKhsksIvtEjaRZiIjIfh0+XQuD\n0YzpScEQBEGyHFY76d/Z2Ym1a9ciLS2t/7GXX365/8+PPfYYli5d+qOfy8rKwqOPPmqtWNcsLSEI\nm3aXYF9BNeamjpB0ZxERkX3aX1ANQQCmJkpz1Xsfqx2pq9VqrFu3Dlrtj68APHfuHNra2pCUlGSt\nl7cYD1cnjB+lQVV9B85Xt0kdh4iI7Eylrh3nqloxdqQ/fD2dJc1itSN1lUoFleryT//OO+9g5cqV\nl912+PBh3HXXXTAajXj00UcxZsyYAV/H19cNKpVyyHm/T6PxvOTrhdNH4sjpOhw5W4/J40It+lrW\n9sOxyJmjjMVRxgFwLPbIUcYByGcsn2WXAQCypo28YmZbjcXm19wbDAbk5eVhzZo1P9o2btw4+Pn5\nYdasWcjPz8ejjz6Kzz//fMDna2rqtGg+jcYTOt2lR+Rhvq7w9XTGN0fLcfPUCDg7WfZNhLVcbixy\n5ShjcZRxAByLPXKUcQDyGYvRZMbO3IvwcHXCSK37ZTNbeiwDvUGw+dXvubm5VzztHh0djVmzZgEA\nJkyYgMbGRphMJhumuzyFQkD62CB0dZtw9IxO6jhERGQnjpc0oK2zB2kJQVAppb+hzOYJCgsLERcX\nd9lt69atw5YtWwD0Xjnv5+cHpdI+jorTx/Ze/LCvoEriJEREZC/2f9sJ0yWaFvaHrHb6vaioCM89\n9xwqKyuhUqmwbds2vPLKK9DpdAgPD7/ke3/5y1/itddew0033YTf/OY3eP/992E0GvHMM89YK941\nC/R1Q+wIH5y+2Iy65i5ofVyljkRERBJqautGwbkGRAZ5IkzrIXUcAFYs9cTERKxfv/5Hj//w3nQA\n/fejBwUFXfZn7MW0pGCcKW/GgYJqLJkxUuo4REQkoewTNRBF+zlKBzij3DVJidXCRa3EgaJqmM2i\n1HGIiEgioihiX0E1nFQKTB4TKHWcfiz1a+CsVmJSfCAaW7txsoyLvBARDVcllS2obexE8mgN3Fyc\npI7Tj6V+jfpOs3CRFyKi4atv8Rap1k2/Epb6NRoZ4oVgfzccLdahvatH6jhERGRjeoMRuafq4O/l\ngrgIX6njXIKlfo0EQcD0pBAYTSIOFvJonYhouDl0shbdPSZMTwqGws7WA2GpX4f0sUFQKQXsOVYF\nUeQFc0REw8k3+VVQCAKmj5Nu3fQrYalfB083NVLitKhp7MSZi81SxyEiIhs5X92Ksto2jIuRfvGW\ny2GpX6dZ43sXdtlzrFLiJEREZCt78nt/58+aYJ+Le7HUr9OoMG8E+7sh74wOrZ0GqeMQEZGVdeqN\nyDlViwBvFyRE+Ukd57JY6tdJEATMmhAKk1nEAV4wR0Tk8A6drIGhx4wZ40Ls7gK5Piz1IZiaGAQn\nlQLf5FfBzAvmiIgcliiK2JNfCaVCsKtpYX+IpT4E7i5OmBSnRV1zF06VNUkdh4iIrKS0qhUVug5M\nGBUAbw/7u0CuD0t9iPoulvgmnxfMERE5qr7f8TPt9AK5Piz1IRoZ4oUwjQfyz9ajpb1b6jhERGRh\nHfoeHD5dB62PK+LtbAa5H2KpD1HvBXMhMJnF/rmAiYjIcRwsrEGP0YyZE+z3Ark+LHULmDImCGon\nBfYer+KSrEREDkQURew5VgmVUkD6WPu9QK4PS90C3FxUmDImEPUtehSd55KsRESO4mxFC6obOjFx\ntAZebmqp41wVS91CZn47w9w3nGGOiMhh9M0gN9vOL5Drw1K3kKhgL0QEeuJ4SQMaW/VSxyEioiFq\n6zTgyJk6BPu7YfQIH6njDApL3YJmTQiBWeQFc0REjuBAYQ2MJhEzx4VAsPML5Pqw1C1o8phAuKiV\n2Hu8CiazWeo4RER0nURRxDfHKqFSKjBVBhfI9WGpW5CLWoUpCUFoautGYSkvmCMikqvTZU2obepC\napwWHq5OUscZNJa6hc0aHwKAS7ISEcnZnmNVAHo/VpUTlrqFhQd6YmSIFwpLG1Df0iV1HCIiukYt\nHQYcLdYhNMAdMaHeUse5Jix1K5g5PgQigL3HecEcEZHc7C+ogsksYuZ4+Vwg14elbgWT4gPh6qzC\nvuNVMJp4wRwRkVyYRRF7j1dBrVJgamKQ1HGuGUvdCpydlJiaGISWDgOOl9RLHYeIiAbp5IVG6Jr1\nmBQfCDcX+Vwg18eqpV5cXIzMzExs2LABAPC73/0ON910E1atWoVVq1Zhz549P/qZZ599FsuXL8eK\nFStQUFBgzXhW9d0Fc1USJyEiosH6Jr/3d/ZMmV0g10dlrSfu7OzE2rVrkZaWdsnj//M//4PZs2df\n9mcOHz6MsrIybNy4EaWlpXj88cexceNGa0W0qlCNB0aFeePE+UbUNXVC6+smdSQiIhpAU1s38s/W\nI1zrgZHBXlLHuS5WO1JXq9VYt24dtFrtoH8mOzsbmZmZAIDo6Gi0tLSgvb3dWhGtbta3cwXvOsrb\n24iI7N03xyphFkXMmhAquwvk+lit1FUqFVxcXH70+IYNG3DHHXfgoYceQmPjpRO01NfXw9f3uwXo\n/fz8oNPprBXR6lJitfByV2N/QTW6DSap4xAR0RUYTWbsOVYFV2cV0hLkd4FcH6udfr+cm2++GT4+\nPoiPj8ebb76J//3f/8Uf/vCHK36/KF59bXJfXzeoVEpLxoRG42mx58qaGoX3vz6DwovNWJAWabHn\nHSxLjkVqjjIWRxkHwLHYI0cZB2DbsezJK0drhwGLZ0YjLNTyi7fYaiw2LfXvf76ekZGBNWvWXLJd\nq9Wivv67q8Xr6uqg0WgGfM6mpk6LZtRoPKHTtVns+VJHB2DTzmJ8uqcEydF+Nj2lY+mxSMlRxuIo\n4wA4FnvkKOMAbD+WT/aUQAAwJV5r8de19FgGeoNg01vafvWrX6G8vBwAkJOTg1GjRl2yPT09Hdu2\nbQMAnDhxAlqtFh4eHraMaHG+ns5IjtWgsr4Dpy82Sx2HiIh+4FxVK85VtSIp2h9aH1ep4wyJ1Y7U\ni4qK8Nxzz6GyshIqlQrbtm3DypUr8etf/xqurq5wc3PDn//8ZwDAQw89hD//+c+YOHEiEhISsGLF\nCgiCgKeeespa8WwqM3kEDp+qw868CsRH+F79B4iIyGZ25vUebN6QEiZxkqGzWqknJiZi/fr1P3p8\n3rx5P3rsxRdf7P/zI488Yq1IkokO9UJEkCfyz+pQ39KFAG95vxMkInIULR0GHD5Vh2B/NyRE+kkd\nZ8g4o5wNCIKAzOQwiCKwm7e3ERHZjW+OVcJkFpExMUy2t7F9H0vdRibF967Ju/d4FQw9vL2NiEhq\nRpMZu/Mr4aJWynKe98thqduIk0qJmeND0KE34tDJWqnjEBENe3lndGhpN2Da2GC4Otv0ZjCrYanb\n0OwJoVAIAnbmVQzqHnwiIrKenXkVAICMZPlfINeHpW5Dfl4umBirQXldO4rLeXsbEZFUymraUFLZ\ngrEj/RHk5zhrc7DUbSzz23eEfe8QiYjI9nb03cbmQEfpAEvd5kaFeWOE1gNHi+vR2KqXOg4R0bDT\n2mlAzsk6BPq6InGk/G9j+z6Wuo0JgoAbksNgFkXszuftbUREtrb3WBWMJjMyJoZB4QC3sX0fS10C\nU8YEwt1FhW+OVaHHyNvbiIhsxWTuvY3N2UmJ9LHBUsexOJa6BNROSswYH4L2rh4cPlUndRwiomEj\nv7geTW3dSB8bBDcXx7iN7ftY6hKZPSEUggDsOMLb24iIbGXHtxcpO9oFcn1Y6hIJ8HbFhFEalNW2\nobSyVeo4REQO72JtG4rLm5EQ6Ytgf3ep41gFS11Cfe8U+26tICIi69nZf5Q+QuIk1sNSl1BcuA9C\nNe7IO6NDU1u31HGIiBxWe1cPDp2sRYC3C5Ki/aWOYzUsdQn13d5mMov45hhvbyMispZ9x6vQYzTj\nhuQwKBSOdRvb97HUJZY2JghuzirsOdb7PxwREVmWyWzGrqMVUDspMD3J8W5j+z6WusSc1UpMHxeM\n1g4Dck9z9TYiIks7drYBDa3dmJoQBDcXJ6njWBVL3Q7cMDEMggBsP1zO29uIiCxsW+5FAMANKY57\ngVwflrodCPBxRUqsFhfr2nGqrEnqOEREDqOksgUlFS1IivZHaIBj3sb2fSx1OzF/cjgA4KvDFyVO\nQkTkOLZ9+zt13qRwiZPYBkvdTkQFe2H0CB8UnWtEha5d6jhERLJX19SJo2d0iAj0RFy4j9RxbIKl\nbkfmf/tOchuP1omIhmx7bjlEAPMmj4DgYKuxXQlL3Y4kxfgjyM8Nh07UcjIaIqIhaO/qwf7Cavh7\nOSMlVit1HJthqdsRhSBg3qQRMJnF/ukMiYjo2u3Or4Shx4w5KSOgUg6fqhs+I5WJqYlB8HJzwp78\nSugNRqnjEBHJTo/RhJ15FXB1VmH6uBCp49gUS93OOKmUyJgYhs5uI/YVVEsdh4hIdrJP1KK1w4CZ\n40Pg6ux4a6YPhKVuh2ZPDIWTSoGvc8thMnPqWCKiwTKLIrYdvgilQkCmg66ZPhCWuh3ydFNj2thg\n1LfokXdGJ3UcIiLZKCxtQHVDJybFB8LPy0XqODZn1VIvLi5GZmYmNmzYAACorq7GnXfeiZUrV+LO\nO++ETndpYeXk5GDKlClYtWoVVq1ahbVr11oznl2bmzoCAnpvb+PUsUREg/PdZDOOPyXs5Vjtw4bO\nzk6sXbsWaWlp/Y+99NJLWLZsGbKysvDuu+/i7bffxm9/+9tLfm7SpEl4+eWXrRVLNgL93DBhtAZH\ni3UoLm9GbLiv1JGIiOzahZpWnL7YjIRIX4QHekodRxJWO1JXq9VYt24dtNrv7g986qmnMG/ePACA\nr68vmpubrfXyDuG7yWjKJU5CRGT/+n5Xzps8PKaEvRyrHamrVCqoVJc+vZubGwDAZDLhvffew/33\n3/+jnyspKcG9996LlpYWrF69Gunp6QO+jq+vG1QqpeWCA9Bo7OMdnkbjibh953CspB56MzDiOt55\n2stYLMFRxuIo4wA4FnvkKOMArm0sdY2dyD1dh8hgL8xKjbC7GeRstV9sfq2/yWTCb3/7W0yZMuWS\nU/MAEBkZidWrV2PBggUoLy/HHXfcge3bt0OtVl/x+ZqaOi2aT6PxhE7XZtHnHIqMCaE4XdaEjdtP\n46fz467pZ+1tLEPhKGNxlHEAHIs9cpRxANc+lo07z8JsFnHDxFDU19vX+hmW3i8DvUGw+dXvjz32\nGCIiIrB69eofbQsMDERWVhYEQUB4eDgCAgJQW1tr64h2ZeJoDTQ+LjhQWIPWDoPUcYiI7E6nvgff\nHK+Cj4cak8cESh1HUjYt9c8++wxOTk544IEHrrj9rbfeAgDodDo0NDQgMHCY7yCFgLmp4TCazNh1\nlFPHEhH90DfHq9BtMCFzmE0JezlWO/1eVFSE5557DpWVlVCpVNi2bRsaGhrg7OyMVatWAQCio6Ox\nZs0aPPTQQ/jzn/+MjIwMPPLII9i5cyd6enqwZs2aAU+9DxfTxgZj875z2HW0EgumRMDZybLXEBAR\nyZXRZMaOIxVwVisxa/zwmhL2cqxW6omJiVi/fv2gvvfFF1/s//Prr79urUiy5axWYvbEMGw5eAEH\nC6sxe+LwmyWJiOhyDp/qXdVyTsoIuLk4SR1HcsP7PIWM3JAcBpVSwLbccpjNnIyGiEgURXyVUw6F\nIGBOCg92AJa6bHi7qzE1MQh1TV3IP8upY4mITl5oQoWuHSlxGgT4uEodxy6w1GVkbmrvhApfHuLU\nsUREXx4qAwDMmzR8J5v5IZa6jIQEuCN5tAbnq1tx8kKT1HGIiCRTUtmCU2VNSIjyQ1Swl9Rx7AZL\nXWZunBoBANhy8IK0QYiIJNT3O3BhWoS0QewMS11mIoO8kDjSD2fKm3G2gnPnE9HwU1bThoLSBowK\n8+ZiVz/AUpehhWmRAIAtB8ukDUJEJIEvsi8AAG6aGillDLvEUpeh0SN8EDvCB4XnGnChplXqOERE\nNlNV34G8MzpEBHkiIcpP6jh2h6UuUwu/fYf6BY/WiWgY+fJQGUT0nrG0t5XY7AFLXabGRPoiKtgT\necU6VNZ3SB2HiMjq6pq7cOhELUID3DFhdIDUcewSS12mBEHo/2z9y+wLUkYhIrKJrw6VwSyKuDEt\nAgoepV8WS13Gxo0KQJjGHYdO1qLOwuvKExHZk6a2buwvrIbWxxWp8Vqp49gtlrqMKQQBN6ZFQhR7\nZ5kjInJU2w5fhNEkIistAkoFq+tK+F9G5lLjtAj0dcWBwmo0tuqljkNEZHGtnQbsya+En5czpiYG\nSR3HrrHUZU6hEJCVFgGTWcRXh3m0TkSO5+vcchiMZsyfFA6VkrU1EP7XcQBpCUHw93LG3mNVaO0w\nSB2HiMhiOvU92HW0Al5uTpgxLkTqOHaPpe4AVEoF5k+OgMFoxvbccqnjEBFZzM68CnR1mzBvUjjU\nTkqp49g9lrqDmJ4UDC93NXYdrUCHvkfqOEREQ6Y3GPH1kQq4u6gwa0Ko1HFkgaXuINROSsyfFA69\nwYSdeRVSxyEiGrJvjlWhvasHmSkj4OqskjqOLLDUHcisCSFwd1Hh69xy6A1GqeMQEV03Q48JX+Vc\nhLNaiRuSw6SOIxssdQfiolZhTsoIdOiN2JNfJXUcIqLrtiP3Ilo6DMiYEAoPVyep48gGS93B3JAS\nBhe1EtsOX4ShxyR1HCKia2Y0mfHRrrNwUikwd1K41HFkhaXuYNxdnJAxMQwtHQZ8ncMV3IhIfnJO\n1qKuqQszxoXA210tdRxZYak7oLmpI6BWKfDhrrPoMfJonYjkw2gy4/ODF6BSClgwmUfp14ql7oC8\n3NXISA5DfYsee47xs3Uiko+DRTWoa+rC3MkR8PNykTqO7LDUHdSCyeFwdVbii+wydBt4tE5E9q/H\naMbnB87DSaXAsszRUseRJZa6g/J0U2PRjGi0dhiw6yjvWyci+7f3eBUaWrsxe0Io/L1dpY4jS1Yt\n9eLiYmRmZmLDhg0AgOrqaqxatQq33347HnzwQRgMP56n/Nlnn8Xy5cuxYsUKFBQUWDOew1s8MwZu\nzip8eagMXd28b52I7Jehx4Qt2Rfg7KRE1pQIqePIltVKvbOzE2vXrkVaWlr/Yy+//DJuv/12vPfe\ne4iIiMCHH354yc8cPnwYZWVl2LhxI5555hk888wz1oo3LHi4OmHe5HB06I34+gjnhCci+7U7vxIt\n7QZkpoTBi1e8Xzerlbparca6deug1Wr7H8vJycENN9wAAJg9ezays7Mv+Zns7GxkZmYCAKKjo9HS\n0oL29nZrRRwWMpPD4OHqhG2HL6K9i3PCE5H90RuM+CK7DK7OSszjfelDYrVSV6lUcHG59MrFrq4u\nqNW978D8/f2h0+ku2V5fXw9fX9/+r/38/H70PXRtXJ1VyJoSga5uE7ZxvXUiskM7jlSgvasH81LD\nOXvcEA1qhvyioiLodDrMnj0bL774Io4dO4Zf/epXSElJue4XFkXRIt/j6+sGlcqyy/FpNJ4WfT4p\naTSeWDo3FjvyyrEzrwIr5sXDx9NZ6ljXxVH2i6OMA+BY7JHcxtHe1YNtueXwdHPCbQvi4ebyXanL\nbSwDsdVYBlXqTz/9NP7yl7/gyJEjKCwsxJNPPok//elPeOedd67pxdzc3KDX6+Hi4oLa2tpLTs0D\ngFarRX19ff/XdXV10Gg0Az5nU1PnNWW4Go3GEzpdm0WfUyrfH8uCyRF49+tirP/iBFbcMEriZNfO\nUfaLo4wD4FjskRzHsXnfOXR09WDprGh0tOnR0aYHIM+xXImlxzLQG4RBnX53dnZGZGQkdu7ciWXL\nliEmJgYKxbWfuZ86dSq2bdsGANi+fTumT59+yfb09PT+7SdOnIBWq4WHh8c1vw792IxxIfDzcsbu\n/Eo0tXVLHYeICG2dBmzPLYeXW+/01jR0g2rmrq4ubN26FTt27MC0adPQ3NyM1tbWAX+mqKgIq1at\nwieffIJ33nkHq1atwurVq7F582bcfvvtaG5uxuLFiwEADz30EPR6PSZOnIiEhASsWLECTz/9NJ56\n6qmhj5AAAE4qBRalR6HHaMYX2RekjkNEhK9yLkJvMOHGtEg4qy37MepwJYiD+OD60KFDeOedd7Bw\n4UJkZWXhlVdeQUREBBYtWmSLjAOy9OkZRz7lYzSZ8cS6HDS06vHne6YgQEaTOzjKfnGUcQAciz2S\n0zha2rvx6OvZcHd1wl/umQKnH1wbJaexXI0tT78P6jP1KVOmIDExER4eHqivr0daWhomTpxosYBk\nGyqlAjdPi8K6LSfx+YEL+FlWvNSRiGiY+uJQGQxGM1ZMjfxRodP1G9Tp97Vr12Lr1q1obm7GihUr\nsGHDBqxZs8bK0cgaJo8JRLC/Gw4U1qC20bIXGRIRDUZjqx578isR4O2CaUnBUsdxKIMq9ZMnT2Lp\n0qXYunUrlixZgpdeegllZVyrW44UCgGLp4+EWRTx6YHzUschomFoy8ELMJpELEqPgkrJJUgsaVD/\nNfs+dt+zZw8yMjIA4LLztpM8JMdqMELrgZwTtajUccY+IrKduuYu7CuoRqCfG9ISA6WO43AGVepR\nUVHIyspCR0cH4uPjsXnzZnh7e1s7G1mJQhCwZPpIiAA+3c+jdSKync/3n4fJLGLxtCgor+PWaBrY\noCefKS4uRnR0NAAgJiYGzz//vFWDkXWNi/FHVLAXjpzRoaymDRFBjjNzExHZp+qGDhw8UYNQjTtS\n47VX/wG6ZoN6m6TX67Fr1y488MAD+OUvf4kDBw70z+FO8iQIApbMiAIAfLLvnMRpiGg4+HT/eYgi\nsHjaSCgEQeo4DmlQpf7kk0+ivb0dK1aswLJly1BfX48nnnjC2tnIyhIi/RA7wgcFpQ04XdYkdRwi\ncmDnq1tx+FQdIoM8MXF0gNRxHNagSr2+vh6PPvooZs2ahdmzZ+P3v/89amtrrZ2NrEwQBCzLiAEA\nbNxdAvMgFtAhIrpWoihi464SAMDyjBgIPEq3mkFPE9vV1dX/dWdnJ7q7OX+4I4gK9sLkMYEoq2lD\nzkm+USMiyzt2th7F5c0YHxOA2HDfq/8AXbdBXSi3fPlyLFiwAImJiQB6F1t58MEHrRqMbOcnM0Yi\n70wdPv6mFCmxGs7uREQWYzSZsWlPKRSCgKWzo6WO4/AGdaR+66234j//+Q8WL16MJUuW4P3330dJ\nSYm1s5GNBPi4IjN5BBpau7HjSIXUcYjIgew9XoWaxk7MHB+CYH93qeM4vEEdqQNAcHAwgoO/m86v\noKDAKoFIGjdOjcC+gipsyb6AaUnB8HTj3Q1ENDRd3UZ8uv88nNVKLJoWJXWcYeG67/wfxOJuJCPu\nLk64KT0KXd0mfH7ggtRxiMgBfHmoDG2dPciaEgFvdx4o2MJ1lzqvXnQ8GRNDofVxxe78Si72QkRD\n0tiqx/bccvh6OmNu6gip4wwbA55+nzlz5mXLWxRFNDXxvmZHo1Iq8JNZ0XhtcxE+/KYU9y8ZK3Uk\nIpKpT/aeQ4/RjCXTR8LZiRff2sqApf7ee+/ZKgfZiZRYDaJDvJB3RoezFc0YFeYjdSQikpmymjYc\nLKpBmMYDUxODpI4zrAxY6qGhobbKQXZCEAQszxiFZzfk4YNdJXh8VTI/aiGiQRNFER/sLoGI3olm\nFAr+/rAlLpFDPxIT5o3kWA1Kq1px5IxO6jhEJCOF5xpxqqwJiVF+SIjykzrOsMNSp8u6dVY0lAoB\nH+4pQY/RLHUcIpIBk9mMTbtLIAjAstkxUscZlljqdFmBvm6YPSEUumY9dudXSh2HiGTgQGENKus7\nkD42GGFaD6njDEssdbqim9LYymX8AAAgAElEQVQj4eqswucHzqND3yN1HCKyY3qDEZ/sPQe1kwJL\npo+UOs6wxVKnK/J0U2NhWgQ69EZsOXhB6jhEZMe+yrmIlg4D5qWGw9fTWeo4wxZLnQaUmRIGfy9n\n7MyrgK656+o/QETDTnN7N746fBFe7mrMnxwudZxhjaVOA3JSKXHLzGgYTSI++qZU6jhEZIc27zsH\nQ48Zi6dHwdV50EuKkBWw1OmqJo8JRGSQJw6fqkNxebPUcYjIjpTVtGHf8WoE+7thelLw1X+ArIql\nTlelEATcPmc0AGDD9mKYzLzFjYgAsyhiw/YzEAHcPmc0lApWitRsep5k06ZN+Oyzz/q/LioqQn5+\nfv/XCQkJmDhxYv/X//73v6FUcs5gexAT6o30sUE4UFiD3UcrkZnCBRqIhrsDhdUorWpFSqwGCZGc\naMYe2LTUly5diqVLlwIADh8+jK1bt16y3cPDA+vXr7dlJLoGS2fF4GhxPT7Zdx6p8YFcSpFoGOvQ\n9+DDPaVQOymw4oZRUsehb0l2ruTVV1/FfffdJ9XL03XwcldjyfQodHUb8dEeXjRHNJxt3ncebZ09\nuGlqJPy8XKSOQ9+SpNQLCgoQHBwMjUZzyeMGgwEPP/wwVqxYgbfffluKaHQVsyeGIkzjgf2F1Sip\nbJE6DhFJ4GJtG3YdrUCgryvmpvIWNnsiiKIo2vpF//CHP+DGG2/E5MmTL3n8P//5DxYtWgRBELBy\n5Ur88Y9/xNixA6/pbTSaoFLxc3dbOnGuAb97dT+iw7zx1wdnQslVmIiGDVEU8btX9+Pk+Ub88Rdp\nmBinlToSfY8kNxTm5OTgiSee+NHjt912W/+fp0yZguLi4quWelNTp0WzaTSe0OnaLPqcUrHWWLSe\naqQlBCH7RA0++vo0Zk8Ms/hr/JCj7BdHGQfAsdgjW4zjYFE1Tp5vxMTRGozwd7Xa6znKPgEsPxaN\nxvOK22x++r22thbu7u5Qqy+9yOrcuXN4+OGHIYoijEYjjh49ilGjePGFvVo6OxouaiU+3nsObZ0G\nqeMQkQ106o34YHcp1CoFVtzAVdjskc1LXafTwc/vu1sf3nzzTeTn52PkyJEICgrCrbfeittuuw0z\nZ85EUlKSrePRIPl4OGPxtCh06I2caY5omPh0/3m0dhhwY1oEArxdpY5Dl2Hz0++JiYn45z//2f/1\n3Xff3f/n3/zmN7aOQ0OQkRyGfQXV2He8GjPGhWJkiJfUkYjISirq2rEzrwJaH1fO727HOP0PXTeV\nUoGVc0dDBLBh+xmYzTa/5pKIbEAURWz4uhhmUcTtc0bBiRcn2y2WOg1JbLgvJo8JxIWaNuwtqJI6\nDhFZQc7JWhSXN2N8TACSogOkjkMDYKnTkC2bHQNntRIf7SlFe1eP1HGIyIK6uo3YuLsETioFbsvk\nxcv2jqVOQ+br6Yyb03svmvt47zmp4xCRBX1+4AJa2g3ImhIBjQ8vjrN3LHWyiMyUMAT7u+Gb/Epc\nqGmVOg4RWUBlfQe+PlKOAG8XLODFcbLAUieLUCkV+K85fRfN9V5QQ0TyJYoi3vu6GCaziNsyR0Ht\nxIvj5IClThYzJtIPqXFanKtqxZ78SqnjENEQHCyqwamyJiRF+2N8DC+OkwuWOlnUbZmj4OaswqY9\npWho0Usdh4iuQ0uHAe/vPAtnJyVWzh0NQeD6DnLBUieL8vFwxvIbYtBtMOGdbWcgwXpBRDRE735d\njA69EbfOiubMcTLDUieLmzY2GGMifVF4rgGHTtRKHYeIrkHeGR2OnK5DTJg3Zk8MlToOXSOWOlmc\nIAj46fw4qJ0UeG9HMVo7uOALkRx06HuwYfsZqJQCfrYgDgqedpcdljpZhcbHFT+ZEY0OvRHv7SiW\nOg4RDcIHu0rQ0mHAovQoBPu7Sx2HrgNLnazmhuQwRId64fCpOuQX66SOQ0QDOHmhEfsKqhGu9eCC\nLTLGUierUSgE3LkgHiqlgPXbz6BTzylkiexRt8GEf289DYUg4GdZ8VApWQ1yxT1HVhUa4I6bpkai\nud2AD3aXSB2HiC7j473nUN+ix/zJ4YgI8pQ6Dg0BS52sbsGUCIRpPLD3eDVOXmiUOg4RfU9pZQt2\nHClHoJ8bFqVHSh2HhoilTlanUirws6w4CALwf1+dRrfBJHUkIgLQYzTj7a2nIQL42YI4TgXrAFjq\nZBNRwV6YNykcumY9PtnHldyI7MEX2RdQVd+B2RNDMXqEj9RxyAJY6mQzN0+LgtbXFV8fKUdpVYvU\ncYiGtfK6dnyRXQY/L2fcOjNa6jhkISx1shlnJyV+tiAOogj8+8vTMJrMUkciGpZMZjPe/vIUTGYR\nd8yLg6uzSupIZCEsdbKp2HBfzJoQisr6DnyRXSZ1HKJh6evcClyoaUNaQhCSov2ljkMWxFInm1s6\nKxq+ns7YcvACLta2SR2HaFipaezE5n3n4OnmhNsyR0kdhyyMpU425+qswk/nx8JkFrHu85Mw9PBq\neCJbMJrMeOOzEzAYzVg5NxYerk5SRyILY6mTJJKiAzB7Yu9p+E17SqWOQzQsfLr/PMpq2pCeGITU\nOK3UccgKWOokmWWzYxDs74adeRUoKK2XOg6RQztzsQlfZpdB4+OC2+eMljoOWQlLnSTj7KTEPYsS\noFIK+NcXp7hEK5GVdOh7sG7LSQiCgF/clMCr3R0YS50kFR7oiZ/MjEZrZw/+9eUpiKIodSQihyKK\nIt756gwaW7uxKD0SMaHeUkciK2Kpk+TmpI7AmEhfFJQ2YHd+pdRxiBzKwaIa5J6uQ0yoN26cGiF1\nHLIym5Z6Tk4OpkyZglWrVmHVqlVYu3btJdsPHjyIW2+9FcuXL8err75qy2gkIYUg4K4bx8DdRYWN\nu0pQWd8hdSQih1DX1IkNXxfDRa3EL24aA6WCx3GOzuYfrEyaNAkvv/zyZbc9/fTTeOuttxAYGIiV\nK1di3rx5iImJsXFCkoKvpzPuXBCPVz8pxJufncATd6TAScVfQETXy2Q2Y93nJ9FtMOEXC8dA4+Mq\ndSSyAbv5rVleXg5vb28EBwdDoVBg5syZyM7OljoW2VByrAYzxoWgvK4dH+/lbW5EQ/H5gQsorWrF\n5DGBmJIQKHUcshGbH6mXlJTg3nvvRUtLC1avXo309HQAgE6ng5+fX//3+fn5oby8/KrP5+vrBpXK\nsssFajSeFn0+KcltLL9aPgGlVS3Ydrgc0yaEYfzo7+6lldtYrsRRxgFwLPZIo/HEqfON2HLwArS+\nrvj17cmynWTGUfYJYLux2LTUIyMjsXr1aixYsADl5eW44447sH37dqjV6ut+zqamTgsm7P0Pr9M5\nxtSlch3Lf2fF49n1eXjh3TysvWsyPFydZDuWH3KUcQAciz3SaDxRVt6E59fnQkTv36Wudj262vVS\nR7tmjrJPAMuPZaA3CDY9/R4YGIisrCwIgoDw8HAEBASgtrYWAKDValFf/90EJLW1tdBqOePRcBQV\n7IXF06PQ0m7Av7ee5m1uRNfg3a/PoL5FjxvTIrlG+jBk01L/7LPP8NZbbwHoPd3e0NCAwMDez3rC\nwsLQ3t6OiooKGI1G7N69u//UPA0/CyZHIHaED44W67CvoFrqOESysOdoBbJP1CIq2AuL0iOljkMS\nsOnp94yMDDzyyCPYuXMnenp6sGbNGmzZsgWenp6YM2cO1qxZg4cffhgAkJWVhaioKFvGIzuiUAj4\nxU1j8Ie3DuO9HcWYnBQCZ0HqVET2q765C699dBzOTkrcvWgMVEq7uQ6abEgQZX5u09KfufBzHPuS\ne7oOr20uQniQJ353+wS4qOU9vaUj7JM+HIv96DGa8OyGoyiracPPFsRh+rgQqSMNmdz3yfc57Gfq\nRNcqNU6LG5LDcLGmjZ+vE12GKIpYv70YZTVtyEwNx7SkYKkjkYRY6mT3lmfEID7SD4dP1eHr3Kvf\n5kg0nHxzvAr7C6oREeiJe3+SBEHg51TDGUud7J5KqcDvfpoKb3c1PthditNlTVJHIrILpVUteO/r\nYri7qHD/kkQ4O1l2zg6SH5Y6yYKflwt+uTgRggC8/mkRGlvld98tkSW1dhjwj0+KYDKJuOfmBARw\nGlgCS51kZPQIHyzLiEFrZw9e21yEHqNZ6khEkjCZzXj90yI0tXVjyYyRSIzylzoS2QmWOslKZnIY\npiQEorSqFe/vPCt1HCJJfLTnHE5fbMaEUQHISuNyqvQdljrJiiAI+On8OIRpPLA7vxL7OTENDTO5\np+vw1eGLCPRzw88XjoGCF8bR97DUSXacnZRYfUsi3JxVeGfbGZTVOMa9rERXU1nfgX99cerbvwNj\n4eos73kbyPJY6iRLWl83/OKmMTCazPjfjwvR3tUjdSQiq+rUG/G/Hxeiu8eE/74xHqEB7lJHIjvE\nUifZGhcTgEXpkWho1eONz07AbObENOSYzKKIt744idrGTsybNAKpcVzsii6PpU6ytmhaFJKi/XHi\nfCM+2XdO6jhEVrH1UBnyz9YjLtwHt86KljoO2TGWOsmaQuhd+EXj44IvssuQd0YndSQiiyo614CP\n956Dr6cz7r05EUoFf23TlfH/DpI9dxcnrL4lCWonBdZ9fgKlVS1SRyKyiIu1bfjH5iIoFQLuW5II\nL3e11JHIzrHUySGM0Hrg3psT0WMy4+UPC1DX1Cl1JKIhaWzV46VNx6E3mPDzhWMQHeItdSSSAZY6\nOYzxMQFYOTcWbZ09ePGD42jrNEgdiei6dOp78OKm42huN2DZ7BhMig+UOhLJBEudHMrsCaHImhKB\n2qYuvPxRAQw9JqkjEV0To8mMVz8pQqWuAzdMDMO8SSOkjkQywlInh3PLzJGYMiYQpZWtWPf5Sd7q\nRrIhiiLe/vI0TpU1YcKoANyWOYpLqdI1YamTw1EIAn6WFY+4cB/kFevw/i7OEU/y8Mm+c8g+UYOR\nIV64e1ECFAoWOl0bljo5JCeVAqtvGYuQAHfsOFKB7bnlUkciGtA3xyqx5WAZtD6ueODWJK6NTteF\npU4Oy83FCQ8tHQdvDzU27jyLI6frpI5EdFkFpQ1Yv60YHq5OeGjZOHi58dY1uj4sdXJo/t4ueGjp\nOKjVSrz5+UmcrWiWOhLRJS7UtOK1zUVQKgU8eGsSAv3cpI5EMsZSJ4cXHuiJ+xcnwmwW8fKHBahu\n6JA6EhEAoL65Cy9t6r1L4+6bEhAdynvRaWhY6jQsJI70x08XxKJDb8SLHxxHSwfvYSdpdXx7L3pr\nhwG3ZY5CcqxG6kjkAFjqNGxMTwrBovRI1Lfo8eLGY1yulSTT1W3ES5uOo7qhd9W1zBTei06WwVKn\nYeXmaVGYNT4EF+va8cL7+Sx2srmubiP+9sExlFa2Ii0hEEtnx0gdiRwIS52GFUEQsHJeLGaMC8HF\n2nb89f1j6NCz2Mk2urp7P/4prWzFlIRA3HXjGCg4uQxZEEudhh2FIOCO+bGYMS4YZbVteIHFTjbQ\n1W3Ei5uOo6SyBVPGBOLnN47h5DJkcSpbv+Dzzz+PvLw8GI1G3HPPPZg7d27/toyMDAQFBUGp7J10\n4YUXXkBgIBcyIMvrLfY4iCKwr6Aaf33/GB5ZMR5uLk5SRyMH1PcZeklFCybFa3HXwngWOlmFTUv9\n0KFDOHv2LDZu3IimpiYsWbLkklIHgHXr1sHd3d2WsWiYUggCfrogDiKA/QXVeIHFTlagN/QW+tlv\nC/0XN42BUsGTpGQdNi311NRUJCUlAQC8vLzQ1dUFk8nUf2ROZGsKQcCdC+IAEdhfWI2/bjyGh5ez\n2Mky9AYjXvqAhU62Y9P/u5RKJdzcemdL+vDDDzFjxowfFfpTTz2F2267DS+88AJEkatrkfUpBAF3\nZsUhfWwQzle34a8bj6NTb5Q6Fslc7xF6AYorWpAax0In2xBECZpzx44deOONN/Cvf/0Lnp6e/Y9v\n3rwZ06dPh7e3N+6//34sWbIE8+fPH/C5jEYTVCoe6dPQmcwiXt6Yj11HyjE63Ad/unsq3F15xE7X\nTt9txJp/HsKJcw1IHxeC3/xXMpRKFjpZn81Lfd++ffj73/+Of/7zn/Dx8bni97377rtoaGjAAw88\nMODz6XRtFs2n0Xha/DmlwrFcO7NZxFtfnOpf/vJ/lo2Hm4vlPqXiPrFPlhxLt8GElzYdx5nyZiTH\nanDPogSobFTo3Cf2ydJj0Wg8r7jNpm8d29ra8Pzzz+ONN974UaG3tbXhrrvugsHQO31nbm4uRo0a\nZct4RFAoBNx1YzzSEoJwrqoVf92Yj1ZOKUuD1KHvwYsfHJOk0IkAG18o9+WXX6KpqQm//vWv+x+b\nPHkyYmNjMWfOHMyYMQPLly+Hs7MzxowZc9VT70TW0FfsCgVwoLAGz6w/gl8vHYdgf96VQVema+7q\nn/o1JU6Lu28aw0Inm5PkM3VL4un3K+NYhkYURXy6/zw+O3AB7i4q/OonSRg94sofGQ0G94l9GupY\nzle34u+bjqO1swfzJ4Xj1tnRkswUx31inxz29DuRnAiCgMXTR+JnWXHQG0x44f18HD5VK3UssjP5\nxTo89+5RtHX1YOXc0ViWEcOpX0kyNp9RjkhupieFwM/LBf/4pBCvf3oC9S16LJgcDoG/uIe9HUfK\n8Z8dZ+HkpMCvbknC+FEBUkeiYY5H6kSDkBDph8f+Kxm+ns74cE8p1m87A5PZLHUskohZFPH+zrN4\nb8dZeLqr8ejtE1noZBdY6kSDFKb1wBN3pCBc64E9x6rwykeF0Bs4Sc1wY+gx4bXNRdieW45gfzc8\nsSoZUcFeUsciAsBSJ7omvp7OePS/JiJxpB8KShvwl3ePoqmtW+pYZCOtnQb8v/fzkXdGh7hwHzy+\nKhkBPq5SxyLqx1Inukauzio8eGsSZo7vXZP9mfVHUKFrlzoWWVltYyeefScPpZWtSEsIxEPLxsOd\nawSQnWGpE10HpUKBO+bF4iczR6KxtRvPrs/DoZM1UsciKzlarMPT7xxBXXMXFk6NxM8XjoGTir8+\nyf7w6nei6yQIAm5Mi4TGxxVvbz2NNz87iVMXmnB75mg4q7kegSPoMZrwwa5S7DxaAbVKgbtujEf6\n2GCpYxFdEUudaIgmxQciItATr396AvsKqlFS2YJf3pyIMK2H1NFoCKobOvDGpydwsa4doQHuuPfm\nBIRquE/JvvH8EZEFBPq54fFVychMDkN1QyfWvnMEe/IruXywTB0orMaf/n0EF+vaMXN8CJ74aQoL\nnWSBR+pEFuKkUuD2OaMRH+mLf31xCu9sO4OTZU24c34s3HhBlSzoDUZs2F6Mg0U1cHVW4t6bEzAp\nPlDqWESDxlInsrAJozT443974o3PTuDI6TpcqG7FPTcnIDrEW+poNICLtW147dMTqG3sRFSwJ+65\nORFa3q5GMsPT70RW4Oflgt/ePgELp0aioUWPv2w4iq05ZTCbeTre3oiiiC37z+Hpd46gtrET8yeF\n47GVySx0kiUeqRNZiVKhwC0zRiI+3Advfn4Sm3aXorSqDSsyohHgzcKwB01t3diw/Qzyz9bDw9UJ\nP18Yj6RoTvdK8sVSJ7Ky+Eg//PG/J+GfW07i6Jk6FJXWY+HUSMybFM57nSViNJmxM68Cm/efR7fB\nhLHRAbhzfix8PZ2ljkY0JCx1IhvwclfjoWXjUHSxGW99WoSP957DgaIarJwzGglRflLHG1bOXGzC\nhq+LUanrgLuLCivmx+KWG2LR0MBZAUn+WOpENiIIAjJSwhEd6IFP9p3HrqMV+OvGY0iJ02JFRgz8\nvFykjujQWtq78cHuEmSfqIUAYOb4EPxkZjQ8XJ2gUHAZXXIMLHUiG3NzccJ/zRmN6UnBWL/9DI6c\nrkNhaQMWTYvEnJQRUCl5St6STGYzdh2txOZ959DVbUJEkCdWzY3FyBCurEaOh6VOJJHwQE88tjIZ\nBwqrsWl3KTbtLsX+gmqsnBuL+AhfqeM5hLMVzVi/rRgVuna4u6iwal4sZo4L4ZE5OSyWOpGEFIKA\n6UkhmDBKg0/2nsOe/Er8v//kY1K8Fkumj0Sgn5vUEWVJ19yFz/afx4Gi3kV2piUF49ZZ0fByU0uc\njMi6WOpEdsDD1Qmr5sVi+rhgrN9WjMOn6pB7qg7JcVpkTQlHZBBPFQ9GRV07vswpw+GTdTCLIsK1\nHlg5LxYxoZz4h4YHljqRHYkM8sLv70hG3hkdvswuw5HTdThyug4Jkb7ImhKBuAhfCAJPHf9QcXkz\nvjxUhoLSBgBAmMYdC6ZEYFK8FkoFr1Gg4YOlTmRnFIKA1DgtUmI1OFnWhC+zy3DiQhNOXGhCVLAn\nsqZEYMJoDRTDvNzNooiC0gZ8eagMJRUtAIDRYd7ISovA2JH+fPNDwxJLnchOCYKAhEg/JET64Xx1\nK77MLsPRYh1e/aQIQX5uWDA5HGmJQcPuanmjyYzcU3X4MqcMlboOAMD4mAAsmBKOUWE+EqcjkhZL\nnUgGooK9cP8tY1Hd0IGtOReRXVSDt7eexub95zE1MQgpsVqEB3o47NGpKIqo1HUg93QdDhbVoKFV\nD4UgIC0hCAumhCOMy6ISAWCpE8lKsL87/jsrHounReHrI+XYc6wKX2SX4YvsMmh9XZEap0VqnBYj\ntI5R8JW6duSerkPu6TpUN3QCANROCtyQHIZ5qSMQwEVXiC7BUieSIT8vFyzPGIXF00eisLQBR87U\n4VhJfX/BB/q6IjVei5RY+RX85YrcSaVAcqwGqXFaJEX7w0XNX11El8O/GUQy5uykREqcFilxWnT3\nmFBY2oDc03U4XlqPLQfLsOVgGQL93JAap8WYCF9EBHnC1dm+/tp3dRtxsbYNpy82I/d0Harqez8n\nd1IpkDxag5Q4LcbFsMiJBsPmf0ueffZZHD9+HIIg4PHHH0dSUlL/toMHD+Jvf/sblEolZsyYgfvv\nv9/W8Yhk65KCN5hQcK634AtK6rHl4AVsOXgBABDo54bIIM/+f8IDbVf0eoMRF2vbcaGmDWU1rbhQ\n04aahk70rTKvUiowcbQGKXEajIsOsLs3IET2zqZ/Yw4fPoyysjJs3LgRpaWlePzxx7Fx48b+7U8/\n/TTeeustBAYGYuXKlZg3bx5iYmJsGZHIITirlf2fr3cbTCg634DSqlaU1bThQk0bck7WIudkLQBA\nwKVFr/FxhZuLCu4uTv3/VjsprnoKXxRFGIxmdOqN6ND39P+7vlmPCzVtuFDTekmBA4CLWonRI3wQ\nEeSJkSFeGDvSn0VONAQ2/duTnZ2NzMxMAEB0dDRaWlrQ3t4ODw8PlJeXw9vbG8HBwQCAmTNnIjs7\nm6VONETOaiWSY7VIjtUC6L2/W9fc1Vvw1b1lW1bbhkMnO3Ho26L/IZVSgJuLE9xdVN8VvVqF5lY9\nOvQ96NAb0anvgdEkXvbn+3KMGuHT/+YhIsgTgX5uw/5+eyJLsmmp19fXIyEhof9rPz8/6HQ6eHh4\nQKfTwc/P75Jt5eXlV31OX183qFRKi+bUaDwt+nxS4ljsjz2MI1DrhcTRgf1fm80iaho6UFLRjIYW\nPdq7etDeaej9d1cPOjp70Pbt17VNXTCbe8tboRDg4eoED1cnBPm7wcNVDQ9XJ7i79T7m6aaGr5cL\nYsK8ERLgYdcLqdjDfrEERxkHwLFcD0nPc4nild/VD1ZTU6cFknxHo/GETtdm0eeUCsdif+x5HE4A\n4sO8gbCB50kXRRF6gwkBAR5ob+0a9JX1DQ3tFkhpHfa8X66Fo4wD4Fiu9nxXYtOpqLRaLerr6/u/\nrqurg0ajuey22tpaaLVaW8YjokEQBAGuziq4uTjJ6lY5ouHApqWenp6Obdu2AQBOnDgBrVYLD4/e\nmaDCwsLQ3t6OiooKGI1G7N69G+np6baMR0REJGs2Pf0+ceJEJCQkYMWKFRAEAU899RQ+/vhjeHp6\nYs6cOVizZg0efvhhAEBWVhaioqJsGY+IiEjWbP6Z+iOPPHLJ13Fxcf1/Tk1NveQWNyIiIhq84bW8\nExERkQNjqRMRETkIljoREZGDYKkTERE5CJY6ERGRg2CpExEROQiWOhERkYNgqRMRETkIQbTEqipE\nREQkOR6pExEROQiWOhERkYNgqRMRETkIljoREZGDYKkTERE5CJY6ERGRg7D5eur24tlnn8Xx48ch\nCAIef/xxJCUl9W87ePAg/va3v0GpVGLGjBm4//77JUx6dQONJSMjA0FBQVAqlQCAF154AYGBgVJF\nvari4mLcd999uPPOO7Fy5cpLtsltvww0Frntl+effx55eXkwGo245557MHfu3P5tctovA41DTvuk\nq6sLv/vd79DQ0IDu7m7cd999mD17dv92Oe2Tq41FTvsFAPR6PRYuXIj77rsPt9xyS//jNtsn4jCU\nk5Mj3n333aIoimJJSYm4bNmyS7YvWLBArKqqEk0mk3jbbbeJZ8+elSLmoFxtLLNnzxbb29uliHbN\nOjo6xJUrV4pPPPGEuH79+h9tl9N+udpY5LRfsrOzxZ///OeiKIpiY2OjOHPmzEu2y2W/XG0ccton\nX3zxhfjmm2+KoiiKFRUV4ty5cy/ZLpd9IopXH4uc9osoiuLf/vY38ZZbbhE/+uijSx631T4Zlqff\ns7OzkZmZCQCIjo5GS0sL2tvbAQDl5eXw9vZGcHAwFAoFZs6ciezsbCnjDmigsciNWq3GunXroNVq\nf7RNbvtloLHITWpqKv7+978DALy8vNDV1QWTyQRAXvtloHHITVZWFn7xi18AAKqrqy85cpXTPgEG\nHovclJaWoqSkBLNmzbrkcVvuk2F5+r2+vh4JCQn9X/v5+UGn08HDwwM6nQ5+fn6XbCsvL5ci5qAM\nNJY+Tz31FCorK5GcnIyHH34YgiBIEfWqVCoVVKrL/y8pt/0y0Fj6yGW/KJVKuLm5AQA+/PBDzJgx\no/9UqJz2y0Dj6COXfdJnxYoVqKmpweuvv97/mJz2yfddbix95LJfnnvuOTz55JPYvHnzJY/bcp8M\ny1L/IdGBZsr94VgeeB8HblwAAAN2SURBVOABTJ8+Hd7e3rj//vuxbds2zJ8/X6J01EeO+2XHjh34\n8MMP8a9//UvqKENypXHIcZ+8//77OHXqFH7zm9/gs88+s9uyG4wrjUUu+2Xz5s0YP348RowYIWmO\nYXn6XavVor6+vv/ruro6aDSay26rra2161OoA40FABYvXgx/f3+oVCrMmDEDxcXFUsQcMrntl6uR\n237Zt28fXn/9daxbtw6enp79j8ttv1xpHIC89klRURGqq6sBAPHx8TCZTGhsbAQgv30y0FgA+eyX\nPXv2YOfOnVi2bBk2bdqEf/zjHzh48CAA2+6TYVnq6enp2LZtGwDgxIkT0Gq1/aerw8LC0N7ejoqK\nChiNRuzevRvp6elSxh3QQGNpa2vDXXfdBYPBAADIzc3FqFGjJMs6FHLbLwOR235pa2vD888/jzfe\neAM+Pj6XbJPTfhloHHLbJ0eOHOk/01BfX4/Ozk74+voCkNc+AQYei5z2y0svvYSPPvoIH3zwAZYu\nXYr77rsPU6dOBWDbfTJsV2l74YUXcOTIEQiCgKeeegonT56Ep6cn5syZg9zcXLzwwgsAgLlz5+Ku\nu+6SOO3ABhrL//3f/2Hz5s1wdnbGmDFj8OSTT9rtKbqioiI899xzqKyshEqlQmBgIDIyMhAWFia7\n/XK1schpv2zcuBGvvPIKoqKi+h+bPHkyYmNjZbVfrjYOOe0TvV6P3//+96iuroZer8fq1avR3Nws\ny99hVxuLnPZLn1deeQWhoaEAYPN9MmxLnYiIyNEMy9PvREREjoilTkRE5CBY6kRERA6CpU5EROQg\nWOpEREQOgqVORETkIFjqREREDoKlTkSDlpGRgdbWVgDAgw8+iMceewxA74IVCxculDIaEYGlTkTX\nIC0tDXl5eRBFEQ0NDf0rTeXk5GDatGkSpyMirtJGRIOWnp6O3NxcBAcHY+TIkWhtbUV1dTVycnIw\nd+5cqeMRDXs8UieiQUtLS8PRo0eRk5OD1NRUpKSk4PDhwzh27BhSU1Oljkc07LHUiWjQfH19IYoi\n9u7di0mTJiElJQVbt26FVquFi4uL1PGIhj2WOhFdk0mTJqGiogKBgYGIjY1Ffn6+XS/tSTSccJU2\n+v/t1wEJAAAAgKD/r/sReiICYMKpA8CEqAPAhKgDwISoA8CEqAPAhKgDwISoA8CEqAPARPjK6bjy\njJrkAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f77687fe128>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "wJeKCS0eaEtI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gradient descent with numpy"
      ]
    },
    {
      "metadata": {
        "id": "kY6ygCqTNa1L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7253
        },
        "outputId": "6e0f131a-2183-4224-8372-c89b55edfd43"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_data = [1.0,2.0,3.0]\n",
        "y_data = [2.0,4.0,6.0]\n",
        "\n",
        "w=1.0 # random guess\n",
        "\n",
        "def forward(x):\n",
        "  return x*w\n",
        "\n",
        "def loss(x,y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "def gradient(x,y):\n",
        "  return 2 * x * ( x * w - y )\n",
        "\n",
        "print(\"predict (before training)\", 4, forward(4))\n",
        "\n",
        "for epoch in range(100):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    grad = gradient(x_val, y_val)\n",
        "    w = w - 0.01 * grad\n",
        "    print(\"\\tgrad: \",x_val, y_val, grad)\n",
        "    l = loss(x_val, y_val)\n",
        "    \n",
        "  print(\"progress: \",epoch, \"w=\",w,\"loss=\",l)\n",
        "  \n",
        "print(\"predict (after training)\", \"4 hours\" , forward(4))\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -7.84\n",
            "\tgrad:  3.0 6.0 -16.2288\n",
            "progress:  0 w= 1.260688 loss= 4.919240100095999\n",
            "\tgrad:  1.0 2.0 -1.478624\n",
            "\tgrad:  2.0 4.0 -5.796206079999999\n",
            "\tgrad:  3.0 6.0 -11.998146585599997\n",
            "progress:  1 w= 1.453417766656 loss= 2.688769240265834\n",
            "\tgrad:  1.0 2.0 -1.093164466688\n",
            "\tgrad:  2.0 4.0 -4.285204709416961\n",
            "\tgrad:  3.0 6.0 -8.87037374849311\n",
            "progress:  2 w= 1.5959051959019805 loss= 1.4696334962911515\n",
            "\tgrad:  1.0 2.0 -0.8081896081960389\n",
            "\tgrad:  2.0 4.0 -3.1681032641284723\n",
            "\tgrad:  3.0 6.0 -6.557973756745939\n",
            "progress:  3 w= 1.701247862192685 loss= 0.8032755585999681\n",
            "\tgrad:  1.0 2.0 -0.59750427561463\n",
            "\tgrad:  2.0 4.0 -2.3422167604093502\n",
            "\tgrad:  3.0 6.0 -4.848388694047353\n",
            "progress:  4 w= 1.7791289594933983 loss= 0.43905614881022015\n",
            "\tgrad:  1.0 2.0 -0.44174208101320334\n",
            "\tgrad:  2.0 4.0 -1.7316289575717576\n",
            "\tgrad:  3.0 6.0 -3.584471942173538\n",
            "progress:  5 w= 1.836707389300983 loss= 0.2399802903801062\n",
            "\tgrad:  1.0 2.0 -0.3265852213980338\n",
            "\tgrad:  2.0 4.0 -1.2802140678802925\n",
            "\tgrad:  3.0 6.0 -2.650043120512205\n",
            "progress:  6 w= 1.8792758133988885 loss= 0.1311689630744999\n",
            "\tgrad:  1.0 2.0 -0.241448373202223\n",
            "\tgrad:  2.0 4.0 -0.946477622952715\n",
            "\tgrad:  3.0 6.0 -1.9592086795121197\n",
            "progress:  7 w= 1.910747160155559 loss= 0.07169462478267678\n",
            "\tgrad:  1.0 2.0 -0.17850567968888198\n",
            "\tgrad:  2.0 4.0 -0.6997422643804168\n",
            "\tgrad:  3.0 6.0 -1.4484664872674653\n",
            "progress:  8 w= 1.9340143044689266 loss= 0.03918700813247573\n",
            "\tgrad:  1.0 2.0 -0.13197139106214673\n",
            "\tgrad:  2.0 4.0 -0.5173278529636143\n",
            "\tgrad:  3.0 6.0 -1.0708686556346834\n",
            "progress:  9 w= 1.9512159834655312 loss= 0.021418922423117836\n",
            "\tgrad:  1.0 2.0 -0.09756803306893769\n",
            "\tgrad:  2.0 4.0 -0.38246668963023644\n",
            "\tgrad:  3.0 6.0 -0.7917060475345892\n",
            "progress:  10 w= 1.9639333911678687 loss= 0.01170720245384975\n",
            "\tgrad:  1.0 2.0 -0.07213321766426262\n",
            "\tgrad:  2.0 4.0 -0.2827622132439096\n",
            "\tgrad:  3.0 6.0 -0.5853177814148953\n",
            "progress:  11 w= 1.9733355232910992 loss= 0.006398948863435593\n",
            "\tgrad:  1.0 2.0 -0.05332895341780164\n",
            "\tgrad:  2.0 4.0 -0.2090494973977819\n",
            "\tgrad:  3.0 6.0 -0.4327324596134101\n",
            "progress:  12 w= 1.9802866323953892 loss= 0.003497551760830656\n",
            "\tgrad:  1.0 2.0 -0.039426735209221686\n",
            "\tgrad:  2.0 4.0 -0.15455280202014876\n",
            "\tgrad:  3.0 6.0 -0.3199243001817109\n",
            "progress:  13 w= 1.9854256707695 loss= 0.001911699652671057\n",
            "\tgrad:  1.0 2.0 -0.02914865846100012\n",
            "\tgrad:  2.0 4.0 -0.11426274116712065\n",
            "\tgrad:  3.0 6.0 -0.2365238742159388\n",
            "progress:  14 w= 1.9892250235079405 loss= 0.0010449010656399273\n",
            "\tgrad:  1.0 2.0 -0.021549952984118992\n",
            "\tgrad:  2.0 4.0 -0.08447581569774698\n",
            "\tgrad:  3.0 6.0 -0.17486493849433593\n",
            "progress:  15 w= 1.9920339305797026 loss= 0.0005711243580809696\n",
            "\tgrad:  1.0 2.0 -0.015932138840594856\n",
            "\tgrad:  2.0 4.0 -0.062453984255132156\n",
            "\tgrad:  3.0 6.0 -0.12927974740812687\n",
            "progress:  16 w= 1.994110589284741 loss= 0.0003121664271570621\n",
            "\tgrad:  1.0 2.0 -0.011778821430517894\n",
            "\tgrad:  2.0 4.0 -0.046172980007630926\n",
            "\tgrad:  3.0 6.0 -0.09557806861579543\n",
            "progress:  17 w= 1.9956458879852805 loss= 0.0001706246229305199\n",
            "\tgrad:  1.0 2.0 -0.008708224029438938\n",
            "\tgrad:  2.0 4.0 -0.03413623819540135\n",
            "\tgrad:  3.0 6.0 -0.07066201306448505\n",
            "progress:  18 w= 1.9967809527381737 loss= 9.326038746484765e-05\n",
            "\tgrad:  1.0 2.0 -0.006438094523652627\n",
            "\tgrad:  2.0 4.0 -0.02523733053271826\n",
            "\tgrad:  3.0 6.0 -0.052241274202728505\n",
            "progress:  19 w= 1.9976201197307648 loss= 5.097447086306101e-05\n",
            "\tgrad:  1.0 2.0 -0.004759760538470381\n",
            "\tgrad:  2.0 4.0 -0.01865826131080439\n",
            "\tgrad:  3.0 6.0 -0.03862260091336722\n",
            "progress:  20 w= 1.998240525958391 loss= 2.7861740127856012e-05\n",
            "\tgrad:  1.0 2.0 -0.0035189480832178432\n",
            "\tgrad:  2.0 4.0 -0.01379427648621423\n",
            "\tgrad:  3.0 6.0 -0.028554152326460525\n",
            "progress:  21 w= 1.99869919972735 loss= 1.5228732143933469e-05\n",
            "\tgrad:  1.0 2.0 -0.002601600545300009\n",
            "\tgrad:  2.0 4.0 -0.01019827413757568\n",
            "\tgrad:  3.0 6.0 -0.021110427464781978\n",
            "progress:  22 w= 1.9990383027488265 loss= 8.323754426231206e-06\n",
            "\tgrad:  1.0 2.0 -0.001923394502346909\n",
            "\tgrad:  2.0 4.0 -0.007539706449199102\n",
            "\tgrad:  3.0 6.0 -0.01560719234984198\n",
            "progress:  23 w= 1.9992890056818404 loss= 4.549616284094891e-06\n",
            "\tgrad:  1.0 2.0 -0.0014219886363191492\n",
            "\tgrad:  2.0 4.0 -0.005574195454370212\n",
            "\tgrad:  3.0 6.0 -0.011538584590544687\n",
            "progress:  24 w= 1.999474353368653 loss= 2.486739429417538e-06\n",
            "\tgrad:  1.0 2.0 -0.0010512932626940419\n",
            "\tgrad:  2.0 4.0 -0.004121069589761106\n",
            "\tgrad:  3.0 6.0 -0.008530614050808794\n",
            "progress:  25 w= 1.9996113831376856 loss= 1.3592075910762856e-06\n",
            "\tgrad:  1.0 2.0 -0.0007772337246287897\n",
            "\tgrad:  2.0 4.0 -0.0030467562005451754\n",
            "\tgrad:  3.0 6.0 -0.006306785335127074\n",
            "progress:  26 w= 1.9997126908902887 loss= 7.429187207079447e-07\n",
            "\tgrad:  1.0 2.0 -0.0005746182194226179\n",
            "\tgrad:  2.0 4.0 -0.002252503420136165\n",
            "\tgrad:  3.0 6.0 -0.00466268207967957\n",
            "progress:  27 w= 1.9997875889274812 loss= 4.060661735575354e-07\n",
            "\tgrad:  1.0 2.0 -0.0004248221450375844\n",
            "\tgrad:  2.0 4.0 -0.0016653028085471533\n",
            "\tgrad:  3.0 6.0 -0.0034471768136938863\n",
            "progress:  28 w= 1.9998429619451539 loss= 2.2194855602869353e-07\n",
            "\tgrad:  1.0 2.0 -0.00031407610969225175\n",
            "\tgrad:  2.0 4.0 -0.0012311783499932005\n",
            "\tgrad:  3.0 6.0 -0.0025485391844828342\n",
            "progress:  29 w= 1.9998838998815958 loss= 1.213131374411496e-07\n",
            "\tgrad:  1.0 2.0 -0.00023220023680847746\n",
            "\tgrad:  2.0 4.0 -0.0009102249282886277\n",
            "\tgrad:  3.0 6.0 -0.0018841656015560204\n",
            "progress:  30 w= 1.9999141657892625 loss= 6.630760559646474e-08\n",
            "\tgrad:  1.0 2.0 -0.00017166842147497974\n",
            "\tgrad:  2.0 4.0 -0.0006729402121816719\n",
            "\tgrad:  3.0 6.0 -0.0013929862392156878\n",
            "progress:  31 w= 1.9999365417379913 loss= 3.624255915449335e-08\n",
            "\tgrad:  1.0 2.0 -0.0001269165240174175\n",
            "\tgrad:  2.0 4.0 -0.0004975127741477792\n",
            "\tgrad:  3.0 6.0 -0.0010298514424817995\n",
            "progress:  32 w= 1.9999530845453979 loss= 1.9809538924707548e-08\n",
            "\tgrad:  1.0 2.0 -9.383090920422887e-05\n",
            "\tgrad:  2.0 4.0 -0.00036781716408107457\n",
            "\tgrad:  3.0 6.0 -0.0007613815296476645\n",
            "progress:  33 w= 1.9999653148414271 loss= 1.0827542027017377e-08\n",
            "\tgrad:  1.0 2.0 -6.937031714571162e-05\n",
            "\tgrad:  2.0 4.0 -0.0002719316432120422\n",
            "\tgrad:  3.0 6.0 -0.0005628985014531906\n",
            "progress:  34 w= 1.999974356846045 loss= 5.9181421028034105e-09\n",
            "\tgrad:  1.0 2.0 -5.1286307909848006e-05\n",
            "\tgrad:  2.0 4.0 -0.00020104232700646207\n",
            "\tgrad:  3.0 6.0 -0.0004161576169003922\n",
            "progress:  35 w= 1.9999810417085633 loss= 3.2347513278475087e-09\n",
            "\tgrad:  1.0 2.0 -3.7916582873442906e-05\n",
            "\tgrad:  2.0 4.0 -0.0001486330048638962\n",
            "\tgrad:  3.0 6.0 -0.0003076703200690645\n",
            "progress:  36 w= 1.9999859839076413 loss= 1.7680576050779005e-09\n",
            "\tgrad:  1.0 2.0 -2.8032184717474706e-05\n",
            "\tgrad:  2.0 4.0 -0.0001098861640933535\n",
            "\tgrad:  3.0 6.0 -0.00022746435967313516\n",
            "progress:  37 w= 1.9999896377347262 loss= 9.6638887447731e-10\n",
            "\tgrad:  1.0 2.0 -2.0724530547688857e-05\n",
            "\tgrad:  2.0 4.0 -8.124015974608767e-05\n",
            "\tgrad:  3.0 6.0 -0.00016816713067413502\n",
            "progress:  38 w= 1.999992339052936 loss= 5.282109892545845e-10\n",
            "\tgrad:  1.0 2.0 -1.5321894128117464e-05\n",
            "\tgrad:  2.0 4.0 -6.006182498197177e-05\n",
            "\tgrad:  3.0 6.0 -0.00012432797771566584\n",
            "progress:  39 w= 1.9999943361699042 loss= 2.887107421958329e-10\n",
            "\tgrad:  1.0 2.0 -1.1327660191629008e-05\n",
            "\tgrad:  2.0 4.0 -4.4404427951505454e-05\n",
            "\tgrad:  3.0 6.0 -9.191716585732479e-05\n",
            "progress:  40 w= 1.9999958126624442 loss= 1.5780416225633037e-10\n",
            "\tgrad:  1.0 2.0 -8.37467511161094e-06\n",
            "\tgrad:  2.0 4.0 -3.282872643772805e-05\n",
            "\tgrad:  3.0 6.0 -6.795546372551087e-05\n",
            "progress:  41 w= 1.999996904251097 loss= 8.625295142578772e-11\n",
            "\tgrad:  1.0 2.0 -6.191497806007362e-06\n",
            "\tgrad:  2.0 4.0 -2.4270671399762023e-05\n",
            "\tgrad:  3.0 6.0 -5.0240289795056015e-05\n",
            "progress:  42 w= 1.999997711275687 loss= 4.71443308235547e-11\n",
            "\tgrad:  1.0 2.0 -4.5774486259198e-06\n",
            "\tgrad:  2.0 4.0 -1.794359861406747e-05\n",
            "\tgrad:  3.0 6.0 -3.714324913239864e-05\n",
            "progress:  43 w= 1.9999983079186507 loss= 2.5768253628059826e-11\n",
            "\tgrad:  1.0 2.0 -3.3841626985164908e-06\n",
            "\tgrad:  2.0 4.0 -1.326591777761621e-05\n",
            "\tgrad:  3.0 6.0 -2.7460449796734565e-05\n",
            "progress:  44 w= 1.9999987490239537 loss= 1.4084469615916932e-11\n",
            "\tgrad:  1.0 2.0 -2.5019520926150562e-06\n",
            "\tgrad:  2.0 4.0 -9.807652203264183e-06\n",
            "\tgrad:  3.0 6.0 -2.0301840059744336e-05\n",
            "progress:  45 w= 1.9999990751383971 loss= 7.698320862431846e-12\n",
            "\tgrad:  1.0 2.0 -1.8497232057157476e-06\n",
            "\tgrad:  2.0 4.0 -7.250914967116273e-06\n",
            "\tgrad:  3.0 6.0 -1.5009393983689279e-05\n",
            "progress:  46 w= 1.9999993162387186 loss= 4.20776540913866e-12\n",
            "\tgrad:  1.0 2.0 -1.3675225627451937e-06\n",
            "\tgrad:  2.0 4.0 -5.3606884460322135e-06\n",
            "\tgrad:  3.0 6.0 -1.109662508014253e-05\n",
            "progress:  47 w= 1.9999994944870796 loss= 2.299889814334344e-12\n",
            "\tgrad:  1.0 2.0 -1.0110258408246864e-06\n",
            "\tgrad:  2.0 4.0 -3.963221296032771e-06\n",
            "\tgrad:  3.0 6.0 -8.20386808086937e-06\n",
            "progress:  48 w= 1.9999996262682318 loss= 1.2570789110540446e-12\n",
            "\tgrad:  1.0 2.0 -7.474635363990956e-07\n",
            "\tgrad:  2.0 4.0 -2.930057062755509e-06\n",
            "\tgrad:  3.0 6.0 -6.065218119744031e-06\n",
            "progress:  49 w= 1.999999723695619 loss= 6.870969979249939e-13\n",
            "\tgrad:  1.0 2.0 -5.526087618612507e-07\n",
            "\tgrad:  2.0 4.0 -2.166226346744793e-06\n",
            "\tgrad:  3.0 6.0 -4.484088535150477e-06\n",
            "progress:  50 w= 1.9999997957248556 loss= 3.7555501141274804e-13\n",
            "\tgrad:  1.0 2.0 -4.08550288710785e-07\n",
            "\tgrad:  2.0 4.0 -1.6015171322436572e-06\n",
            "\tgrad:  3.0 6.0 -3.3151404608133817e-06\n",
            "progress:  51 w= 1.9999998489769344 loss= 2.052716967104274e-13\n",
            "\tgrad:  1.0 2.0 -3.020461312175371e-07\n",
            "\tgrad:  2.0 4.0 -1.1840208351543424e-06\n",
            "\tgrad:  3.0 6.0 -2.4509231284497446e-06\n",
            "progress:  52 w= 1.9999998883468353 loss= 1.1219786256679713e-13\n",
            "\tgrad:  1.0 2.0 -2.2330632942768602e-07\n",
            "\tgrad:  2.0 4.0 -8.753608113920563e-07\n",
            "\tgrad:  3.0 6.0 -1.811996877876254e-06\n",
            "progress:  53 w= 1.9999999174534755 loss= 6.132535848018759e-14\n",
            "\tgrad:  1.0 2.0 -1.6509304900935717e-07\n",
            "\tgrad:  2.0 4.0 -6.471647520100987e-07\n",
            "\tgrad:  3.0 6.0 -1.3396310407642886e-06\n",
            "progress:  54 w= 1.999999938972364 loss= 3.351935118167793e-14\n",
            "\tgrad:  1.0 2.0 -1.220552721115098e-07\n",
            "\tgrad:  2.0 4.0 -4.784566662863199e-07\n",
            "\tgrad:  3.0 6.0 -9.904052991061008e-07\n",
            "progress:  55 w= 1.9999999548815364 loss= 1.8321081844499955e-14\n",
            "\tgrad:  1.0 2.0 -9.023692726373156e-08\n",
            "\tgrad:  2.0 4.0 -3.5372875473171916e-07\n",
            "\tgrad:  3.0 6.0 -7.322185204827747e-07\n",
            "progress:  56 w= 1.9999999666433785 loss= 1.0013977760018664e-14\n",
            "\tgrad:  1.0 2.0 -6.671324292994996e-08\n",
            "\tgrad:  2.0 4.0 -2.615159129248923e-07\n",
            "\tgrad:  3.0 6.0 -5.413379398078177e-07\n",
            "progress:  57 w= 1.9999999753390494 loss= 5.473462367088053e-15\n",
            "\tgrad:  1.0 2.0 -4.932190122985958e-08\n",
            "\tgrad:  2.0 4.0 -1.9334185274999527e-07\n",
            "\tgrad:  3.0 6.0 -4.002176350326181e-07\n",
            "progress:  58 w= 1.9999999817678633 loss= 2.991697274308627e-15\n",
            "\tgrad:  1.0 2.0 -3.6464273378555845e-08\n",
            "\tgrad:  2.0 4.0 -1.429399514307761e-07\n",
            "\tgrad:  3.0 6.0 -2.9588569994132286e-07\n",
            "progress:  59 w= 1.9999999865207625 loss= 1.6352086111474931e-15\n",
            "\tgrad:  1.0 2.0 -2.6958475007887728e-08\n",
            "\tgrad:  2.0 4.0 -1.0567722164012139e-07\n",
            "\tgrad:  3.0 6.0 -2.1875184863517916e-07\n",
            "progress:  60 w= 1.999999990034638 loss= 8.937759877335403e-16\n",
            "\tgrad:  1.0 2.0 -1.993072418216002e-08\n",
            "\tgrad:  2.0 4.0 -7.812843882959442e-08\n",
            "\tgrad:  3.0 6.0 -1.617258700292723e-07\n",
            "progress:  61 w= 1.9999999926324883 loss= 4.885220495987371e-16\n",
            "\tgrad:  1.0 2.0 -1.473502342363986e-08\n",
            "\tgrad:  2.0 4.0 -5.7761292637792394e-08\n",
            "\tgrad:  3.0 6.0 -1.195658771990793e-07\n",
            "progress:  62 w= 1.99999999455311 loss= 2.670175009618106e-16\n",
            "\tgrad:  1.0 2.0 -1.0893780100218464e-08\n",
            "\tgrad:  2.0 4.0 -4.270361841918202e-08\n",
            "\tgrad:  3.0 6.0 -8.839649012770678e-08\n",
            "progress:  63 w= 1.9999999959730488 loss= 1.4594702493172377e-16\n",
            "\tgrad:  1.0 2.0 -8.05390243385773e-09\n",
            "\tgrad:  2.0 4.0 -3.1571296688071016e-08\n",
            "\tgrad:  3.0 6.0 -6.53525820126788e-08\n",
            "progress:  64 w= 1.9999999970228268 loss= 7.977204100704301e-17\n",
            "\tgrad:  1.0 2.0 -5.9543463493128e-09\n",
            "\tgrad:  2.0 4.0 -2.334103754719763e-08\n",
            "\tgrad:  3.0 6.0 -4.8315948575350376e-08\n",
            "progress:  65 w= 1.9999999977989402 loss= 4.360197735196887e-17\n",
            "\tgrad:  1.0 2.0 -4.402119557767037e-09\n",
            "\tgrad:  2.0 4.0 -1.725630838222969e-08\n",
            "\tgrad:  3.0 6.0 -3.5720557178819945e-08\n",
            "progress:  66 w= 1.9999999983727301 loss= 2.3832065197304227e-17\n",
            "\tgrad:  1.0 2.0 -3.254539748809293e-09\n",
            "\tgrad:  2.0 4.0 -1.2757796596929438e-08\n",
            "\tgrad:  3.0 6.0 -2.6408640607655798e-08\n",
            "progress:  67 w= 1.9999999987969397 loss= 1.3026183953845832e-17\n",
            "\tgrad:  1.0 2.0 -2.406120636067044e-09\n",
            "\tgrad:  2.0 4.0 -9.431992964437086e-09\n",
            "\tgrad:  3.0 6.0 -1.9524227568012975e-08\n",
            "progress:  68 w= 1.999999999110563 loss= 7.11988308874388e-18\n",
            "\tgrad:  1.0 2.0 -1.7788739370416806e-09\n",
            "\tgrad:  2.0 4.0 -6.97318647269185e-09\n",
            "\tgrad:  3.0 6.0 -1.4434496264925656e-08\n",
            "progress:  69 w= 1.9999999993424284 loss= 3.89160224698574e-18\n",
            "\tgrad:  1.0 2.0 -1.3151431055291596e-09\n",
            "\tgrad:  2.0 4.0 -5.155360582875801e-09\n",
            "\tgrad:  3.0 6.0 -1.067159693945996e-08\n",
            "progress:  70 w= 1.9999999995138495 loss= 2.1270797208746147e-18\n",
            "\tgrad:  1.0 2.0 -9.72300906454393e-10\n",
            "\tgrad:  2.0 4.0 -3.811418736177075e-09\n",
            "\tgrad:  3.0 6.0 -7.88963561149103e-09\n",
            "progress:  71 w= 1.9999999996405833 loss= 1.1626238773828175e-18\n",
            "\tgrad:  1.0 2.0 -7.18833437218791e-10\n",
            "\tgrad:  2.0 4.0 -2.8178277489132597e-09\n",
            "\tgrad:  3.0 6.0 -5.832902161273523e-09\n",
            "progress:  72 w= 1.999999999734279 loss= 6.354692062078993e-19\n",
            "\tgrad:  1.0 2.0 -5.314420015167798e-10\n",
            "\tgrad:  2.0 4.0 -2.0832526814729135e-09\n",
            "\tgrad:  3.0 6.0 -4.31233715403323e-09\n",
            "progress:  73 w= 1.9999999998035491 loss= 3.4733644793346653e-19\n",
            "\tgrad:  1.0 2.0 -3.92901711165905e-10\n",
            "\tgrad:  2.0 4.0 -1.5401742103904326e-09\n",
            "\tgrad:  3.0 6.0 -3.188159070077745e-09\n",
            "progress:  74 w= 1.9999999998547615 loss= 1.8984796531526204e-19\n",
            "\tgrad:  1.0 2.0 -2.9047697580608656e-10\n",
            "\tgrad:  2.0 4.0 -1.1386696030513122e-09\n",
            "\tgrad:  3.0 6.0 -2.3570478902001923e-09\n",
            "progress:  75 w= 1.9999999998926234 loss= 1.0376765851119951e-19\n",
            "\tgrad:  1.0 2.0 -2.1475310418850313e-10\n",
            "\tgrad:  2.0 4.0 -8.418314934033333e-10\n",
            "\tgrad:  3.0 6.0 -1.7425900722400911e-09\n",
            "progress:  76 w= 1.9999999999206153 loss= 5.671751114309842e-20\n",
            "\tgrad:  1.0 2.0 -1.5876944203796484e-10\n",
            "\tgrad:  2.0 4.0 -6.223768167501476e-10\n",
            "\tgrad:  3.0 6.0 -1.2883241140571045e-09\n",
            "progress:  77 w= 1.9999999999413098 loss= 3.100089617511693e-20\n",
            "\tgrad:  1.0 2.0 -1.17380327679939e-10\n",
            "\tgrad:  2.0 4.0 -4.601314884666863e-10\n",
            "\tgrad:  3.0 6.0 -9.524754318590567e-10\n",
            "progress:  78 w= 1.9999999999566096 loss= 1.6944600977692705e-20\n",
            "\tgrad:  1.0 2.0 -8.678080476443029e-11\n",
            "\tgrad:  2.0 4.0 -3.4018121652934497e-10\n",
            "\tgrad:  3.0 6.0 -7.041780492045291e-10\n",
            "progress:  79 w= 1.9999999999679208 loss= 9.2616919156479e-21\n",
            "\tgrad:  1.0 2.0 -6.415845632545825e-11\n",
            "\tgrad:  2.0 4.0 -2.5150193039280566e-10\n",
            "\tgrad:  3.0 6.0 -5.206075570640678e-10\n",
            "progress:  80 w= 1.9999999999762834 loss= 5.062350511130293e-21\n",
            "\tgrad:  1.0 2.0 -4.743316850408519e-11\n",
            "\tgrad:  2.0 4.0 -1.8593837580738182e-10\n",
            "\tgrad:  3.0 6.0 -3.8489211817704927e-10\n",
            "progress:  81 w= 1.999999999982466 loss= 2.7669155644059242e-21\n",
            "\tgrad:  1.0 2.0 -3.5067948545020045e-11\n",
            "\tgrad:  2.0 4.0 -1.3746692673066718e-10\n",
            "\tgrad:  3.0 6.0 -2.845563784603655e-10\n",
            "progress:  82 w= 1.9999999999870368 loss= 1.5124150106147723e-21\n",
            "\tgrad:  1.0 2.0 -2.5926372160256506e-11\n",
            "\tgrad:  2.0 4.0 -1.0163070385260653e-10\n",
            "\tgrad:  3.0 6.0 -2.1037571684701106e-10\n",
            "progress:  83 w= 1.999999999990416 loss= 8.26683933105326e-22\n",
            "\tgrad:  1.0 2.0 -1.9167778475548403e-11\n",
            "\tgrad:  2.0 4.0 -7.51381179497912e-11\n",
            "\tgrad:  3.0 6.0 -1.5553425214420713e-10\n",
            "progress:  84 w= 1.9999999999929146 loss= 4.518126871054872e-22\n",
            "\tgrad:  1.0 2.0 -1.4170886686315498e-11\n",
            "\tgrad:  2.0 4.0 -5.555023108172463e-11\n",
            "\tgrad:  3.0 6.0 -1.1499068364173581e-10\n",
            "progress:  85 w= 1.9999999999947617 loss= 2.469467919185614e-22\n",
            "\tgrad:  1.0 2.0 -1.0476508549572827e-11\n",
            "\tgrad:  2.0 4.0 -4.106759377009439e-11\n",
            "\tgrad:  3.0 6.0 -8.500933290633839e-11\n",
            "progress:  86 w= 1.9999999999961273 loss= 1.349840097651456e-22\n",
            "\tgrad:  1.0 2.0 -7.745359908994942e-12\n",
            "\tgrad:  2.0 4.0 -3.036149109902908e-11\n",
            "\tgrad:  3.0 6.0 -6.285105769165966e-11\n",
            "progress:  87 w= 1.999999999997137 loss= 7.376551550022107e-23\n",
            "\tgrad:  1.0 2.0 -5.726086271806707e-12\n",
            "\tgrad:  2.0 4.0 -2.2446045022661565e-11\n",
            "\tgrad:  3.0 6.0 -4.646416584819235e-11\n",
            "progress:  88 w= 1.9999999999978835 loss= 4.031726170507742e-23\n",
            "\tgrad:  1.0 2.0 -4.233058348290797e-12\n",
            "\tgrad:  2.0 4.0 -1.659294923683774e-11\n",
            "\tgrad:  3.0 6.0 -3.4351188560322043e-11\n",
            "progress:  89 w= 1.9999999999984353 loss= 2.2033851437431755e-23\n",
            "\tgrad:  1.0 2.0 -3.1294966618133913e-12\n",
            "\tgrad:  2.0 4.0 -1.226752033289813e-11\n",
            "\tgrad:  3.0 6.0 -2.539835008974478e-11\n",
            "progress:  90 w= 1.9999999999988431 loss= 1.2047849775995315e-23\n",
            "\tgrad:  1.0 2.0 -2.3137047833188262e-12\n",
            "\tgrad:  2.0 4.0 -9.070078021977679e-12\n",
            "\tgrad:  3.0 6.0 -1.8779644506139448e-11\n",
            "progress:  91 w= 1.9999999999991447 loss= 6.5840863393251405e-24\n",
            "\tgrad:  1.0 2.0 -1.7106316363424412e-12\n",
            "\tgrad:  2.0 4.0 -6.7057470687359455e-12\n",
            "\tgrad:  3.0 6.0 -1.3882228699912957e-11\n",
            "progress:  92 w= 1.9999999999993676 loss= 3.5991747246272455e-24\n",
            "\tgrad:  1.0 2.0 -1.2647660696529783e-12\n",
            "\tgrad:  2.0 4.0 -4.957811938766099e-12\n",
            "\tgrad:  3.0 6.0 -1.0263789818054647e-11\n",
            "progress:  93 w= 1.9999999999995324 loss= 1.969312363793734e-24\n",
            "\tgrad:  1.0 2.0 -9.352518759442319e-13\n",
            "\tgrad:  2.0 4.0 -3.666400516522117e-12\n",
            "\tgrad:  3.0 6.0 -7.58859641791787e-12\n",
            "progress:  94 w= 1.9999999999996543 loss= 1.0761829795642296e-24\n",
            "\tgrad:  1.0 2.0 -6.914468997365475e-13\n",
            "\tgrad:  2.0 4.0 -2.7107205369247822e-12\n",
            "\tgrad:  3.0 6.0 -5.611511255665391e-12\n",
            "progress:  95 w= 1.9999999999997444 loss= 5.875191475205477e-25\n",
            "\tgrad:  1.0 2.0 -5.111466805374221e-13\n",
            "\tgrad:  2.0 4.0 -2.0037305148434825e-12\n",
            "\tgrad:  3.0 6.0 -4.1460168631601846e-12\n",
            "progress:  96 w= 1.999999999999811 loss= 3.2110109830478153e-25\n",
            "\tgrad:  1.0 2.0 -3.779199175824033e-13\n",
            "\tgrad:  2.0 4.0 -1.4814816040598089e-12\n",
            "\tgrad:  3.0 6.0 -3.064215547965432e-12\n",
            "progress:  97 w= 1.9999999999998603 loss= 1.757455879087579e-25\n",
            "\tgrad:  1.0 2.0 -2.793321129956894e-13\n",
            "\tgrad:  2.0 4.0 -1.0942358130705543e-12\n",
            "\tgrad:  3.0 6.0 -2.2648549702353193e-12\n",
            "progress:  98 w= 1.9999999999998967 loss= 9.608404711682446e-26\n",
            "\tgrad:  1.0 2.0 -2.0650148258027912e-13\n",
            "\tgrad:  2.0 4.0 -8.100187187665142e-13\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  99 w= 1.9999999999999236 loss= 5.250973729513143e-26\n",
            "predict (after training) 4 hours 7.9999999999996945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9Q-DtnFDaPtc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Back Propagation and Autograd with PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "AS2SmOufPebp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "outputId": "fcb8c259-9abf-4840-e85e-c5f47ffd7d1f"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "x_data = [1.0,2.0,3.0]\n",
        "y_data = [2.0,4.0,6.0]\n",
        "\n",
        "w = Variable(torch.Tensor([1.0]), requires_grad = True)\n",
        "\n",
        "def forward(x):\n",
        "  return x*w\n",
        "\n",
        "def loss(x,y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "print(\"predict (before training)\", 4, forward(4))\n",
        "\n",
        "for epoch in range(10):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    l = loss(x_val, y_val)\n",
        "    l.backward()\n",
        "    print(\"\\tgrad: \",x_val, y_val, grad)\n",
        "    w.data = w.data - 0.01 * w.grad.data\n",
        "    \n",
        "    w.grad.data.zero_()\n",
        "    \n",
        "  print(\"progress: \",epoch, \"w=\",w,\"loss=\",l)\n",
        "  \n",
        "print(\"predict (after training)\", \"4 hours\" , forward(4))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (before training) 4 tensor([ 4.])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  0 w= tensor([ 1.2607]) loss= tensor([ 7.3159])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  1 w= tensor([ 1.4534]) loss= tensor([ 3.9988])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  2 w= tensor([ 1.5959]) loss= tensor([ 2.1857])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  3 w= tensor([ 1.7012]) loss= tensor([ 1.1946])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  4 w= tensor([ 1.7791]) loss= tensor([ 0.6530])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  5 w= tensor([ 1.8367]) loss= tensor([ 0.3569])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  6 w= tensor([ 1.8793]) loss= tensor([ 0.1951])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  7 w= tensor([ 1.9107]) loss= tensor([ 0.1066])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  8 w= tensor([ 1.9340]) loss= tensor(1.00000e-02 *\n",
            "       [ 5.8279])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  9 w= tensor([ 1.9512]) loss= tensor(1.00000e-02 *\n",
            "       [ 3.1854])\n",
            "predict (after training) 4 hours tensor([ 7.8049])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v4mLXq39Pfsh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear Regression in PyTorch way\n",
        "\n",
        "## basic pytorch rythm\n",
        "1. Design your model using class with Variables\n",
        "2. Construct loss and optimizer (using pytorch api)\n",
        "3. Training cycle (forward, backward, update)\n",
        "\n",
        "### Other Optimizers\n",
        " - torch.optim.Adagrad\n",
        " - torch.optim.Adam\n",
        " - torch.optim.Adamax\n",
        " - torch.optim.ASGD\n",
        " - torch.optim.LBFGS\n",
        " - torch.optim.RMSprop\n",
        " - torch.optim.Rprop\n",
        " - torch.optim.SGD"
      ]
    },
    {
      "metadata": {
        "id": "67b2i9OaaXbC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "a17b31be-e063-4549-81fe-1ac7e6baca04"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "# step1 Design your model using class with Variables\n",
        "\n",
        "x_data = Variable(torch.Tensor([[1.0],[2.0],[3.0]]))\n",
        "y_data = Variable(torch.Tensor([[2.0],[4.0],[6.0]]))\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1,1) # one input, one output\n",
        "    \n",
        "  def forward(self, x):\n",
        "    y_pred = self.linear(x)\n",
        "    return y_pred\n",
        "\n",
        "# our model instance\n",
        "model = Model()\n",
        "\n",
        "# step2 Construct loss and optimizer (using pytorch api)\n",
        "\n",
        "criterion = torch.nn.MSELoss(size_average=False)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# stpe3 Training cycle: forwad, loss, backward, step\n",
        "\n",
        "for epoch in range(500):\n",
        "  y_pred = model(x_data)\n",
        "  \n",
        "  loss = criterion(y_pred, y_data)\n",
        "  if epoch % 50 == 0:\n",
        "    print(epoch, loss.item())\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "hour_var = Variable(torch.Tensor([[4.0]]))\n",
        "print(\"predict (after training)\", 4, model.forward(hour_var).item())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 108.97834777832031\n",
            "50 0.41449713706970215\n",
            "100 0.20099374651908875\n",
            "150 0.09746389091014862\n",
            "200 0.04726118594408035\n",
            "250 0.022917453199625015\n",
            "300 0.01111286785453558\n",
            "350 0.005388767924159765\n",
            "400 0.002613053424283862\n",
            "450 0.0012670927681028843\n",
            "predict (after training) 4 7.971298694610596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oGqzqg2jPf65",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression with pytorch\n",
        "\n",
        "\n",
        "sigmoid -> 0~1\n",
        "\n",
        "loss function should be different -> cross entropy loss"
      ]
    },
    {
      "metadata": {
        "id": "cvVjiNUQjP6r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "8c9db3b2-350d-4da4-eeac-ca98a16129d6"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0], [4.0]]))\n",
        "y_data = Variable(torch.Tensor([[0.], [0.], [1.], [1.]]))\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1,1)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    y_pred = F.sigmoid(self.linear(x))\n",
        "    return y_pred\n",
        "  \n",
        "model = Model()\n",
        "\n",
        "criterion = torch.nn.BCELoss(size_average=True)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(1000):\n",
        "  y_pred = model(x_data)\n",
        "  \n",
        "  loss = criterion(y_pred, y_data)\n",
        "  if epoch % 50 == 0:\n",
        "    print(epoch, loss.item())\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "hour_var = Variable(torch.Tensor([[1.0]]))\n",
        "print(\"predict 1 hour \",1.0,model(hour_var).item()>0.5)\n",
        "hour_var = Variable(torch.Tensor([[7.0]]))\n",
        "print(\"predict 7 hour \",1.0,model(hour_var).item()>0.5)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.7944773435592651\n",
            "50 0.7147016525268555\n",
            "100 0.6833253502845764\n",
            "150 0.6656846404075623\n",
            "200 0.6512323617935181\n",
            "250 0.6377397775650024\n",
            "300 0.6247899532318115\n",
            "350 0.6122987866401672\n",
            "400 0.6002395749092102\n",
            "450 0.5885959267616272\n",
            "500 0.5773528218269348\n",
            "550 0.5664961338043213\n",
            "600 0.5560117959976196\n",
            "650 0.5458860993385315\n",
            "700 0.5361055731773376\n",
            "750 0.5266572833061218\n",
            "800 0.5175283551216125\n",
            "850 0.5087064504623413\n",
            "900 0.5001795291900635\n",
            "950 0.4919361174106598\n",
            "predict 1 hour  1.0 False\n",
            "predict 7 hour  1.0 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-H9m9L7J9Xwz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Wide and Deep"
      ]
    },
    {
      "metadata": {
        "id": "3DBaDdUiB1Mi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4795773b-b7e4-4e72-9051-752ecd991019"
      },
      "cell_type": "code",
      "source": [
        "cd ../../"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HCBmy3gvFfQ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2fd371b2-898d-4a23-baf5-1acfd4f23511"
      },
      "cell_type": "code",
      "source": [
        "cd data"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w7O-u7fnF0PJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "5609831b-75cd-4e35-b113-27328720d91d"
      },
      "cell_type": "code",
      "source": [
        "!wget https://github.com/hunkim/PyTorchZeroToAll/raw/master/data/diabetes.csv.gz"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-07-13 07:02:39--  https://github.com/hunkim/PyTorchZeroToAll/raw/master/data/diabetes.csv.gz\r\n",
            "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\r\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/diabetes.csv.gz [following]\n",
            "--2018-07-13 07:02:39--  https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/diabetes.csv.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13539 (13K) [application/octet-stream]\n",
            "Saving to: ‘diabetes.csv.gz’\n",
            "\n",
            "diabetes.csv.gz     100%[===================>]  13.22K  51.0KB/s    in 0.3s    \n",
            "\n",
            "2018-07-13 07:02:43 (51.0 KB/s) - ‘diabetes.csv.gz’ saved [13539/13539]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8vFCj4QqF9hr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dfe2190b-95ed-4b78-e69a-dadf973f4870"
      },
      "cell_type": "code",
      "source": [
        "cd ../"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Go25ohFkC1H8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1817
        },
        "outputId": "88388be9-c6d0-47ae-d00c-3206fc9770eb"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "xy = np.loadtxt('./data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
        "x_data = Variable(torch.from_numpy(xy[:, 0:-1]))\n",
        "y_data = Variable(torch.from_numpy(xy[:, [-1]]))\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.l1 = torch.nn.Linear(8,6)\n",
        "    self.l2 = torch.nn.Linear(6,4)\n",
        "    self.l3 = torch.nn.Linear(4,2)\n",
        "    self.l4 = torch.nn.Linear(2,1)\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out1 = self.sigmoid(self.l1(x))\n",
        "    out2 = self.sigmoid(self.l2(out1))\n",
        "    out3 = self.sigmoid(self.l3(out2))\n",
        "    y_pred = self.sigmoid(self.l4(out3))\n",
        "    return y_pred\n",
        "\n",
        "model = Model()\n",
        "\n",
        "criterion = torch.nn.BCELoss(size_average=True)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n",
        "\n",
        "for epoch in range(100):\n",
        "  y_pred = model(x_data)\n",
        "  \n",
        "  loss = criterion(y_pred, y_data)\n",
        "  print(epoch, loss.item())\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.7357483506202698\n",
            "1 0.7218292355537415\n",
            "2 0.7100878357887268\n",
            "3 0.7001838088035583\n",
            "4 0.6918265223503113\n",
            "5 0.6847731471061707\n",
            "6 0.6788142919540405\n",
            "7 0.6737775206565857\n",
            "8 0.6695171594619751\n",
            "9 0.6659101247787476\n",
            "10 0.6628547310829163\n",
            "11 0.660262942314148\n",
            "12 0.6580643057823181\n",
            "13 0.6561964154243469\n",
            "14 0.6546095609664917\n",
            "15 0.6532593369483948\n",
            "16 0.6521099209785461\n",
            "17 0.6511311531066895\n",
            "18 0.6502965688705444\n",
            "19 0.6495855450630188\n",
            "20 0.6489786505699158\n",
            "21 0.648460328578949\n",
            "22 0.6480172872543335\n",
            "23 0.6476388573646545\n",
            "24 0.6473155617713928\n",
            "25 0.6470389366149902\n",
            "26 0.6468020677566528\n",
            "27 0.6465994119644165\n",
            "28 0.6464259624481201\n",
            "29 0.6462767124176025\n",
            "30 0.6461496353149414\n",
            "31 0.6460407972335815\n",
            "32 0.645946741104126\n",
            "33 0.6458666920661926\n",
            "34 0.6457980871200562\n",
            "35 0.6457388401031494\n",
            "36 0.6456878185272217\n",
            "37 0.6456445455551147\n",
            "38 0.6456066966056824\n",
            "39 0.6455749273300171\n",
            "40 0.6455476880073547\n",
            "41 0.6455237865447998\n",
            "42 0.6455032825469971\n",
            "43 0.6454861164093018\n",
            "44 0.6454711556434631\n",
            "45 0.6454585194587708\n",
            "46 0.6454474925994873\n",
            "47 0.6454373598098755\n",
            "48 0.6454290747642517\n",
            "49 0.6454225182533264\n",
            "50 0.6454160809516907\n",
            "51 0.6454107165336609\n",
            "52 0.6454060077667236\n",
            "53 0.6454023122787476\n",
            "54 0.6453993916511536\n",
            "55 0.6453959345817566\n",
            "56 0.6453936100006104\n",
            "57 0.6453917622566223\n",
            "58 0.6453903913497925\n",
            "59 0.645388126373291\n",
            "60 0.6453865766525269\n",
            "61 0.6453854441642761\n",
            "62 0.6453843712806702\n",
            "63 0.6453837156295776\n",
            "64 0.6453827023506165\n",
            "65 0.6453825235366821\n",
            "66 0.6453812122344971\n",
            "67 0.6453805565834045\n",
            "68 0.6453801393508911\n",
            "69 0.645379900932312\n",
            "70 0.6453794240951538\n",
            "71 0.645379364490509\n",
            "72 0.645379364490509\n",
            "73 0.6453791260719299\n",
            "74 0.6453782320022583\n",
            "75 0.6453790068626404\n",
            "76 0.6453778743743896\n",
            "77 0.6453776359558105\n",
            "78 0.6453770995140076\n",
            "79 0.6453779935836792\n",
            "80 0.6453773975372314\n",
            "81 0.6453769207000732\n",
            "82 0.6453772783279419\n",
            "83 0.6453772783279419\n",
            "84 0.6453767418861389\n",
            "85 0.645376443862915\n",
            "86 0.6453765630722046\n",
            "87 0.6453762650489807\n",
            "88 0.6453760266304016\n",
            "89 0.6453765630722046\n",
            "90 0.6453766822814941\n",
            "91 0.6453763842582703\n",
            "92 0.6453757882118225\n",
            "93 0.6453762054443359\n",
            "94 0.6453759074211121\n",
            "95 0.6453753709793091\n",
            "96 0.6453759670257568\n",
            "97 0.645375669002533\n",
            "98 0.6453760862350464\n",
            "99 0.6453762054443359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GlKjSY8YIyPb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# PyTorch DataLoader\n",
        "\n",
        "## Batch Normalization\n",
        "```python\n",
        "for epoch in range(training_epochs):\n",
        "  # Loop over all batches\n",
        "  for i in range(total_batch):\n",
        "    \n",
        "```\n",
        "\n",
        "In the neural networks terminology\n",
        "\n",
        "- one epoch = one forward & backward of all the training examples\n",
        "- batch size = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need\n",
        "- number of iterations = number of passes, each pass using batch size number of examples. To be clear, one pass = one forward + one backward\n",
        "\n",
        "Example: if you have 1000 training examples, and your batch size is 100, then it will take 10 iterations to complete 1 epoch\n",
        "\n",
        "## 3 steps of DataLoader\n",
        "1. \\__init__ : Download, read data, etc.\n",
        "2. \\__getitem__ : return one item on the index\n",
        "3. \\__len__ : return data length"
      ]
    },
    {
      "metadata": {
        "id": "kywuINSrIyvr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1817
        },
        "outputId": "73778c4f-00e1-4104-8c03-a1e31a93e9e7"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class DiabetesDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    xy = np.loadtxt('./data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
        "    self.len = xy.shape[0]\n",
        "    self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
        "    self.y_data = torch.from_numpy(xy[:,[-1]])\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return self.x_data[index], self.y_data[index]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "dataset = DiabetesDataset()\n",
        "train_loader = DataLoader(dataset = dataset,\n",
        "                         batch_size = 32,\n",
        "                         shuffle = True,\n",
        "                         num_workers = 2)\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.l1 = torch.nn.Linear(8,6)\n",
        "    self.l2 = torch.nn.Linear(6,4)\n",
        "    self.l3 = torch.nn.Linear(4,2)\n",
        "    self.l4 = torch.nn.Linear(2,1)\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out1 = self.sigmoid(self.l1(x))\n",
        "    out2 = self.sigmoid(self.l2(out1))\n",
        "    out3 = self.sigmoid(self.l3(out2))\n",
        "    y_pred = self.sigmoid(self.l4(out3))\n",
        "    return y_pred\n",
        "\n",
        "model = Model()\n",
        "\n",
        "criterion = torch.nn.BCELoss(size_average=True)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n",
        "\n",
        "for epoch in range(100):\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    inputs, labels = data\n",
        "    inputs, labels = Variable(inputs), Variable(labels)\n",
        "    \n",
        "    y_pred = model(inputs)\n",
        "    \n",
        "    loss = criterion(y_pred, labels)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(epoch, loss.item())"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.648794949054718\n",
            "1 0.6213365197181702\n",
            "2 0.6461300849914551\n",
            "3 0.5926733016967773\n",
            "4 0.7031210064888\n",
            "5 0.6730528473854065\n",
            "6 0.6464548707008362\n",
            "7 0.5140062570571899\n",
            "8 0.6736871600151062\n",
            "9 0.5888544917106628\n",
            "10 0.5915401577949524\n",
            "11 0.6461012363433838\n",
            "12 0.6728104948997498\n",
            "13 0.5903851389884949\n",
            "14 0.5399762392044067\n",
            "15 0.5935772061347961\n",
            "16 0.619015097618103\n",
            "17 0.6459960341453552\n",
            "18 0.646045982837677\n",
            "19 0.7050577998161316\n",
            "20 0.646075963973999\n",
            "21 0.7635181546211243\n",
            "22 0.6981696486473083\n",
            "23 0.7042323350906372\n",
            "24 0.6712115406990051\n",
            "25 0.7011411190032959\n",
            "26 0.6728771924972534\n",
            "27 0.672528088092804\n",
            "28 0.5360145568847656\n",
            "29 0.7357597351074219\n",
            "30 0.5610777735710144\n",
            "31 0.5641273260116577\n",
            "32 0.563590407371521\n",
            "33 0.7046870589256287\n",
            "34 0.5867522358894348\n",
            "35 0.6461242437362671\n",
            "36 0.784765899181366\n",
            "37 0.6463790535926819\n",
            "38 0.6461242437362671\n",
            "39 0.698178231716156\n",
            "40 0.7333426475524902\n",
            "41 0.7027037143707275\n",
            "42 0.5911239981651306\n",
            "43 0.6166807413101196\n",
            "44 0.6460162997245789\n",
            "45 0.6198229193687439\n",
            "46 0.5958430767059326\n",
            "47 0.7532081604003906\n",
            "48 0.6738279461860657\n",
            "49 0.5634032487869263\n",
            "50 0.7926194667816162\n",
            "51 0.5677016377449036\n",
            "52 0.6460132598876953\n",
            "53 0.6727021932601929\n",
            "54 0.6185165643692017\n",
            "55 0.7043436169624329\n",
            "56 0.5191279053688049\n",
            "57 0.6461740732192993\n",
            "58 0.566677987575531\n",
            "59 0.5978387594223022\n",
            "60 0.7314803004264832\n",
            "61 0.7015030980110168\n",
            "62 0.6162841320037842\n",
            "63 0.6735124588012695\n",
            "64 0.6734366416931152\n",
            "65 0.70343017578125\n",
            "66 0.5649487972259521\n",
            "67 0.5941049456596375\n",
            "68 0.7895611524581909\n",
            "69 0.7002732753753662\n",
            "70 0.6460031867027283\n",
            "71 0.5702381730079651\n",
            "72 0.5627611875534058\n",
            "73 0.5737795233726501\n",
            "74 0.5968956351280212\n",
            "75 0.6756047606468201\n",
            "76 0.5407779812812805\n",
            "77 0.5619282722473145\n",
            "78 0.6733952760696411\n",
            "79 0.5339438319206238\n",
            "80 0.7356340289115906\n",
            "81 0.518516480922699\n",
            "82 0.673785924911499\n",
            "83 0.5944626331329346\n",
            "84 0.5919632315635681\n",
            "85 0.7000965476036072\n",
            "86 0.6206748485565186\n",
            "87 0.6213449239730835\n",
            "88 0.5875396132469177\n",
            "89 0.7082452178001404\n",
            "90 0.5920055508613586\n",
            "91 0.5585992932319641\n",
            "92 0.5657880902290344\n",
            "93 0.6750410199165344\n",
            "94 0.6758177280426025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "95 0.7664047479629517\n",
            "96 0.6171246767044067\n",
            "97 0.6154446005821228\n",
            "98 0.6742143034934998\n",
            "99 0.6735849380493164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wXx6VwWCIy8r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Softmax\n",
        "## MNIST\n",
        "## Softmax\n",
        "probability\n",
        "x -Linear Model + logistic-> z -(softmax) -> y_hat(probability)\n",
        " \n",
        "### Loss(Cost) Function - Cross Entropy\n",
        "y will be one-hot encoded labes, which means one class is one and the others are zeroes."
      ]
    },
    {
      "metadata": {
        "id": "DJmUEc6YIzSD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "945513e1-a032-4450-c5d0-6effff36849d"
      },
      "cell_type": "code",
      "source": [
        "# cross entropy\n",
        "import numpy as np\n",
        "\n",
        "Y = np.array([1,0,0])\n",
        "\n",
        "Y_pred1 = np.array([0.7,0.2,0.1])\n",
        "Y_pred2 = np.array([0.1,0.3,0.6])\n",
        "\n",
        "print(\"loss1 = \",np.sum(-Y*np.log(Y_pred1))) # small loss\n",
        "print(\"loss1 = \",np.sum(-Y*np.log(Y_pred2))) # big loss"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss1 =  0.35667494393873245\n",
            "loss1 =  2.3025850929940455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6gimP8znIzW5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c172c91e-541c-46d7-8dab-06a54f6a46a7"
      },
      "cell_type": "code",
      "source": [
        "# cross entropy in PyTorch\n",
        "\n",
        "# Softmax + CrossEntropy (log softmax + NLLLoss)\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Input is class, not one-hot\n",
        "Y = Variable(torch.LongTensor([0]), requires_grad=False)\n",
        "\n",
        "# Y_pred are logits (not softmax)\n",
        "Y_pred1 = Variable(torch.Tensor([[2.0, 1.0, 0.1]]))\n",
        "Y_pred2 = Variable(torch.Tensor([[0.5, 2.0, 0.3]]))\n",
        "\n",
        "l1 = loss(Y_pred1,Y)\n",
        "l2 = loss(Y_pred2,Y)\n",
        "\n",
        "print(\"PyTorch Loss1 = \",l1.data)\n",
        "print(\"PyTorch Loss2 = \",l2.data)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch Loss1 =  tensor(0.4170)\n",
            "PyTorch Loss2 =  tensor(1.8406)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9jW-eGCcIzcw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MNIST Softmax"
      ]
    },
    {
      "metadata": {
        "id": "SClQmmF4xgRt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 15805
        },
        "outputId": "5ee7d410-a7ca-4830-eae4-c74d0b1a23a6"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Training settings\n",
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/mnist/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data/mnist',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.l1 = nn.Linear(784,520)\n",
        "    self.l2 = nn.Linear(520,320)\n",
        "    self.l3 = nn.Linear(320,240)\n",
        "    self.l4 = nn.Linear(240,120)\n",
        "    self.l5 = nn.Linear(120,10)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 784) # flatten the data (n,1,28,28) -> (n,784)\n",
        "    x = F.relu(self.l1(x))\n",
        "    x = F.relu(self.l2(x))\n",
        "    x = F.relu(self.l3(x))\n",
        "    x = F.relu(self.l4(x))\n",
        "    return self.l5(x)\n",
        "  \n",
        "model = Net()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
        "\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = Variable(data), Variable(target)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss=criterion(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.data[0]))\n",
        "    \n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  for data, target in test_loader:\n",
        "    data, target = Variable(data, volatile=True), Variable(target)\n",
        "    output = model(data)\n",
        "    # sum up batch loss\n",
        "    test_loss += criterion(output, target).data[0]\n",
        "    # get the index of the max\n",
        "    pred = output.data.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "  \n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "  \n",
        "  \n",
        "for epoch in range(1,10):\n",
        "  train(epoch)\n",
        "  test()\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:64: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.306334\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.309829\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.302271\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.308864\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.306837\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.301304\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.291209\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.298502\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.314076\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.300258\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.306045\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.295224\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.299220\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.294944\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.298843\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.297554\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.296324\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.304773\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.300107\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.294209\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.299040\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.294793\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.299771\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.299562\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.292693\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.291731\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.296990\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.292118\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.289996\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.292850\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.285558\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.283973\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.287312\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 2.292663\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.295463\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.295493\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.279225\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.287176\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.286063\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.283204\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.281348\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.284397\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 2.282949\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.281005\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.278327\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.281377\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 2.271119\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 2.272868\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.273669\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.274964\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.270020\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 2.274610\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 2.280053\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 2.267589\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 2.268034\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 2.255182\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 2.259943\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 2.254610\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 2.253086\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 2.247055\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.250449\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 2.234676\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 2.237960\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 2.238931\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 2.235901\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 2.225828\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 2.227491\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 2.197114\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 2.194256\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 2.167390\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.175799\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 2.162223\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 2.122102\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 2.112427\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 2.123760\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 2.075200\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 2.048800\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 1.983633\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 2.026086\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 1.964025\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.938531\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 1.861745\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.871358\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 1.934842\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.783351\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.743197\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 1.776950\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 1.733285\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 1.735552\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.710416\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.660220\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 1.602794\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.535258\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 1.488007\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:74: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0237, Accuracy: 6006/10000 (60%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.542452\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 1.414105\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 1.436924\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 1.344831\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 1.336898\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 1.336825\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 1.311203\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 1.339655\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 1.186616\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 1.209262\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.970312\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.960423\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.931578\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.972363\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 1.040893\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.856631\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.731365\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 1.157770\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.863278\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.817205\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.847946\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.687437\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.905674\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.693585\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.778736\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.804157\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.684396\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.780222\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.750510\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.731887\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.680698\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.717240\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.904236\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.738691\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.496650\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.893913\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.508203\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.591507\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.729901\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.513651\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.663643\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.404576\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.668951\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.400712\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.512476\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.446191\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.863807\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.681477\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.590060\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.372088\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.517245\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.611379\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.523966\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.436094\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.392200\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.570667\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.544984\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.795834\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.570064\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.593954\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.695741\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.485877\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.553579\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.626063\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.546440\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.531897\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.497237\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.524356\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.549960\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.390782\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.605988\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.533808\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.392053\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.448454\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.490968\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.512574\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.537299\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.411325\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.606841\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.389216\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.541012\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.529982\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.260120\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.407333\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.348451\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.586463\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.519087\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.510942\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.325055\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.517221\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.519537\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.546670\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.241736\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.465658\n",
            "\n",
            "Test set: Average loss: 0.0073, Accuracy: 8626/10000 (86%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.409848\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.451176\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.510245\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.339991\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.346584\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.423361\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.478262\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.666268\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.495077\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.313334\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.425497\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.399218\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.659498\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.277961\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.455236\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.469233\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.345889\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.582888\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.340304\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.670523\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.329462\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.312388\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.361999\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.441025\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.503198\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.335348\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.652356\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.374063\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.583629\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.674988\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.615998\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.284957\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.366766\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.236764\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.201354\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.627737\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.426080\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.288544\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.326106\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.469477\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.325455\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.425731\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.282303\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.340788\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.295374\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.395251\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.428026\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.374670\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.414662\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.449120\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.444995\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.319419\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.294567\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.278933\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.277653\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.255991\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.359232\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.370748\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.379403\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.278044\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.318406\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.244809\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.400479\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.220499\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.327846\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.381395\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.261901\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.312043\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.279445\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.186446\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.265409\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.523506\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.428339\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.359362\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.308742\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.243763\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.446014\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.290237\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.263646\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.250338\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.346051\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.248075\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.194692\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.358751\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.224724\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.302921\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.342887\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.127746\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.184391\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.325512\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.325013\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.253371\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.232574\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.377950\n",
            "\n",
            "Test set: Average loss: 0.0046, Accuracy: 9134/10000 (91%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.292083\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.344638\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.278993\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.304131\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.248807\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.337184\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.206920\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.159659\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.492746\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.311929\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.142943\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.270868\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.357539\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.345414\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.518992\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.531610\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.574574\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.283186\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.159334\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.329962\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.367897\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.459119\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.293719\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.398755\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.194790\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.230722\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.318734\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.285023\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.206708\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.271716\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.358109\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.360954\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.252614\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.201864\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.331939\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.208496\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.330085\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.356773\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.177322\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.196807\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.307888\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.468993\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.203715\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.491972\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.167827\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.131863\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.198564\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.192504\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.308916\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.343648\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.127655\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.367882\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.347144\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.267173\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.159727\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.331810\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.480725\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.130523\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.410187\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.254707\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.268701\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.212589\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.270047\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.234030\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.093098\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.218905\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.209335\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.131503\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.166853\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.092146\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.153939\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.195101\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.294258\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.243618\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.128805\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.534724\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.263208\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.182464\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.097364\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.216386\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.170568\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.350405\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.131391\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.233096\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.454814\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.249990\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.339123\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.156935\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.211281\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.246609\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.219996\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.102037\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.376042\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.408234\n",
            "\n",
            "Test set: Average loss: 0.0036, Accuracy: 9354/10000 (93%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.055276\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.149256\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.187986\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.180669\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.089046\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.182608\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.227226\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.380999\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.172114\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.148300\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.217889\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.106839\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.296792\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.115513\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.175975\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.276748\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.099573\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.137656\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.179977\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.133033\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.084604\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.350274\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.107062\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.125796\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.158680\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.105040\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.150027\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.130932\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.134829\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.182703\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.132303\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.369460\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.066889\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.200494\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.208226\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.161561\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.111407\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.235660\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.153381\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.260976\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.156060\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.199124\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.272372\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.346703\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.155172\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.188926\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.134011\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.128031\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.226446\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.205849\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.143829\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.253901\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.255735\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.127547\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.106869\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.177829\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.373819\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.082086\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.158504\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.117879\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.084269\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.093583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.149621\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.229510\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.225315\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.153422\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.101281\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.141725\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.169494\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.244180\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.289985\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.104721\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.091248\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.117574\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.141146\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.041742\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.122283\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.148558\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.185933\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.315731\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.112374\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.209978\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.186936\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.053103\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.179257\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.277870\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.107935\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.264855\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.158690\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.197793\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.304378\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.231971\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.288006\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.150534\n",
            "\n",
            "Test set: Average loss: 0.0028, Accuracy: 9502/10000 (95%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.237303\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.056673\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.144378\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.239742\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.153480\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.158720\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.139601\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.169677\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.129985\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.281960\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.070366\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.235952\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.099095\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.159396\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.101316\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.172353\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.139521\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.112603\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.098204\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.112817\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.094340\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.146676\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.187825\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.085409\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.194703\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.065002\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.073306\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.077059\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.162564\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.100026\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.074596\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.195299\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.259780\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.052539\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.168290\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.144301\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.153148\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.291493\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.181477\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.092407\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.115612\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.378632\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.145676\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.321948\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.236345\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.278831\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.162106\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.162982\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.079688\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.125104\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.022696\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.158132\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.192699\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.186330\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.156526\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.129914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.098134\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.136036\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.127512\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.167329\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.097320\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.073047\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.204097\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.066249\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.145663\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.130812\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.111309\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.186512\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.123947\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.357607\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.091000\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.308836\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.359191\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.151941\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.135344\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.236656\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.113102\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.124826\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.177666\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.251870\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.128310\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.394936\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.171820\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.048255\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.167984\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.100873\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.124845\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.183008\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.188874\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.061045\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.085174\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.061191\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.126940\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.068676\n",
            "\n",
            "Test set: Average loss: 0.0024, Accuracy: 9560/10000 (95%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.123568\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.146677\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.059296\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.054157\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.148012\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.062243\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.146204\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.048547\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.147353\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.131394\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.140421\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.098031\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.156160\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.060860\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.137866\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.154195\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.201791\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.167421\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.134120\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.184730\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.145794\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.094350\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.081015\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.127679\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.095148\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.057563\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.055455\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.156984\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.152317\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.038591\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.226115\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.061496\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.204470\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.180556\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.334070\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.120678\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.099839\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.101735\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.106879\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.149986\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.044785\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.080107\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.036954\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.211107\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.299594\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.064850\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.122766\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.084347\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.179843\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.130925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.102735\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.240271\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.090354\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.055085\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.359076\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.331029\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.185470\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.166233\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.204220\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.048164\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.080791\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.160903\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.114828\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.137211\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.128852\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.129362\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.089879\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.089977\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.073365\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.179596\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.182475\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.066597\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.038829\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.042995\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.076425\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.268964\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.176316\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.170653\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.224843\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.035076\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.073007\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.068980\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.057046\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.076648\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.264746\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.079543\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.049164\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.137742\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.054670\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.122277\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.080891\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.103301\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.129044\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.066072\n",
            "\n",
            "Test set: Average loss: 0.0020, Accuracy: 9636/10000 (96%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.177988\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.063501\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.116887\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.140255\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.111685\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.144899\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.181893\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.095462\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.059165\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.031515\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.119520\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.050506\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.122624\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.188144\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.023118\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.058549\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.237602\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.025147\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.182670\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.112911\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.101649\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.159392\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.073757\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.197690\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.126483\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.125613\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.330605\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.150432\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.122835\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.115275\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.128392\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.187461\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.118322\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.098341\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.111864\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.160491\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.094219\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.056037\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.058777\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.029205\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.123893\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.049591\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.079158\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.126425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.042678\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.036897\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.025117\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.078235\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.142140\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.419422\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.114778\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.041368\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.198620\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.045085\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.091482\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.071719\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.222049\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.056298\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.100866\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.082707\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.065763\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.114655\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.223181\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.192647\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.037394\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.040113\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.060317\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.246449\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.370980\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.129070\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.083891\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.106203\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.085605\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.116229\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.124135\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.206548\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.120908\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.116199\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.074855\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.139806\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.020736\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.158016\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.100884\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.265911\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.123881\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.122783\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.292073\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.101749\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.124096\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.097723\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.181523\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.130036\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.102171\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.195897\n",
            "\n",
            "Test set: Average loss: 0.0017, Accuracy: 9662/10000 (96%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.236624\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.096349\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.134536\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.115006\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.037646\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.362130\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.060155\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.078247\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.051120\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.136561\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.045595\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.067895\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.058531\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.046737\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.171267\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.028282\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.140682\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.079607\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.083156\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.107058\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.060589\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.125804\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.094512\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.270399\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.064633\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.066081\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.070855\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.045397\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.115108\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.045422\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.123342\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.130310\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.100873\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.152654\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.018467\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.192007\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.212883\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.050507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.030710\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.051464\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.157380\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.071997\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.100140\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.054861\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.190482\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.058952\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.068964\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.101033\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.033069\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.076644\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.032200\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.060681\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.081223\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.099959\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.101244\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.041294\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.014635\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.062021\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.118771\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.036886\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.147644\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.140308\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.103220\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.036653\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.037583\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.073624\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.133138\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.023985\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.070005\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.075323\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.087529\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.031640\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.112378\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.079220\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.123654\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.028366\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.175048\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.094437\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.070341\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.388232\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.117975\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.079789\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.051957\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.070111\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.167045\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.099378\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.121176\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.193393\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.157340\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.093832\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.074391\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.063020\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.097154\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.054684\n",
            "\n",
            "Test set: Average loss: 0.0019, Accuracy: 9636/10000 (96%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0nLbm_3vxgZj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN MNIST"
      ]
    },
    {
      "metadata": {
        "id": "6R16sd-kxggB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 15931
        },
        "outputId": "e03e3aca-60a1-4272-ea78-95b58f6d6918"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Training settings\n",
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/mnist/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data/mnist',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1,10,kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(10,20,kernel_size=5)\n",
        "    self.mp = nn.MaxPool2d(2)\n",
        "    self.fc = nn.Linear(320,10)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    in_size = x.size(0) # it will be batch size(= 64)\n",
        "    x = F.relu(self.mp(self.conv1(x)))\n",
        "    x = F.relu(self.mp(self.conv2(x)))\n",
        "    x = x.view(in_size, -1) # Flatten the tensor\n",
        "    x = self.fc(x)\n",
        "    return F.log_softmax(x)\n",
        "  \n",
        "model = Net()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
        "\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = Variable(data), Variable(target)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss=criterion(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.data[0]))\n",
        "    \n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  for data, target in test_loader:\n",
        "    data, target = Variable(data, volatile=True), Variable(target)\n",
        "    output = model(data)\n",
        "    # sum up batch loss\n",
        "    test_loss += criterion(output, target).data[0]\n",
        "    # get the index of the max\n",
        "    pred = output.data.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "  \n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "  \n",
        "  \n",
        "for epoch in range(1,10):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.309991\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:63: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.297201\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.285767\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.283107\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.264187\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.259223\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.238337\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.209724\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.188255\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.136418\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.043453\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.908784\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.669399\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.500570\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.292404\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.019720\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.921914\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.845721\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.504024\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.697794\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.444514\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.559681\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.724844\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.430557\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.711728\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.478566\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.381231\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.340986\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.449913\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.287599\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.331087\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.227327\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.404156\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.372725\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.412813\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.428040\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.392914\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.215380\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.435364\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.263203\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.216489\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.325587\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.205102\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.391512\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.445008\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.279680\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.300018\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.156729\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.357561\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.484443\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.212224\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.215966\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.210320\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.242184\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.227748\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.275545\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.320960\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.433516\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.194640\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.237273\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.181957\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.348086\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.255043\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.163039\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.359049\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.174236\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.139521\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.185175\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.307809\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.062331\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.161151\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.170992\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.203303\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.360714\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.322090\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.274134\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.297299\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.210541\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.485666\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.306454\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.293520\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.254647\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.175456\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.195515\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.367280\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.123105\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.240115\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.264880\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.324433\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.141501\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.328560\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.237504\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.241757\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.269117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:73: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0026, Accuracy: 9516/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.202822\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.163061\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.184862\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.248304\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.155054\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.282059\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.243484\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.189671\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.233211\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.240475\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.287806\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.109980\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.075736\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.093522\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.173941\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.193251\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.284812\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.124818\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.091599\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.161812\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.213944\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.246671\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.173192\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.152217\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.088736\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.226009\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.253047\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.093855\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.102316\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.173144\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.203972\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.141695\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.116137\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.149656\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.097786\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.161541\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.135932\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.141745\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.107297\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.107980\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.284445\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.132211\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.068363\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.468539\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.129651\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.130329\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.203148\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.440021\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.110482\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.183226\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.123990\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.063014\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.105509\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.120969\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.133988\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.171363\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.313365\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.071782\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.127393\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.260887\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.193743\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.109561\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.174676\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.124121\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.054067\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.135849\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.136100\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.043876\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.094227\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.074401\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.091239\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.090217\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.047012\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.329586\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.086899\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.312388\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.100028\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.091200\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.090713\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.126702\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.150017\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.036749\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.186588\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.029400\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.279342\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.362196\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.084986\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.093624\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.204578\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.353688\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.106621\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.209289\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.128535\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.153586\n",
            "\n",
            "Test set: Average loss: 0.0018, Accuracy: 9636/10000 (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.095022\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.088479\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.077330\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.114766\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.154135\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.175986\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.141616\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.263992\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.062919\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.099693\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.276752\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.134809\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.192457\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.276365\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.156501\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.158746\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.087154\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.224277\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.066455\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.055126\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.138108\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.055499\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.091469\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.121779\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.107490\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.109196\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.064849\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.030260\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.085431\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.137417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.280665\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.083347\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.112412\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.187633\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.078290\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.144023\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.044879\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.082057\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.034349\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.068586\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.188014\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.151901\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.138773\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.079159\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.207012\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.166879\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.127080\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.071359\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.141123\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.073799\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.101086\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.026484\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.050819\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.096806\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.163006\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.038552\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.070343\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.056232\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.175969\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.094525\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.104050\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.140365\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.088040\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.109525\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.025710\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.103221\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.132214\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.088897\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.125712\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.179834\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.025802\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.292633\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.033822\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.126411\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.108774\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.053624\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.051802\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.158894\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.051270\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.188710\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.042388\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.047208\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.107139\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.064874\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.108035\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.101905\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.063316\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.066858\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.119723\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.054455\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.031169\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.202510\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.069985\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.168164\n",
            "\n",
            "Test set: Average loss: 0.0015, Accuracy: 9691/10000 (96%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.110576\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.106700\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.048269\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.058555\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.100935\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.105772\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.092719\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.074941\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.137125\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.066296\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.112858\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.052773\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.148377\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.128309\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.155408\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.022183\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.186104\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.136079\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.042884\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.110697\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.052170\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.131292\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.136404\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.131023\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.193144\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.116429\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.196784\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.149755\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.082686\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.130124\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.174015\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.100373\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.061982\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.047555\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.041690\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.093329\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.077760\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.068258\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.080124\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.249047\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.150238\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.206592\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.074057\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.060435\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.073629\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.099662\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.138809\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.113587\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.092350\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.078611\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.178553\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.162274\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.076125\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.044200\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.113318\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.041380\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.021398\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.061424\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.116043\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.049599\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.033941\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.044365\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.334231\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.057783\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.052055\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.069432\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.119703\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.032144\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.199313\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.051006\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.075242\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.056380\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.038682\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.023835\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.143471\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.024140\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.126302\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.022989\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.112703\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.159226\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.129814\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.054790\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.012726\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.066479\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.018858\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.049384\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.046767\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.106099\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.040280\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.035877\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.047598\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.022665\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.027998\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.039318\n",
            "\n",
            "Test set: Average loss: 0.0013, Accuracy: 9746/10000 (97%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.035792\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.076409\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.055075\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.118990\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.164323\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.371806\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.067133\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.110609\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.086909\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.023844\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.227169\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.057667\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.125112\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.139116\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.189715\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.142363\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.057764\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.196790\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.116549\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.210469\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.060307\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.162418\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.047727\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.019883\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.035314\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.075762\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.031778\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.041836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.023060\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.090038\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.108487\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.013857\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.070524\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.081826\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.056408\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.035031\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.055569\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.243988\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.038896\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.051304\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.064622\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.148337\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.047819\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.023280\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.080469\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.064437\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.041079\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.064999\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.090714\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.049842\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.078052\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.096816\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.101083\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.113415\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.062907\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.064113\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.114604\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.129919\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.034536\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.178424\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.142671\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.029033\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.031141\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.176863\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.059162\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.035317\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.101308\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.017597\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.039242\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.144635\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.206880\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.058259\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.029459\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.202398\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.059928\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.114372\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.082075\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.067840\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.016536\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.078650\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.057595\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.073684\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.030422\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.021128\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.054178\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.105752\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.080236\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.024020\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.030666\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.153083\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.113315\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.025765\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.090590\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.056681\n",
            "\n",
            "Test set: Average loss: 0.0011, Accuracy: 9759/10000 (97%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.073023\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.087174\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.034003\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.054065\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.076249\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.038137\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.153901\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.123154\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.099982\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.094222\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.041704\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.192611\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.241769\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.033306\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.030475\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.023955\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.083646\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.015396\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.045724\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.016659\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.164658\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.085031\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.127918\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.057310\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.047188\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.038645\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.115935\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.014619\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.059861\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.036510\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.259770\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.051884\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.077361\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.160883\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.097600\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.085421\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.095030\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.068872\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.026421\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.090758\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.038626\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.061077\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.068941\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.107431\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.036707\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.112046\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.086848\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.107085\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.078150\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.071300\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.259591\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.020605\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.087626\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.071242\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.044240\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.034195\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.055873\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.139290\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.152978\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.041669\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.193105\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.027857\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.065398\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.099260\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.042379\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.097640\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.069727\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.076904\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.091931\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.020128\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.033516\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.150041\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.039377\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.108293\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.035285\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.028375\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.091854\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.031173\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.020791\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.332279\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.081714\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.094416\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.040147\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.070678\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.083481\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.056478\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.182343\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.012541\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.104852\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.077250\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.079445\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.069798\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.065983\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.111843\n",
            "\n",
            "Test set: Average loss: 0.0010, Accuracy: 9788/10000 (97%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.083789\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.182446\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.107149\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.069418\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.016408\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.041045\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.090438\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.104922\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.065485\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.071017\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.178314\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.069106\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.038924\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.032345\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.206922\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.065465\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.159181\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.036555\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.082513\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.137864\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.040678\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.081050\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.073492\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.022748\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.178067\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.065212\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.036109\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.030733\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.065187\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.079633\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.054676\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.243983\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.031955\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.064276\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.060129\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.041453\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.055203\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.038134\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.083956\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.025595\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.085133\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.143852\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.069949\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.033904\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.055570\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.053475\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.051485\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.120666\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.064615\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.105212\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.017575\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.042655\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.111521\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.027163\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.033027\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.203276\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.010262\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.021186\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.159027\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.074130\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.104218\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.052894\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.066780\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.060994\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.121914\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.051374\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.151823\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.054460\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.031635\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.074131\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.053372\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.078163\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.091467\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.141067\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.041627\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.029106\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.028207\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.063028\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.126977\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.061340\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.058586\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.068139\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.044630\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.102551\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.078990\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.023253\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.061561\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.062342\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.042179\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.142136\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.049146\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.009987\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.101296\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.019861\n",
            "\n",
            "Test set: Average loss: 0.0010, Accuracy: 9778/10000 (97%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.078195\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.178932\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.036348\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.202927\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.107099\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.083427\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.101594\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.130413\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.033117\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.022808\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.055236\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.159957\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.067453\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.014709\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.117598\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.098733\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.042955\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.059081\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.034445\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.083037\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.117641\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.091331\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.019061\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.085667\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.036886\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.039707\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.162240\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.033427\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.028177\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.060414\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.051032\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.055980\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.056447\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.023331\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.026784\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.021110\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.047099\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.119035\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.047017\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.007638\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.037605\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.033951\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.083909\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.023866\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.036399\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.017306\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.040992\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.012468\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.074152\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.072006\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.071134\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.095920\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.040293\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.021134\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.188491\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.097403\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.023414\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.055466\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.006951\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.020133\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.159003\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.084294\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.043677\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.012661\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.098541\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.053557\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.095916\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.020788\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.143681\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.033573\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.020874\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.064834\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.050629\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.154310\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.170119\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.070874\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.096515\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.029642\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.016144\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.049442\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.015134\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.048032\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.230142\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.048187\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.045508\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.041260\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.038756\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.078291\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.164626\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.018120\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.018160\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.016846\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.055035\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.074787\n",
            "\n",
            "Test set: Average loss: 0.0008, Accuracy: 9825/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.109798\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.026564\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.184602\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.201492\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.006466\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.019050\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.065696\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.047265\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.043357\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.052075\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.014190\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.050943\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.101732\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.059343\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.094256\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.038275\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.033076\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.034631\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.134868\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.038499\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.060276\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.050524\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.074532\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.017645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.023858\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.088781\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.033849\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.042061\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.038318\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.067636\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.008028\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.028340\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.136370\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.092012\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.008246\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.123859\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.018509\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.034506\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.056429\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.137685\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.021861\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.005822\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.017320\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.058783\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.015895\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.076324\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.024820\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.050578\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.035275\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.025822\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.090268\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.099781\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.077133\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.008529\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.015856\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.014858\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.035895\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.138993\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.072168\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.086924\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.156460\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.045813\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.061586\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.007985\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.007587\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.013530\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.093631\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.053878\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.039236\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.011870\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.033065\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.010963\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.033854\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.044919\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.039812\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.007332\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.012501\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.104439\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.069066\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.013461\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.036645\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.029974\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.039538\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.095677\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.115349\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.021753\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.085968\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.052373\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.055482\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.032844\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.011900\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.061632\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.032008\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.069745\n",
            "\n",
            "Test set: Average loss: 0.0008, Accuracy: 9843/10000 (98%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kiiwxILOxglR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Toy Inception MNIST\n",
        "4 branches need\n",
        "1. 1\\*1 branch\n",
        "2. avg pooling branch + 1\\*1 branch\n",
        "3. 1\\*1 + 3\\*3 branch\n",
        "4. 1\\*1 + 5\\*5 branch\n"
      ]
    },
    {
      "metadata": {
        "id": "0rzr_6l0xgpr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        },
        "outputId": "5cece635-a679-442c-f929-bd64e721ab4b"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Training settings\n",
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/mnist/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data/mnist',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "class InceptionA(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "    super(InceptionA, self).__init__()\n",
        "    self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "    \n",
        "    self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "    self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
        "    \n",
        "    self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "    self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
        "    self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
        "    \n",
        "    self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    branch1x1 = self.branch1x1(x)\n",
        "    \n",
        "    branch5x5 = self.branch5x5_1(x)\n",
        "    branch5x5 = self.branch5x5_2(branch5x5)\n",
        "    \n",
        "    branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "    branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "    branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "    \n",
        "    branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding = 1)\n",
        "    branch_pool = self.branch_pool(branch_pool)\n",
        "    \n",
        "    output = [branch1x1,branch5x5, branch3x3dbl, branch_pool]\n",
        "    return torch.cat(output, 1)\n",
        "\n",
        "    \n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1,10,kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(88,20,kernel_size=5)\n",
        "    \n",
        "    self.incept1 = InceptionA(in_channels = 10)\n",
        "    self.incept2 = InceptionA(in_channels = 20)\n",
        "    \n",
        "    self.mp = nn.MaxPool2d(2)\n",
        "    self.fc = nn.Linear(1408,10)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    in_size = x.size(0) # it will be batch size(= 64)\n",
        "    x = F.relu(self.mp(self.conv1(x))).cuda()\n",
        "    x = self.incept1(x)\n",
        "    x = F.relu(self.mp(self.conv2(x))).cuda()\n",
        "    x = self.incept2(x)\n",
        "    x = x.view(in_size, -1) # Flatten the tensor\n",
        "    x = self.fc(x)\n",
        "    return F.log_softmax(x)\n",
        "  \n",
        "model = Net().cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
        "\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = Variable(data), Variable(target)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss=criterion(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.data[0]))\n",
        "    \n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  for data, target in test_loader:\n",
        "    data, target = Variable(data, volatile=True), Variable(target)\n",
        "    output = model(data)\n",
        "    # sum up batch loss\n",
        "    test_loss += criterion(output, target).data[0]\n",
        "    # get the index of the max\n",
        "    pred = output.data.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cuda().sum()\n",
        "  \n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "  \n",
        "  \n",
        "for epoch in range(1,10):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-a38d3f010433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-a38d3f010433>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-a38d3f010433>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0min_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# it will be batch size(= 64)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincept1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #2 'weight'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "-WE7KLe8-oEi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TzTVJ5pyxguJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1vVZWr3Nxgzz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lcKNzwW40EDi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "c3d78ac0-4218-462e-f396-19e3b7c79753"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "id": "irrnfXYT86NK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7d9870e9-6518-406d-ef65-31b447ae33e4"
      },
      "cell_type": "code",
      "source": [
        "batch_size= 1\n",
        "learning_rate = 0.0002\n",
        "epoch = 100"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pJE8OhZP9IST",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d730fcd0-94f9-4f2d-e3b5-a72804a84b40"
      },
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper parameters\n",
        "epoch = 5\n",
        "num_classes = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./drive/data/',\n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./drive/data/',\n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sftULFYmJwDc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "011d18f2-ae1b-4c9c-b622-4f2f098a1ccb"
      },
      "cell_type": "code",
      "source": [
        "def conv_2_block(in_dim,out_dim):\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(in_dim,out_dim,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_dim,out_dim,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def conv_3_block(in_dim,out_dim):\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(in_dim,out_dim,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_dim,out_dim,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_dim,out_dim,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2)\n",
        "    )\n",
        "    return model"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GpHj_gZsKnFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "1536bdff-4dd6-40d8-de61-73533f43b376"
      },
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, base_dim, num_classes=num_classes):\n",
        "        super(VGG, self).__init__()\n",
        "        self.feature = nn.Sequential(\n",
        "            conv_2_block(3,base_dim),\n",
        "            conv_2_block(base_dim,2*base_dim),\n",
        "            conv_3_block(2*base_dim,4*base_dim),\n",
        "            conv_3_block(4*base_dim,8*base_dim),\n",
        "            conv_3_block(8*base_dim,8*base_dim),            \n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(8*base_dim, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(100, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(20, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layer(x)\n",
        "        return x\n",
        "    \n",
        "model = VGG(base_dim=64).cuda()\n",
        "\n",
        "for i in model.named_children():\n",
        "    print(i)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('feature', Sequential(\n",
            "  (0): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (4): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "))\n",
            "('fc_layer', Sequential(\n",
            "  (0): Linear(in_features=512, out_features=100, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.5)\n",
            "  (3): Linear(in_features=100, out_features=20, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Dropout(p=0.5)\n",
            "  (6): Linear(in_features=20, out_features=10, bias=True)\n",
            "))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hk0i9TEdKnqD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "e61b032c-6941-4cee-b0a0-5cad8b97d904"
      },
      "cell_type": "code",
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b18qvMTQK5Do",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1b3360f4-b75b-4825-e788-620f9bccf2df"
      },
      "cell_type": "code",
      "source": [
        "for i in range(epoch):\n",
        "    for img,label in train_loader:\n",
        "        img = Variable(img).cuda()\n",
        "        label = Variable(label).cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(img)\n",
        "        loss = loss_func(output,label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(loss)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.3021, device='cuda:0')\n",
            "tensor(2.3004, device='cuda:0')\n",
            "tensor(2.3034, device='cuda:0')\n",
            "tensor(2.3032, device='cuda:0')\n",
            "tensor(2.3020, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2IwnEj56L64n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}