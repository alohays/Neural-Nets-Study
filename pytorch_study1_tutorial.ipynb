{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_study1_tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/alohays/Neural-Nets-Study/blob/master/pytorch_study1_tutorial.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "MImc_Bblezm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3bea3167-64f0-478d-9e5d-ca9d251c23f4"
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.0\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xxMC6YMXfWBJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "d7a64251-7b3d-4107-f02b-5fc5478419b7"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OfVj0nNQIbHq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "70a73bcf-1c95-454a-99c4-0c0bb3b04a54"
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fuse: mountpoint is not empty\r\n",
            "fuse: if you are sure this is safe, use the 'nonempty' mount option\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ARofJoUUpLC7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41bef93d-d4db-4701-fdb4-579330e1c29d"
      },
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper parameters\n",
        "num_epochs = 5\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HItRVUx0Z-3m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear Model with numpy"
      ]
    },
    {
      "metadata": {
        "id": "1bZDU3OLPYvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4051
        },
        "outputId": "da2bff72-c6b5-4ac1-8d53-0f581cef2fc2"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_data = [1.0,2.0,3.0]\n",
        "y_data = [2.0,4.0,6.0]\n",
        "\n",
        "# w=1.0\n",
        "\n",
        "def forward(x):\n",
        "  return x*w\n",
        "\n",
        "def loss(x,y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "w_list=[]\n",
        "mse_list=[]\n",
        "\n",
        "for w in np.arange(0.0, 4.1, 0.1):\n",
        "  print(\"w=\",w)\n",
        "  l_sum=0\n",
        "  for x_val, y_val in zip(x_data,y_data):\n",
        "    y_pred_val = forward(x_val)\n",
        "    l = loss(x_val,y_val)\n",
        "    l_sum+=l\n",
        "    print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
        "  print(\"MSE=\",l_sum / 3)\n",
        "  w_list.append(w)\n",
        "  mse_list.append(l_sum / 3)\n",
        "  \n",
        "plt.plot(w_list, mse_list)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('w')\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w= 0.0\n",
            "\t 1.0 2.0 0.0 4.0\n",
            "\t 2.0 4.0 0.0 16.0\n",
            "\t 3.0 6.0 0.0 36.0\n",
            "MSE= 18.666666666666668\n",
            "w= 0.1\n",
            "\t 1.0 2.0 0.1 3.61\n",
            "\t 2.0 4.0 0.2 14.44\n",
            "\t 3.0 6.0 0.30000000000000004 32.49\n",
            "MSE= 16.846666666666668\n",
            "w= 0.2\n",
            "\t 1.0 2.0 0.2 3.24\n",
            "\t 2.0 4.0 0.4 12.96\n",
            "\t 3.0 6.0 0.6000000000000001 29.160000000000004\n",
            "MSE= 15.120000000000003\n",
            "w= 0.30000000000000004\n",
            "\t 1.0 2.0 0.30000000000000004 2.8899999999999997\n",
            "\t 2.0 4.0 0.6000000000000001 11.559999999999999\n",
            "\t 3.0 6.0 0.9000000000000001 26.009999999999998\n",
            "MSE= 13.486666666666665\n",
            "w= 0.4\n",
            "\t 1.0 2.0 0.4 2.5600000000000005\n",
            "\t 2.0 4.0 0.8 10.240000000000002\n",
            "\t 3.0 6.0 1.2000000000000002 23.04\n",
            "MSE= 11.946666666666667\n",
            "w= 0.5\n",
            "\t 1.0 2.0 0.5 2.25\n",
            "\t 2.0 4.0 1.0 9.0\n",
            "\t 3.0 6.0 1.5 20.25\n",
            "MSE= 10.5\n",
            "w= 0.6000000000000001\n",
            "\t 1.0 2.0 0.6000000000000001 1.9599999999999997\n",
            "\t 2.0 4.0 1.2000000000000002 7.839999999999999\n",
            "\t 3.0 6.0 1.8000000000000003 17.639999999999993\n",
            "MSE= 9.146666666666663\n",
            "w= 0.7000000000000001\n",
            "\t 1.0 2.0 0.7000000000000001 1.6899999999999995\n",
            "\t 2.0 4.0 1.4000000000000001 6.759999999999998\n",
            "\t 3.0 6.0 2.1 15.209999999999999\n",
            "MSE= 7.886666666666666\n",
            "w= 0.8\n",
            "\t 1.0 2.0 0.8 1.44\n",
            "\t 2.0 4.0 1.6 5.76\n",
            "\t 3.0 6.0 2.4000000000000004 12.959999999999997\n",
            "MSE= 6.719999999999999\n",
            "w= 0.9\n",
            "\t 1.0 2.0 0.9 1.2100000000000002\n",
            "\t 2.0 4.0 1.8 4.840000000000001\n",
            "\t 3.0 6.0 2.7 10.889999999999999\n",
            "MSE= 5.646666666666666\n",
            "w= 1.0\n",
            "\t 1.0 2.0 1.0 1.0\n",
            "\t 2.0 4.0 2.0 4.0\n",
            "\t 3.0 6.0 3.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 1.1\n",
            "\t 1.0 2.0 1.1 0.8099999999999998\n",
            "\t 2.0 4.0 2.2 3.2399999999999993\n",
            "\t 3.0 6.0 3.3000000000000003 7.289999999999998\n",
            "MSE= 3.779999999999999\n",
            "w= 1.2000000000000002\n",
            "\t 1.0 2.0 1.2000000000000002 0.6399999999999997\n",
            "\t 2.0 4.0 2.4000000000000004 2.5599999999999987\n",
            "\t 3.0 6.0 3.6000000000000005 5.759999999999997\n",
            "MSE= 2.986666666666665\n",
            "w= 1.3\n",
            "\t 1.0 2.0 1.3 0.48999999999999994\n",
            "\t 2.0 4.0 2.6 1.9599999999999997\n",
            "\t 3.0 6.0 3.9000000000000004 4.409999999999998\n",
            "MSE= 2.2866666666666657\n",
            "w= 1.4000000000000001\n",
            "\t 1.0 2.0 1.4000000000000001 0.3599999999999998\n",
            "\t 2.0 4.0 2.8000000000000003 1.4399999999999993\n",
            "\t 3.0 6.0 4.2 3.2399999999999993\n",
            "MSE= 1.6799999999999995\n",
            "w= 1.5\n",
            "\t 1.0 2.0 1.5 0.25\n",
            "\t 2.0 4.0 3.0 1.0\n",
            "\t 3.0 6.0 4.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 1.6\n",
            "\t 1.0 2.0 1.6 0.15999999999999992\n",
            "\t 2.0 4.0 3.2 0.6399999999999997\n",
            "\t 3.0 6.0 4.800000000000001 1.4399999999999984\n",
            "MSE= 0.746666666666666\n",
            "w= 1.7000000000000002\n",
            "\t 1.0 2.0 1.7000000000000002 0.0899999999999999\n",
            "\t 2.0 4.0 3.4000000000000004 0.3599999999999996\n",
            "\t 3.0 6.0 5.1000000000000005 0.809999999999999\n",
            "MSE= 0.4199999999999995\n",
            "w= 1.8\n",
            "\t 1.0 2.0 1.8 0.03999999999999998\n",
            "\t 2.0 4.0 3.6 0.15999999999999992\n",
            "\t 3.0 6.0 5.4 0.3599999999999996\n",
            "MSE= 0.1866666666666665\n",
            "w= 1.9000000000000001\n",
            "\t 1.0 2.0 1.9000000000000001 0.009999999999999974\n",
            "\t 2.0 4.0 3.8000000000000003 0.0399999999999999\n",
            "\t 3.0 6.0 5.7 0.0899999999999999\n",
            "MSE= 0.046666666666666586\n",
            "w= 2.0\n",
            "\t 1.0 2.0 2.0 0.0\n",
            "\t 2.0 4.0 4.0 0.0\n",
            "\t 3.0 6.0 6.0 0.0\n",
            "MSE= 0.0\n",
            "w= 2.1\n",
            "\t 1.0 2.0 2.1 0.010000000000000018\n",
            "\t 2.0 4.0 4.2 0.04000000000000007\n",
            "\t 3.0 6.0 6.300000000000001 0.09000000000000043\n",
            "MSE= 0.046666666666666835\n",
            "w= 2.2\n",
            "\t 1.0 2.0 2.2 0.04000000000000007\n",
            "\t 2.0 4.0 4.4 0.16000000000000028\n",
            "\t 3.0 6.0 6.6000000000000005 0.36000000000000065\n",
            "MSE= 0.18666666666666698\n",
            "w= 2.3000000000000003\n",
            "\t 1.0 2.0 2.3000000000000003 0.09000000000000016\n",
            "\t 2.0 4.0 4.6000000000000005 0.36000000000000065\n",
            "\t 3.0 6.0 6.9 0.8100000000000006\n",
            "MSE= 0.42000000000000054\n",
            "w= 2.4000000000000004\n",
            "\t 1.0 2.0 2.4000000000000004 0.16000000000000028\n",
            "\t 2.0 4.0 4.800000000000001 0.6400000000000011\n",
            "\t 3.0 6.0 7.200000000000001 1.4400000000000026\n",
            "MSE= 0.7466666666666679\n",
            "w= 2.5\n",
            "\t 1.0 2.0 2.5 0.25\n",
            "\t 2.0 4.0 5.0 1.0\n",
            "\t 3.0 6.0 7.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 2.6\n",
            "\t 1.0 2.0 2.6 0.3600000000000001\n",
            "\t 2.0 4.0 5.2 1.4400000000000004\n",
            "\t 3.0 6.0 7.800000000000001 3.2400000000000024\n",
            "MSE= 1.6800000000000008\n",
            "w= 2.7\n",
            "\t 1.0 2.0 2.7 0.49000000000000027\n",
            "\t 2.0 4.0 5.4 1.960000000000001\n",
            "\t 3.0 6.0 8.100000000000001 4.410000000000006\n",
            "MSE= 2.2866666666666693\n",
            "w= 2.8000000000000003\n",
            "\t 1.0 2.0 2.8000000000000003 0.6400000000000005\n",
            "\t 2.0 4.0 5.6000000000000005 2.560000000000002\n",
            "\t 3.0 6.0 8.4 5.760000000000002\n",
            "MSE= 2.986666666666668\n",
            "w= 2.9000000000000004\n",
            "\t 1.0 2.0 2.9000000000000004 0.8100000000000006\n",
            "\t 2.0 4.0 5.800000000000001 3.2400000000000024\n",
            "\t 3.0 6.0 8.700000000000001 7.290000000000005\n",
            "MSE= 3.780000000000003\n",
            "w= 3.0\n",
            "\t 1.0 2.0 3.0 1.0\n",
            "\t 2.0 4.0 6.0 4.0\n",
            "\t 3.0 6.0 9.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 3.1\n",
            "\t 1.0 2.0 3.1 1.2100000000000002\n",
            "\t 2.0 4.0 6.2 4.840000000000001\n",
            "\t 3.0 6.0 9.3 10.890000000000004\n",
            "MSE= 5.646666666666668\n",
            "w= 3.2\n",
            "\t 1.0 2.0 3.2 1.4400000000000004\n",
            "\t 2.0 4.0 6.4 5.760000000000002\n",
            "\t 3.0 6.0 9.600000000000001 12.96000000000001\n",
            "MSE= 6.720000000000003\n",
            "w= 3.3000000000000003\n",
            "\t 1.0 2.0 3.3000000000000003 1.6900000000000006\n",
            "\t 2.0 4.0 6.6000000000000005 6.7600000000000025\n",
            "\t 3.0 6.0 9.9 15.210000000000003\n",
            "MSE= 7.886666666666668\n",
            "w= 3.4000000000000004\n",
            "\t 1.0 2.0 3.4000000000000004 1.960000000000001\n",
            "\t 2.0 4.0 6.800000000000001 7.840000000000004\n",
            "\t 3.0 6.0 10.200000000000001 17.640000000000008\n",
            "MSE= 9.14666666666667\n",
            "w= 3.5\n",
            "\t 1.0 2.0 3.5 2.25\n",
            "\t 2.0 4.0 7.0 9.0\n",
            "\t 3.0 6.0 10.5 20.25\n",
            "MSE= 10.5\n",
            "w= 3.6\n",
            "\t 1.0 2.0 3.6 2.5600000000000005\n",
            "\t 2.0 4.0 7.2 10.240000000000002\n",
            "\t 3.0 6.0 10.8 23.040000000000006\n",
            "MSE= 11.94666666666667\n",
            "w= 3.7\n",
            "\t 1.0 2.0 3.7 2.8900000000000006\n",
            "\t 2.0 4.0 7.4 11.560000000000002\n",
            "\t 3.0 6.0 11.100000000000001 26.010000000000016\n",
            "MSE= 13.486666666666673\n",
            "w= 3.8000000000000003\n",
            "\t 1.0 2.0 3.8000000000000003 3.240000000000001\n",
            "\t 2.0 4.0 7.6000000000000005 12.960000000000004\n",
            "\t 3.0 6.0 11.4 29.160000000000004\n",
            "MSE= 15.120000000000005\n",
            "w= 3.9000000000000004\n",
            "\t 1.0 2.0 3.9000000000000004 3.610000000000001\n",
            "\t 2.0 4.0 7.800000000000001 14.440000000000005\n",
            "\t 3.0 6.0 11.700000000000001 32.49000000000001\n",
            "MSE= 16.84666666666667\n",
            "w= 4.0\n",
            "\t 1.0 2.0 4.0 4.0\n",
            "\t 2.0 4.0 8.0 16.0\n",
            "\t 3.0 6.0 12.0 36.0\n",
            "MSE= 18.666666666666668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFYCAYAAABKymUhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njJrkAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f77687fe128>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "wJeKCS0eaEtI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gradient descent with numpy"
      ]
    },
    {
      "metadata": {
        "id": "kY6ygCqTNa1L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7253
        },
        "outputId": "6e0f131a-2183-4224-8372-c89b55edfd43"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_data = [1.0,2.0,3.0]\n",
        "y_data = [2.0,4.0,6.0]\n",
        "\n",
        "w=1.0 # random guess\n",
        "\n",
        "def forward(x):\n",
        "  return x*w\n",
        "\n",
        "def loss(x,y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "def gradient(x,y):\n",
        "  return 2 * x * ( x * w - y )\n",
        "\n",
        "print(\"predict (before training)\", 4, forward(4))\n",
        "\n",
        "for epoch in range(100):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    grad = gradient(x_val, y_val)\n",
        "    w = w - 0.01 * grad\n",
        "    print(\"\\tgrad: \",x_val, y_val, grad)\n",
        "    l = loss(x_val, y_val)\n",
        "    \n",
        "  print(\"progress: \",epoch, \"w=\",w,\"loss=\",l)\n",
        "  \n",
        "print(\"predict (after training)\", \"4 hours\" , forward(4))\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -7.84\n",
            "\tgrad:  3.0 6.0 -16.2288\n",
            "progress:  0 w= 1.260688 loss= 4.919240100095999\n",
            "\tgrad:  1.0 2.0 -1.478624\n",
            "\tgrad:  2.0 4.0 -5.796206079999999\n",
            "\tgrad:  3.0 6.0 -11.998146585599997\n",
            "progress:  1 w= 1.453417766656 loss= 2.688769240265834\n",
            "\tgrad:  1.0 2.0 -1.093164466688\n",
            "\tgrad:  2.0 4.0 -4.285204709416961\n",
            "\tgrad:  3.0 6.0 -8.87037374849311\n",
            "progress:  2 w= 1.5959051959019805 loss= 1.4696334962911515\n",
            "\tgrad:  1.0 2.0 -0.8081896081960389\n",
            "\tgrad:  2.0 4.0 -3.1681032641284723\n",
            "\tgrad:  3.0 6.0 -6.557973756745939\n",
            "progress:  3 w= 1.701247862192685 loss= 0.8032755585999681\n",
            "\tgrad:  1.0 2.0 -0.59750427561463\n",
            "\tgrad:  2.0 4.0 -2.3422167604093502\n",
            "\tgrad:  3.0 6.0 -4.848388694047353\n",
            "progress:  4 w= 1.7791289594933983 loss= 0.43905614881022015\n",
            "\tgrad:  1.0 2.0 -0.44174208101320334\n",
            "\tgrad:  2.0 4.0 -1.7316289575717576\n",
            "\tgrad:  3.0 6.0 -3.584471942173538\n",
            "progress:  5 w= 1.836707389300983 loss= 0.2399802903801062\n",
            "\tgrad:  1.0 2.0 -0.3265852213980338\n",
            "\tgrad:  2.0 4.0 -1.2802140678802925\n",
            "\tgrad:  3.0 6.0 -2.650043120512205\n",
            "progress:  6 w= 1.8792758133988885 loss= 0.1311689630744999\n",
            "\tgrad:  1.0 2.0 -0.241448373202223\n",
            "\tgrad:  2.0 4.0 -0.946477622952715\n",
            "\tgrad:  3.0 6.0 -1.9592086795121197\n",
            "progress:  7 w= 1.910747160155559 loss= 0.07169462478267678\n",
            "\tgrad:  1.0 2.0 -0.17850567968888198\n",
            "\tgrad:  2.0 4.0 -0.6997422643804168\n",
            "\tgrad:  3.0 6.0 -1.4484664872674653\n",
            "progress:  8 w= 1.9340143044689266 loss= 0.03918700813247573\n",
            "\tgrad:  1.0 2.0 -0.13197139106214673\n",
            "\tgrad:  2.0 4.0 -0.5173278529636143\n",
            "\tgrad:  3.0 6.0 -1.0708686556346834\n",
            "progress:  9 w= 1.9512159834655312 loss= 0.021418922423117836\n",
            "\tgrad:  1.0 2.0 -0.09756803306893769\n",
            "\tgrad:  2.0 4.0 -0.38246668963023644\n",
            "\tgrad:  3.0 6.0 -0.7917060475345892\n",
            "progress:  10 w= 1.9639333911678687 loss= 0.01170720245384975\n",
            "\tgrad:  1.0 2.0 -0.07213321766426262\n",
            "\tgrad:  2.0 4.0 -0.2827622132439096\n",
            "\tgrad:  3.0 6.0 -0.5853177814148953\n",
            "progress:  11 w= 1.9733355232910992 loss= 0.006398948863435593\n",
            "\tgrad:  1.0 2.0 -0.05332895341780164\n",
            "\tgrad:  2.0 4.0 -0.2090494973977819\n",
            "\tgrad:  3.0 6.0 -0.4327324596134101\n",
            "progress:  12 w= 1.9802866323953892 loss= 0.003497551760830656\n",
            "\tgrad:  1.0 2.0 -0.039426735209221686\n",
            "\tgrad:  2.0 4.0 -0.15455280202014876\n",
            "\tgrad:  3.0 6.0 -0.3199243001817109\n",
            "progress:  13 w= 1.9854256707695 loss= 0.001911699652671057\n",
            "\tgrad:  1.0 2.0 -0.02914865846100012\n",
            "\tgrad:  2.0 4.0 -0.11426274116712065\n",
            "\tgrad:  3.0 6.0 -0.2365238742159388\n",
            "progress:  14 w= 1.9892250235079405 loss= 0.0010449010656399273\n",
            "\tgrad:  1.0 2.0 -0.021549952984118992\n",
            "\tgrad:  2.0 4.0 -0.08447581569774698\n",
            "\tgrad:  3.0 6.0 -0.17486493849433593\n",
            "progress:  15 w= 1.9920339305797026 loss= 0.0005711243580809696\n",
            "\tgrad:  1.0 2.0 -0.015932138840594856\n",
            "\tgrad:  2.0 4.0 -0.062453984255132156\n",
            "\tgrad:  3.0 6.0 -0.12927974740812687\n",
            "progress:  16 w= 1.994110589284741 loss= 0.0003121664271570621\n",
            "\tgrad:  1.0 2.0 -0.011778821430517894\n",
            "\tgrad:  2.0 4.0 -0.046172980007630926\n",
            "\tgrad:  3.0 6.0 -0.09557806861579543\n",
            "progress:  17 w= 1.9956458879852805 loss= 0.0001706246229305199\n",
            "\tgrad:  1.0 2.0 -0.008708224029438938\n",
            "\tgrad:  2.0 4.0 -0.03413623819540135\n",
            "\tgrad:  3.0 6.0 -0.07066201306448505\n",
            "progress:  18 w= 1.9967809527381737 loss= 9.326038746484765e-05\n",
            "\tgrad:  1.0 2.0 -0.006438094523652627\n",
            "\tgrad:  2.0 4.0 -0.02523733053271826\n",
            "\tgrad:  3.0 6.0 -0.052241274202728505\n",
            "progress:  19 w= 1.9976201197307648 loss= 5.097447086306101e-05\n",
            "\tgrad:  1.0 2.0 -0.004759760538470381\n",
            "\tgrad:  2.0 4.0 -0.01865826131080439\n",
            "\tgrad:  3.0 6.0 -0.03862260091336722\n",
            "progress:  20 w= 1.998240525958391 loss= 2.7861740127856012e-05\n",
            "\tgrad:  1.0 2.0 -0.0035189480832178432\n",
            "\tgrad:  2.0 4.0 -0.01379427648621423\n",
            "\tgrad:  3.0 6.0 -0.028554152326460525\n",
            "progress:  21 w= 1.99869919972735 loss= 1.5228732143933469e-05\n",
            "\tgrad:  1.0 2.0 -0.002601600545300009\n",
            "\tgrad:  2.0 4.0 -0.01019827413757568\n",
            "\tgrad:  3.0 6.0 -0.021110427464781978\n",
            "progress:  22 w= 1.9990383027488265 loss= 8.323754426231206e-06\n",
            "\tgrad:  1.0 2.0 -0.001923394502346909\n",
            "\tgrad:  2.0 4.0 -0.007539706449199102\n",
            "\tgrad:  3.0 6.0 -0.01560719234984198\n",
            "progress:  23 w= 1.9992890056818404 loss= 4.549616284094891e-06\n",
            "\tgrad:  1.0 2.0 -0.0014219886363191492\n",
            "\tgrad:  2.0 4.0 -0.005574195454370212\n",
            "\tgrad:  3.0 6.0 -0.011538584590544687\n",
            "progress:  24 w= 1.999474353368653 loss= 2.486739429417538e-06\n",
            "\tgrad:  1.0 2.0 -0.0010512932626940419\n",
            "\tgrad:  2.0 4.0 -0.004121069589761106\n",
            "\tgrad:  3.0 6.0 -0.008530614050808794\n",
            "progress:  25 w= 1.9996113831376856 loss= 1.3592075910762856e-06\n",
            "\tgrad:  1.0 2.0 -0.0007772337246287897\n",
            "\tgrad:  2.0 4.0 -0.0030467562005451754\n",
            "\tgrad:  3.0 6.0 -0.006306785335127074\n",
            "progress:  26 w= 1.9997126908902887 loss= 7.429187207079447e-07\n",
            "\tgrad:  1.0 2.0 -0.0005746182194226179\n",
            "\tgrad:  2.0 4.0 -0.002252503420136165\n",
            "\tgrad:  3.0 6.0 -0.00466268207967957\n",
            "progress:  27 w= 1.9997875889274812 loss= 4.060661735575354e-07\n",
            "\tgrad:  1.0 2.0 -0.0004248221450375844\n",
            "\tgrad:  2.0 4.0 -0.0016653028085471533\n",
            "\tgrad:  3.0 6.0 -0.0034471768136938863\n",
            "progress:  28 w= 1.9998429619451539 loss= 2.2194855602869353e-07\n",
            "\tgrad:  1.0 2.0 -0.00031407610969225175\n",
            "\tgrad:  2.0 4.0 -0.0012311783499932005\n",
            "\tgrad:  3.0 6.0 -0.0025485391844828342\n",
            "progress:  29 w= 1.9998838998815958 loss= 1.213131374411496e-07\n",
            "\tgrad:  1.0 2.0 -0.00023220023680847746\n",
            "\tgrad:  2.0 4.0 -0.0009102249282886277\n",
            "\tgrad:  3.0 6.0 -0.0018841656015560204\n",
            "progress:  30 w= 1.9999141657892625 loss= 6.630760559646474e-08\n",
            "\tgrad:  1.0 2.0 -0.00017166842147497974\n",
            "\tgrad:  2.0 4.0 -0.0006729402121816719\n",
            "\tgrad:  3.0 6.0 -0.0013929862392156878\n",
            "progress:  31 w= 1.9999365417379913 loss= 3.624255915449335e-08\n",
            "\tgrad:  1.0 2.0 -0.0001269165240174175\n",
            "\tgrad:  2.0 4.0 -0.0004975127741477792\n",
            "\tgrad:  3.0 6.0 -0.0010298514424817995\n",
            "progress:  32 w= 1.9999530845453979 loss= 1.9809538924707548e-08\n",
            "\tgrad:  1.0 2.0 -9.383090920422887e-05\n",
            "\tgrad:  2.0 4.0 -0.00036781716408107457\n",
            "\tgrad:  3.0 6.0 -0.0007613815296476645\n",
            "progress:  33 w= 1.9999653148414271 loss= 1.0827542027017377e-08\n",
            "\tgrad:  1.0 2.0 -6.937031714571162e-05\n",
            "\tgrad:  2.0 4.0 -0.0002719316432120422\n",
            "\tgrad:  3.0 6.0 -0.0005628985014531906\n",
            "progress:  34 w= 1.999974356846045 loss= 5.9181421028034105e-09\n",
            "\tgrad:  1.0 2.0 -5.1286307909848006e-05\n",
            "\tgrad:  2.0 4.0 -0.00020104232700646207\n",
            "\tgrad:  3.0 6.0 -0.0004161576169003922\n",
            "progress:  35 w= 1.9999810417085633 loss= 3.2347513278475087e-09\n",
            "\tgrad:  1.0 2.0 -3.7916582873442906e-05\n",
            "\tgrad:  2.0 4.0 -0.0001486330048638962\n",
            "\tgrad:  3.0 6.0 -0.0003076703200690645\n",
            "progress:  36 w= 1.9999859839076413 loss= 1.7680576050779005e-09\n",
            "\tgrad:  1.0 2.0 -2.8032184717474706e-05\n",
            "\tgrad:  2.0 4.0 -0.0001098861640933535\n",
            "\tgrad:  3.0 6.0 -0.00022746435967313516\n",
            "progress:  37 w= 1.9999896377347262 loss= 9.6638887447731e-10\n",
            "\tgrad:  1.0 2.0 -2.0724530547688857e-05\n",
            "\tgrad:  2.0 4.0 -8.124015974608767e-05\n",
            "\tgrad:  3.0 6.0 -0.00016816713067413502\n",
            "progress:  38 w= 1.999992339052936 loss= 5.282109892545845e-10\n",
            "\tgrad:  1.0 2.0 -1.5321894128117464e-05\n",
            "\tgrad:  2.0 4.0 -6.006182498197177e-05\n",
            "\tgrad:  3.0 6.0 -0.00012432797771566584\n",
            "progress:  39 w= 1.9999943361699042 loss= 2.887107421958329e-10\n",
            "\tgrad:  1.0 2.0 -1.1327660191629008e-05\n",
            "\tgrad:  2.0 4.0 -4.4404427951505454e-05\n",
            "\tgrad:  3.0 6.0 -9.191716585732479e-05\n",
            "progress:  40 w= 1.9999958126624442 loss= 1.5780416225633037e-10\n",
            "\tgrad:  1.0 2.0 -8.37467511161094e-06\n",
            "\tgrad:  2.0 4.0 -3.282872643772805e-05\n",
            "\tgrad:  3.0 6.0 -6.795546372551087e-05\n",
            "progress:  41 w= 1.999996904251097 loss= 8.625295142578772e-11\n",
            "\tgrad:  1.0 2.0 -6.191497806007362e-06\n",
            "\tgrad:  2.0 4.0 -2.4270671399762023e-05\n",
            "\tgrad:  3.0 6.0 -5.0240289795056015e-05\n",
            "progress:  42 w= 1.999997711275687 loss= 4.71443308235547e-11\n",
            "\tgrad:  1.0 2.0 -4.5774486259198e-06\n",
            "\tgrad:  2.0 4.0 -1.794359861406747e-05\n",
            "\tgrad:  3.0 6.0 -3.714324913239864e-05\n",
            "progress:  43 w= 1.9999983079186507 loss= 2.5768253628059826e-11\n",
            "\tgrad:  1.0 2.0 -3.3841626985164908e-06\n",
            "\tgrad:  2.0 4.0 -1.326591777761621e-05\n",
            "\tgrad:  3.0 6.0 -2.7460449796734565e-05\n",
            "progress:  44 w= 1.9999987490239537 loss= 1.4084469615916932e-11\n",
            "\tgrad:  1.0 2.0 -2.5019520926150562e-06\n",
            "\tgrad:  2.0 4.0 -9.807652203264183e-06\n",
            "\tgrad:  3.0 6.0 -2.0301840059744336e-05\n",
            "progress:  45 w= 1.9999990751383971 loss= 7.698320862431846e-12\n",
            "\tgrad:  1.0 2.0 -1.8497232057157476e-06\n",
            "\tgrad:  2.0 4.0 -7.250914967116273e-06\n",
            "\tgrad:  3.0 6.0 -1.5009393983689279e-05\n",
            "progress:  46 w= 1.9999993162387186 loss= 4.20776540913866e-12\n",
            "\tgrad:  1.0 2.0 -1.3675225627451937e-06\n",
            "\tgrad:  2.0 4.0 -5.3606884460322135e-06\n",
            "\tgrad:  3.0 6.0 -1.109662508014253e-05\n",
            "progress:  47 w= 1.9999994944870796 loss= 2.299889814334344e-12\n",
            "\tgrad:  1.0 2.0 -1.0110258408246864e-06\n",
            "\tgrad:  2.0 4.0 -3.963221296032771e-06\n",
            "\tgrad:  3.0 6.0 -8.20386808086937e-06\n",
            "progress:  48 w= 1.9999996262682318 loss= 1.2570789110540446e-12\n",
            "\tgrad:  1.0 2.0 -7.474635363990956e-07\n",
            "\tgrad:  2.0 4.0 -2.930057062755509e-06\n",
            "\tgrad:  3.0 6.0 -6.065218119744031e-06\n",
            "progress:  49 w= 1.999999723695619 loss= 6.870969979249939e-13\n",
            "\tgrad:  1.0 2.0 -5.526087618612507e-07\n",
            "\tgrad:  2.0 4.0 -2.166226346744793e-06\n",
            "\tgrad:  3.0 6.0 -4.484088535150477e-06\n",
            "progress:  50 w= 1.9999997957248556 loss= 3.7555501141274804e-13\n",
            "\tgrad:  1.0 2.0 -4.08550288710785e-07\n",
            "\tgrad:  2.0 4.0 -1.6015171322436572e-06\n",
            "\tgrad:  3.0 6.0 -3.3151404608133817e-06\n",
            "progress:  51 w= 1.9999998489769344 loss= 2.052716967104274e-13\n",
            "\tgrad:  1.0 2.0 -3.020461312175371e-07\n",
            "\tgrad:  2.0 4.0 -1.1840208351543424e-06\n",
            "\tgrad:  3.0 6.0 -2.4509231284497446e-06\n",
            "progress:  52 w= 1.9999998883468353 loss= 1.1219786256679713e-13\n",
            "\tgrad:  1.0 2.0 -2.2330632942768602e-07\n",
            "\tgrad:  2.0 4.0 -8.753608113920563e-07\n",
            "\tgrad:  3.0 6.0 -1.811996877876254e-06\n",
            "progress:  53 w= 1.9999999174534755 loss= 6.132535848018759e-14\n",
            "\tgrad:  1.0 2.0 -1.6509304900935717e-07\n",
            "\tgrad:  2.0 4.0 -6.471647520100987e-07\n",
            "\tgrad:  3.0 6.0 -1.3396310407642886e-06\n",
            "progress:  54 w= 1.999999938972364 loss= 3.351935118167793e-14\n",
            "\tgrad:  1.0 2.0 -1.220552721115098e-07\n",
            "\tgrad:  2.0 4.0 -4.784566662863199e-07\n",
            "\tgrad:  3.0 6.0 -9.904052991061008e-07\n",
            "progress:  55 w= 1.9999999548815364 loss= 1.8321081844499955e-14\n",
            "\tgrad:  1.0 2.0 -9.023692726373156e-08\n",
            "\tgrad:  2.0 4.0 -3.5372875473171916e-07\n",
            "\tgrad:  3.0 6.0 -7.322185204827747e-07\n",
            "progress:  56 w= 1.9999999666433785 loss= 1.0013977760018664e-14\n",
            "\tgrad:  1.0 2.0 -6.671324292994996e-08\n",
            "\tgrad:  2.0 4.0 -2.615159129248923e-07\n",
            "\tgrad:  3.0 6.0 -5.413379398078177e-07\n",
            "progress:  57 w= 1.9999999753390494 loss= 5.473462367088053e-15\n",
            "\tgrad:  1.0 2.0 -4.932190122985958e-08\n",
            "\tgrad:  2.0 4.0 -1.9334185274999527e-07\n",
            "\tgrad:  3.0 6.0 -4.002176350326181e-07\n",
            "progress:  58 w= 1.9999999817678633 loss= 2.991697274308627e-15\n",
            "\tgrad:  1.0 2.0 -3.6464273378555845e-08\n",
            "\tgrad:  2.0 4.0 -1.429399514307761e-07\n",
            "\tgrad:  3.0 6.0 -2.9588569994132286e-07\n",
            "progress:  59 w= 1.9999999865207625 loss= 1.6352086111474931e-15\n",
            "\tgrad:  1.0 2.0 -2.6958475007887728e-08\n",
            "\tgrad:  2.0 4.0 -1.0567722164012139e-07\n",
            "\tgrad:  3.0 6.0 -2.1875184863517916e-07\n",
            "progress:  60 w= 1.999999990034638 loss= 8.937759877335403e-16\n",
            "\tgrad:  1.0 2.0 -1.993072418216002e-08\n",
            "\tgrad:  2.0 4.0 -7.812843882959442e-08\n",
            "\tgrad:  3.0 6.0 -1.617258700292723e-07\n",
            "progress:  61 w= 1.9999999926324883 loss= 4.885220495987371e-16\n",
            "\tgrad:  1.0 2.0 -1.473502342363986e-08\n",
            "\tgrad:  2.0 4.0 -5.7761292637792394e-08\n",
            "\tgrad:  3.0 6.0 -1.195658771990793e-07\n",
            "progress:  62 w= 1.99999999455311 loss= 2.670175009618106e-16\n",
            "\tgrad:  1.0 2.0 -1.0893780100218464e-08\n",
            "\tgrad:  2.0 4.0 -4.270361841918202e-08\n",
            "\tgrad:  3.0 6.0 -8.839649012770678e-08\n",
            "progress:  63 w= 1.9999999959730488 loss= 1.4594702493172377e-16\n",
            "\tgrad:  1.0 2.0 -8.05390243385773e-09\n",
            "\tgrad:  2.0 4.0 -3.1571296688071016e-08\n",
            "\tgrad:  3.0 6.0 -6.53525820126788e-08\n",
            "progress:  64 w= 1.9999999970228268 loss= 7.977204100704301e-17\n",
            "\tgrad:  1.0 2.0 -5.9543463493128e-09\n",
            "\tgrad:  2.0 4.0 -2.334103754719763e-08\n",
            "\tgrad:  3.0 6.0 -4.8315948575350376e-08\n",
            "progress:  65 w= 1.9999999977989402 loss= 4.360197735196887e-17\n",
            "\tgrad:  1.0 2.0 -4.402119557767037e-09\n",
            "\tgrad:  2.0 4.0 -1.725630838222969e-08\n",
            "\tgrad:  3.0 6.0 -3.5720557178819945e-08\n",
            "progress:  66 w= 1.9999999983727301 loss= 2.3832065197304227e-17\n",
            "\tgrad:  1.0 2.0 -3.254539748809293e-09\n",
            "\tgrad:  2.0 4.0 -1.2757796596929438e-08\n",
            "\tgrad:  3.0 6.0 -2.6408640607655798e-08\n",
            "progress:  67 w= 1.9999999987969397 loss= 1.3026183953845832e-17\n",
            "\tgrad:  1.0 2.0 -2.406120636067044e-09\n",
            "\tgrad:  2.0 4.0 -9.431992964437086e-09\n",
            "\tgrad:  3.0 6.0 -1.9524227568012975e-08\n",
            "progress:  68 w= 1.999999999110563 loss= 7.11988308874388e-18\n",
            "\tgrad:  1.0 2.0 -1.7788739370416806e-09\n",
            "\tgrad:  2.0 4.0 -6.97318647269185e-09\n",
            "\tgrad:  3.0 6.0 -1.4434496264925656e-08\n",
            "progress:  69 w= 1.9999999993424284 loss= 3.89160224698574e-18\n",
            "\tgrad:  1.0 2.0 -1.3151431055291596e-09\n",
            "\tgrad:  2.0 4.0 -5.155360582875801e-09\n",
            "\tgrad:  3.0 6.0 -1.067159693945996e-08\n",
            "progress:  70 w= 1.9999999995138495 loss= 2.1270797208746147e-18\n",
            "\tgrad:  1.0 2.0 -9.72300906454393e-10\n",
            "\tgrad:  2.0 4.0 -3.811418736177075e-09\n",
            "\tgrad:  3.0 6.0 -7.88963561149103e-09\n",
            "progress:  71 w= 1.9999999996405833 loss= 1.1626238773828175e-18\n",
            "\tgrad:  1.0 2.0 -7.18833437218791e-10\n",
            "\tgrad:  2.0 4.0 -2.8178277489132597e-09\n",
            "\tgrad:  3.0 6.0 -5.832902161273523e-09\n",
            "progress:  72 w= 1.999999999734279 loss= 6.354692062078993e-19\n",
            "\tgrad:  1.0 2.0 -5.314420015167798e-10\n",
            "\tgrad:  2.0 4.0 -2.0832526814729135e-09\n",
            "\tgrad:  3.0 6.0 -4.31233715403323e-09\n",
            "progress:  73 w= 1.9999999998035491 loss= 3.4733644793346653e-19\n",
            "\tgrad:  1.0 2.0 -3.92901711165905e-10\n",
            "\tgrad:  2.0 4.0 -1.5401742103904326e-09\n",
            "\tgrad:  3.0 6.0 -3.188159070077745e-09\n",
            "progress:  74 w= 1.9999999998547615 loss= 1.8984796531526204e-19\n",
            "\tgrad:  1.0 2.0 -2.9047697580608656e-10\n",
            "\tgrad:  2.0 4.0 -1.1386696030513122e-09\n",
            "\tgrad:  3.0 6.0 -2.3570478902001923e-09\n",
            "progress:  75 w= 1.9999999998926234 loss= 1.0376765851119951e-19\n",
            "\tgrad:  1.0 2.0 -2.1475310418850313e-10\n",
            "\tgrad:  2.0 4.0 -8.418314934033333e-10\n",
            "\tgrad:  3.0 6.0 -1.7425900722400911e-09\n",
            "progress:  76 w= 1.9999999999206153 loss= 5.671751114309842e-20\n",
            "\tgrad:  1.0 2.0 -1.5876944203796484e-10\n",
            "\tgrad:  2.0 4.0 -6.223768167501476e-10\n",
            "\tgrad:  3.0 6.0 -1.2883241140571045e-09\n",
            "progress:  77 w= 1.9999999999413098 loss= 3.100089617511693e-20\n",
            "\tgrad:  1.0 2.0 -1.17380327679939e-10\n",
            "\tgrad:  2.0 4.0 -4.601314884666863e-10\n",
            "\tgrad:  3.0 6.0 -9.524754318590567e-10\n",
            "progress:  78 w= 1.9999999999566096 loss= 1.6944600977692705e-20\n",
            "\tgrad:  1.0 2.0 -8.678080476443029e-11\n",
            "\tgrad:  2.0 4.0 -3.4018121652934497e-10\n",
            "\tgrad:  3.0 6.0 -7.041780492045291e-10\n",
            "progress:  79 w= 1.9999999999679208 loss= 9.2616919156479e-21\n",
            "\tgrad:  1.0 2.0 -6.415845632545825e-11\n",
            "\tgrad:  2.0 4.0 -2.5150193039280566e-10\n",
            "\tgrad:  3.0 6.0 -5.206075570640678e-10\n",
            "progress:  80 w= 1.9999999999762834 loss= 5.062350511130293e-21\n",
            "\tgrad:  1.0 2.0 -4.743316850408519e-11\n",
            "\tgrad:  2.0 4.0 -1.8593837580738182e-10\n",
            "\tgrad:  3.0 6.0 -3.8489211817704927e-10\n",
            "progress:  81 w= 1.999999999982466 loss= 2.7669155644059242e-21\n",
            "\tgrad:  1.0 2.0 -3.5067948545020045e-11\n",
            "\tgrad:  2.0 4.0 -1.3746692673066718e-10\n",
            "\tgrad:  3.0 6.0 -2.845563784603655e-10\n",
            "progress:  82 w= 1.9999999999870368 loss= 1.5124150106147723e-21\n",
            "\tgrad:  1.0 2.0 -2.5926372160256506e-11\n",
            "\tgrad:  2.0 4.0 -1.0163070385260653e-10\n",
            "\tgrad:  3.0 6.0 -2.1037571684701106e-10\n",
            "progress:  83 w= 1.999999999990416 loss= 8.26683933105326e-22\n",
            "\tgrad:  1.0 2.0 -1.9167778475548403e-11\n",
            "\tgrad:  2.0 4.0 -7.51381179497912e-11\n",
            "\tgrad:  3.0 6.0 -1.5553425214420713e-10\n",
            "progress:  84 w= 1.9999999999929146 loss= 4.518126871054872e-22\n",
            "\tgrad:  1.0 2.0 -1.4170886686315498e-11\n",
            "\tgrad:  2.0 4.0 -5.555023108172463e-11\n",
            "\tgrad:  3.0 6.0 -1.1499068364173581e-10\n",
            "progress:  85 w= 1.9999999999947617 loss= 2.469467919185614e-22\n",
            "\tgrad:  1.0 2.0 -1.0476508549572827e-11\n",
            "\tgrad:  2.0 4.0 -4.106759377009439e-11\n",
            "\tgrad:  3.0 6.0 -8.500933290633839e-11\n",
            "progress:  86 w= 1.9999999999961273 loss= 1.349840097651456e-22\n",
            "\tgrad:  1.0 2.0 -7.745359908994942e-12\n",
            "\tgrad:  2.0 4.0 -3.036149109902908e-11\n",
            "\tgrad:  3.0 6.0 -6.285105769165966e-11\n",
            "progress:  87 w= 1.999999999997137 loss= 7.376551550022107e-23\n",
            "\tgrad:  1.0 2.0 -5.726086271806707e-12\n",
            "\tgrad:  2.0 4.0 -2.2446045022661565e-11\n",
            "\tgrad:  3.0 6.0 -4.646416584819235e-11\n",
            "progress:  88 w= 1.9999999999978835 loss= 4.031726170507742e-23\n",
            "\tgrad:  1.0 2.0 -4.233058348290797e-12\n",
            "\tgrad:  2.0 4.0 -1.659294923683774e-11\n",
            "\tgrad:  3.0 6.0 -3.4351188560322043e-11\n",
            "progress:  89 w= 1.9999999999984353 loss= 2.2033851437431755e-23\n",
            "\tgrad:  1.0 2.0 -3.1294966618133913e-12\n",
            "\tgrad:  2.0 4.0 -1.226752033289813e-11\n",
            "\tgrad:  3.0 6.0 -2.539835008974478e-11\n",
            "progress:  90 w= 1.9999999999988431 loss= 1.2047849775995315e-23\n",
            "\tgrad:  1.0 2.0 -2.3137047833188262e-12\n",
            "\tgrad:  2.0 4.0 -9.070078021977679e-12\n",
            "\tgrad:  3.0 6.0 -1.8779644506139448e-11\n",
            "progress:  91 w= 1.9999999999991447 loss= 6.5840863393251405e-24\n",
            "\tgrad:  1.0 2.0 -1.7106316363424412e-12\n",
            "\tgrad:  2.0 4.0 -6.7057470687359455e-12\n",
            "\tgrad:  3.0 6.0 -1.3882228699912957e-11\n",
            "progress:  92 w= 1.9999999999993676 loss= 3.5991747246272455e-24\n",
            "\tgrad:  1.0 2.0 -1.2647660696529783e-12\n",
            "\tgrad:  2.0 4.0 -4.957811938766099e-12\n",
            "\tgrad:  3.0 6.0 -1.0263789818054647e-11\n",
            "progress:  93 w= 1.9999999999995324 loss= 1.969312363793734e-24\n",
            "\tgrad:  1.0 2.0 -9.352518759442319e-13\n",
            "\tgrad:  2.0 4.0 -3.666400516522117e-12\n",
            "\tgrad:  3.0 6.0 -7.58859641791787e-12\n",
            "progress:  94 w= 1.9999999999996543 loss= 1.0761829795642296e-24\n",
            "\tgrad:  1.0 2.0 -6.914468997365475e-13\n",
            "\tgrad:  2.0 4.0 -2.7107205369247822e-12\n",
            "\tgrad:  3.0 6.0 -5.611511255665391e-12\n",
            "progress:  95 w= 1.9999999999997444 loss= 5.875191475205477e-25\n",
            "\tgrad:  1.0 2.0 -5.111466805374221e-13\n",
            "\tgrad:  2.0 4.0 -2.0037305148434825e-12\n",
            "\tgrad:  3.0 6.0 -4.1460168631601846e-12\n",
            "progress:  96 w= 1.999999999999811 loss= 3.2110109830478153e-25\n",
            "\tgrad:  1.0 2.0 -3.779199175824033e-13\n",
            "\tgrad:  2.0 4.0 -1.4814816040598089e-12\n",
            "\tgrad:  3.0 6.0 -3.064215547965432e-12\n",
            "progress:  97 w= 1.9999999999998603 loss= 1.757455879087579e-25\n",
            "\tgrad:  1.0 2.0 -2.793321129956894e-13\n",
            "\tgrad:  2.0 4.0 -1.0942358130705543e-12\n",
            "\tgrad:  3.0 6.0 -2.2648549702353193e-12\n",
            "progress:  98 w= 1.9999999999998967 loss= 9.608404711682446e-26\n",
            "\tgrad:  1.0 2.0 -2.0650148258027912e-13\n",
            "\tgrad:  2.0 4.0 -8.100187187665142e-13\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  99 w= 1.9999999999999236 loss= 5.250973729513143e-26\n",
            "predict (after training) 4 hours 7.9999999999996945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9Q-DtnFDaPtc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Back Propagation and Autograd with PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "AS2SmOufPebp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "outputId": "fcb8c259-9abf-4840-e85e-c5f47ffd7d1f"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "x_data = [1.0,2.0,3.0]\n",
        "y_data = [2.0,4.0,6.0]\n",
        "\n",
        "w = Variable(torch.Tensor([1.0]), requires_grad = True)\n",
        "\n",
        "def forward(x):\n",
        "  return x*w\n",
        "\n",
        "def loss(x,y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "print(\"predict (before training)\", 4, forward(4))\n",
        "\n",
        "for epoch in range(10):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    l = loss(x_val, y_val)\n",
        "    l.backward()\n",
        "    print(\"\\tgrad: \",x_val, y_val, grad)\n",
        "    w.data = w.data - 0.01 * w.grad.data\n",
        "    \n",
        "    w.grad.data.zero_()\n",
        "    \n",
        "  print(\"progress: \",epoch, \"w=\",w,\"loss=\",l)\n",
        "  \n",
        "print(\"predict (after training)\", \"4 hours\" , forward(4))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (before training) 4 tensor([ 4.])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  0 w= tensor([ 1.2607]) loss= tensor([ 7.3159])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  1 w= tensor([ 1.4534]) loss= tensor([ 3.9988])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  2 w= tensor([ 1.5959]) loss= tensor([ 2.1857])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  3 w= tensor([ 1.7012]) loss= tensor([ 1.1946])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  4 w= tensor([ 1.7791]) loss= tensor([ 0.6530])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  5 w= tensor([ 1.8367]) loss= tensor([ 0.3569])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  6 w= tensor([ 1.8793]) loss= tensor([ 0.1951])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  7 w= tensor([ 1.9107]) loss= tensor([ 0.1066])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  8 w= tensor([ 1.9340]) loss= tensor(1.00000e-02 *\n",
            "       [ 5.8279])\n",
            "\tgrad:  1.0 2.0 -1.6786572132332367e-12\n",
            "\tgrad:  2.0 4.0 -1.6786572132332367e-12\n",
            "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
            "progress:  9 w= tensor([ 1.9512]) loss= tensor(1.00000e-02 *\n",
            "       [ 3.1854])\n",
            "predict (after training) 4 hours tensor([ 7.8049])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v4mLXq39Pfsh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear Regression in PyTorch way\n",
        "\n",
        "## basic pytorch rythm\n",
        "1. Design your model using class with Variables\n",
        "2. Construct loss and optimizer (using pytorch api)\n",
        "3. Training cycle (forward, backward, update)\n",
        "\n",
        "### Other Optimizers\n",
        " - torch.optim.Adagrad\n",
        " - torch.optim.Adam\n",
        " - torch.optim.Adamax\n",
        " - torch.optim.ASGD\n",
        " - torch.optim.LBFGS\n",
        " - torch.optim.RMSprop\n",
        " - torch.optim.Rprop\n",
        " - torch.optim.SGD"
      ]
    },
    {
      "metadata": {
        "id": "67b2i9OaaXbC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "a17b31be-e063-4549-81fe-1ac7e6baca04"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "# step1 Design your model using class with Variables\n",
        "\n",
        "x_data = Variable(torch.Tensor([[1.0],[2.0],[3.0]]))\n",
        "y_data = Variable(torch.Tensor([[2.0],[4.0],[6.0]]))\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1,1) # one input, one output\n",
        "    \n",
        "  def forward(self, x):\n",
        "    y_pred = self.linear(x)\n",
        "    return y_pred\n",
        "\n",
        "# our model instance\n",
        "model = Model()\n",
        "\n",
        "# step2 Construct loss and optimizer (using pytorch api)\n",
        "\n",
        "criterion = torch.nn.MSELoss(size_average=False)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# stpe3 Training cycle: forwad, loss, backward, step\n",
        "\n",
        "for epoch in range(500):\n",
        "  y_pred = model(x_data)\n",
        "  \n",
        "  loss = criterion(y_pred, y_data)\n",
        "  if epoch % 50 == 0:\n",
        "    print(epoch, loss.item())\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "hour_var = Variable(torch.Tensor([[4.0]]))\n",
        "print(\"predict (after training)\", 4, model.forward(hour_var).item())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 108.97834777832031\n",
            "50 0.41449713706970215\n",
            "100 0.20099374651908875\n",
            "150 0.09746389091014862\n",
            "200 0.04726118594408035\n",
            "250 0.022917453199625015\n",
            "300 0.01111286785453558\n",
            "350 0.005388767924159765\n",
            "400 0.002613053424283862\n",
            "450 0.0012670927681028843\n",
            "predict (after training) 4 7.971298694610596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oGqzqg2jPf65",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression with pytorch\n",
        "\n",
        "\n",
        "sigmoid -> 0~1\n",
        "\n",
        "loss function should be different -> cross entropy loss"
      ]
    },
    {
      "metadata": {
        "id": "cvVjiNUQjP6r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "8c9db3b2-350d-4da4-eeac-ca98a16129d6"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0], [4.0]]))\n",
        "y_data = Variable(torch.Tensor([[0.], [0.], [1.], [1.]]))\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.linear = torch.nn.Linear(1,1)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    y_pred = F.sigmoid(self.linear(x))\n",
        "    return y_pred\n",
        "  \n",
        "model = Model()\n",
        "\n",
        "criterion = torch.nn.BCELoss(size_average=True)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(1000):\n",
        "  y_pred = model(x_data)\n",
        "  \n",
        "  loss = criterion(y_pred, y_data)\n",
        "  if epoch % 50 == 0:\n",
        "    print(epoch, loss.item())\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "hour_var = Variable(torch.Tensor([[1.0]]))\n",
        "print(\"predict 1 hour \",1.0,model(hour_var).item()>0.5)\n",
        "hour_var = Variable(torch.Tensor([[7.0]]))\n",
        "print(\"predict 7 hour \",1.0,model(hour_var).item()>0.5)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.7944773435592651\n",
            "50 0.7147016525268555\n",
            "100 0.6833253502845764\n",
            "150 0.6656846404075623\n",
            "200 0.6512323617935181\n",
            "250 0.6377397775650024\n",
            "300 0.6247899532318115\n",
            "350 0.6122987866401672\n",
            "400 0.6002395749092102\n",
            "450 0.5885959267616272\n",
            "500 0.5773528218269348\n",
            "550 0.5664961338043213\n",
            "600 0.5560117959976196\n",
            "650 0.5458860993385315\n",
            "700 0.5361055731773376\n",
            "750 0.5266572833061218\n",
            "800 0.5175283551216125\n",
            "850 0.5087064504623413\n",
            "900 0.5001795291900635\n",
            "950 0.4919361174106598\n",
            "predict 1 hour  1.0 False\n",
            "predict 7 hour  1.0 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-H9m9L7J9Xwz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Wide and Deep"
      ]
    },
    {
      "metadata": {
        "id": "3DBaDdUiB1Mi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4795773b-b7e4-4e72-9051-752ecd991019"
      },
      "cell_type": "code",
      "source": [
        "cd ../../"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HCBmy3gvFfQ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6cf1556c-d778-4dd7-801d-26b0347a6821"
      },
      "cell_type": "code",
      "source": [
        "cd drive/data"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w7O-u7fnF0PJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "4733c3ac-c518-48a8-eecf-de5e2ca73a9d"
      },
      "cell_type": "code",
      "source": [
        "!wget https://github.com/hunkim/PyTorchZeroToAll/raw/master/data/diabetes.csv.gz"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-07-13 05:23:05--  https://github.com/hunkim/PyTorchZeroToAll/raw/master/data/diabetes.csv.gz\r\n",
            "Resolving github.com (github.com)... 192.30.255.113, 192.30.255.112\r\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/diabetes.csv.gz [following]\n",
            "--2018-07-13 05:23:06--  https://raw.githubusercontent.com/hunkim/PyTorchZeroToAll/master/data/diabetes.csv.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13539 (13K) [application/octet-stream]\n",
            "Saving to: ‘diabetes.csv.gz’\n",
            "\n",
            "diabetes.csv.gz     100%[===================>]  13.22K  43.7KB/s    in 0.3s    \n",
            "\n",
            "2018-07-13 05:23:10 (43.7 KB/s) - ‘diabetes.csv.gz’ saved [13539/13539]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8vFCj4QqF9hr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "af54e9df-8cbd-4f1a-dfd3-90619db628d3"
      },
      "cell_type": "code",
      "source": [
        "cd ../"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Go25ohFkC1H8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1817
        },
        "outputId": "88388be9-c6d0-47ae-d00c-3206fc9770eb"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "xy = np.loadtxt('./data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
        "x_data = Variable(torch.from_numpy(xy[:, 0:-1]))\n",
        "y_data = Variable(torch.from_numpy(xy[:, [-1]]))\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.l1 = torch.nn.Linear(8,6)\n",
        "    self.l2 = torch.nn.Linear(6,4)\n",
        "    self.l3 = torch.nn.Linear(4,2)\n",
        "    self.l4 = torch.nn.Linear(2,1)\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out1 = self.sigmoid(self.l1(x))\n",
        "    out2 = self.sigmoid(self.l2(out1))\n",
        "    out3 = self.sigmoid(self.l3(out2))\n",
        "    y_pred = self.sigmoid(self.l4(out3))\n",
        "    return y_pred\n",
        "\n",
        "model = Model()\n",
        "\n",
        "criterion = torch.nn.BCELoss(size_average=True)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n",
        "\n",
        "for epoch in range(100):\n",
        "  y_pred = model(x_data)\n",
        "  \n",
        "  loss = criterion(y_pred, y_data)\n",
        "  print(epoch, loss.item())\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.7357483506202698\n",
            "1 0.7218292355537415\n",
            "2 0.7100878357887268\n",
            "3 0.7001838088035583\n",
            "4 0.6918265223503113\n",
            "5 0.6847731471061707\n",
            "6 0.6788142919540405\n",
            "7 0.6737775206565857\n",
            "8 0.6695171594619751\n",
            "9 0.6659101247787476\n",
            "10 0.6628547310829163\n",
            "11 0.660262942314148\n",
            "12 0.6580643057823181\n",
            "13 0.6561964154243469\n",
            "14 0.6546095609664917\n",
            "15 0.6532593369483948\n",
            "16 0.6521099209785461\n",
            "17 0.6511311531066895\n",
            "18 0.6502965688705444\n",
            "19 0.6495855450630188\n",
            "20 0.6489786505699158\n",
            "21 0.648460328578949\n",
            "22 0.6480172872543335\n",
            "23 0.6476388573646545\n",
            "24 0.6473155617713928\n",
            "25 0.6470389366149902\n",
            "26 0.6468020677566528\n",
            "27 0.6465994119644165\n",
            "28 0.6464259624481201\n",
            "29 0.6462767124176025\n",
            "30 0.6461496353149414\n",
            "31 0.6460407972335815\n",
            "32 0.645946741104126\n",
            "33 0.6458666920661926\n",
            "34 0.6457980871200562\n",
            "35 0.6457388401031494\n",
            "36 0.6456878185272217\n",
            "37 0.6456445455551147\n",
            "38 0.6456066966056824\n",
            "39 0.6455749273300171\n",
            "40 0.6455476880073547\n",
            "41 0.6455237865447998\n",
            "42 0.6455032825469971\n",
            "43 0.6454861164093018\n",
            "44 0.6454711556434631\n",
            "45 0.6454585194587708\n",
            "46 0.6454474925994873\n",
            "47 0.6454373598098755\n",
            "48 0.6454290747642517\n",
            "49 0.6454225182533264\n",
            "50 0.6454160809516907\n",
            "51 0.6454107165336609\n",
            "52 0.6454060077667236\n",
            "53 0.6454023122787476\n",
            "54 0.6453993916511536\n",
            "55 0.6453959345817566\n",
            "56 0.6453936100006104\n",
            "57 0.6453917622566223\n",
            "58 0.6453903913497925\n",
            "59 0.645388126373291\n",
            "60 0.6453865766525269\n",
            "61 0.6453854441642761\n",
            "62 0.6453843712806702\n",
            "63 0.6453837156295776\n",
            "64 0.6453827023506165\n",
            "65 0.6453825235366821\n",
            "66 0.6453812122344971\n",
            "67 0.6453805565834045\n",
            "68 0.6453801393508911\n",
            "69 0.645379900932312\n",
            "70 0.6453794240951538\n",
            "71 0.645379364490509\n",
            "72 0.645379364490509\n",
            "73 0.6453791260719299\n",
            "74 0.6453782320022583\n",
            "75 0.6453790068626404\n",
            "76 0.6453778743743896\n",
            "77 0.6453776359558105\n",
            "78 0.6453770995140076\n",
            "79 0.6453779935836792\n",
            "80 0.6453773975372314\n",
            "81 0.6453769207000732\n",
            "82 0.6453772783279419\n",
            "83 0.6453772783279419\n",
            "84 0.6453767418861389\n",
            "85 0.645376443862915\n",
            "86 0.6453765630722046\n",
            "87 0.6453762650489807\n",
            "88 0.6453760266304016\n",
            "89 0.6453765630722046\n",
            "90 0.6453766822814941\n",
            "91 0.6453763842582703\n",
            "92 0.6453757882118225\n",
            "93 0.6453762054443359\n",
            "94 0.6453759074211121\n",
            "95 0.6453753709793091\n",
            "96 0.6453759670257568\n",
            "97 0.645375669002533\n",
            "98 0.6453760862350464\n",
            "99 0.6453762054443359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GlKjSY8YIyPb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# PyTorch DataLoader\n",
        "\n",
        "## Batch Normalization\n",
        "```python\n",
        "for epoch in range(training_epochs):\n",
        "  # Loop over all batches\n",
        "  for i in range(total_batch):\n",
        "    \n",
        "```\n",
        "\n",
        "In the neural networks terminology\n",
        "\n",
        "- one epoch = one forward & backward of all the training examples\n",
        "- batch size = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need\n",
        "- number of iterations = number of passes, each pass using batch size number of examples. To be clear, one pass = one forward + one backward\n",
        "\n",
        "Example: if you have 1000 training examples, and your batch size is 100, then it will take 10 iterations to complete 1 epoch\n"
      ]
    },
    {
      "metadata": {
        "id": "kywuINSrIyvr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wXx6VwWCIy8r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DJmUEc6YIzSD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gimP8znIzW5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9jW-eGCcIzcw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lcKNzwW40EDi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "c3d78ac0-4218-462e-f396-19e3b7c79753"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "id": "irrnfXYT86NK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7d9870e9-6518-406d-ef65-31b447ae33e4"
      },
      "cell_type": "code",
      "source": [
        "batch_size= 1\n",
        "learning_rate = 0.0002\n",
        "epoch = 100"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pJE8OhZP9IST",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d730fcd0-94f9-4f2d-e3b5-a72804a84b40"
      },
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper parameters\n",
        "epoch = 5\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./drive/data/',\n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./drive/data/',\n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sftULFYmJwDc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "011d18f2-ae1b-4c9c-b622-4f2f098a1ccb"
      },
      "cell_type": "code",
      "source": [
        "def conv_2_block(in_dim,out_dim):\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(in_dim,out_dim,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_dim,out_dim,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def conv_3_block(in_dim,out_dim):\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(in_dim,out_dim,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_dim,out_dim,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_dim,out_dim,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2)\n",
        "    )\n",
        "    return model"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GpHj_gZsKnFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "1536bdff-4dd6-40d8-de61-73533f43b376"
      },
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, base_dim, num_classes=num_classes):\n",
        "        super(VGG, self).__init__()\n",
        "        self.feature = nn.Sequential(\n",
        "            conv_2_block(3,base_dim),\n",
        "            conv_2_block(base_dim,2*base_dim),\n",
        "            conv_3_block(2*base_dim,4*base_dim),\n",
        "            conv_3_block(4*base_dim,8*base_dim),\n",
        "            conv_3_block(8*base_dim,8*base_dim),            \n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(8*base_dim, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(100, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(20, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layer(x)\n",
        "        return x\n",
        "    \n",
        "model = VGG(base_dim=64).cuda()\n",
        "\n",
        "for i in model.named_children():\n",
        "    print(i)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('feature', Sequential(\n",
            "  (0): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (4): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "))\n",
            "('fc_layer', Sequential(\n",
            "  (0): Linear(in_features=512, out_features=100, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.5)\n",
            "  (3): Linear(in_features=100, out_features=20, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Dropout(p=0.5)\n",
            "  (6): Linear(in_features=20, out_features=10, bias=True)\n",
            "))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hk0i9TEdKnqD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "e61b032c-6941-4cee-b0a0-5cad8b97d904"
      },
      "cell_type": "code",
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b18qvMTQK5Do",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1b3360f4-b75b-4825-e788-620f9bccf2df"
      },
      "cell_type": "code",
      "source": [
        "for i in range(epoch):\n",
        "    for img,label in train_loader:\n",
        "        img = Variable(img).cuda()\n",
        "        label = Variable(label).cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(img)\n",
        "        loss = loss_func(output,label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(loss)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.3021, device='cuda:0')\n",
            "tensor(2.3004, device='cuda:0')\n",
            "tensor(2.3034, device='cuda:0')\n",
            "tensor(2.3032, device='cuda:0')\n",
            "tensor(2.3020, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2IwnEj56L64n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}